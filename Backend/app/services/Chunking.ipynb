{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from -r requirements.txt (line 1)) (2.32.3)\n",
      "Requirement already satisfied: fitz in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from -r requirements.txt (line 2)) (0.0.1.dev2)\n",
      "Requirement already satisfied: PyMuPDF in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from -r requirements.txt (line 3)) (1.25.4)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from -r requirements.txt (line 4)) (2.0.2)\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from -r requirements.txt (line 5)) (2.2.3)\n",
      "Requirement already satisfied: sentence-transformers in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from -r requirements.txt (line 6)) (3.4.1)\n",
      "Requirement already satisfied: spacy in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from -r requirements.txt (line 7)) (3.8.3)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from -r requirements.txt (line 8)) (4.67.1)\n",
      "Requirement already satisfied: scikit-learn in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from -r requirements.txt (line 9)) (1.6.1)\n",
      "Requirement already satisfied: transformers in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from -r requirements.txt (line 10)) (4.50.0)\n",
      "Requirement already satisfied: jupyterlab in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from -r requirements.txt (line 11)) (4.3.6)\n",
      "Requirement already satisfied: wheel in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from -r requirements.txt (line 12)) (0.45.1)\n",
      "Requirement already satisfied: matplotlib in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from -r requirements.txt (line 13)) (3.9.4)\n",
      "Requirement already satisfied: cython in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from -r requirements.txt (line 14)) (3.0.12)\n",
      "Requirement already satisfied: langchain-community in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from -r requirements.txt (line 15)) (0.3.20)\n",
      "Requirement already satisfied: langchain in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from -r requirements.txt (line 16)) (0.3.23)\n",
      "Requirement already satisfied: openai in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from -r requirements.txt (line 17)) (0.28.1)\n",
      "Requirement already satisfied: tiktoken in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from -r requirements.txt (line 18)) (0.9.0)\n",
      "Requirement already satisfied: nltk in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from -r requirements.txt (line 19)) (3.9.1)\n",
      "Requirement already satisfied: chroma-migrate in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from -r requirements.txt (line 20)) (0.0.7)\n",
      "Requirement already satisfied: chromadb in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from -r requirements.txt (line 22)) (1.0.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from requests->-r requirements.txt (line 1)) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from requests->-r requirements.txt (line 1)) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from requests->-r requirements.txt (line 1)) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from requests->-r requirements.txt (line 1)) (2025.1.31)\n",
      "Requirement already satisfied: configobj in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from fitz->-r requirements.txt (line 2)) (5.0.9)\n",
      "Requirement already satisfied: configparser in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from fitz->-r requirements.txt (line 2)) (7.2.0)\n",
      "Requirement already satisfied: httplib2 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from fitz->-r requirements.txt (line 2)) (0.22.0)\n",
      "Requirement already satisfied: nibabel in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from fitz->-r requirements.txt (line 2)) (5.3.2)\n",
      "Requirement already satisfied: nipype in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from fitz->-r requirements.txt (line 2)) (1.10.0)\n",
      "Requirement already satisfied: pyxnat in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from fitz->-r requirements.txt (line 2)) (1.6.3)\n",
      "Requirement already satisfied: scipy in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from fitz->-r requirements.txt (line 2)) (1.13.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from pandas->-r requirements.txt (line 5)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from pandas->-r requirements.txt (line 5)) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from pandas->-r requirements.txt (line 5)) (2025.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from sentence-transformers->-r requirements.txt (line 6)) (2.6.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from sentence-transformers->-r requirements.txt (line 6)) (0.29.3)\n",
      "Requirement already satisfied: Pillow in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from sentence-transformers->-r requirements.txt (line 6)) (11.1.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from spacy->-r requirements.txt (line 7)) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from spacy->-r requirements.txt (line 7)) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from spacy->-r requirements.txt (line 7)) (1.0.12)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from spacy->-r requirements.txt (line 7)) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from spacy->-r requirements.txt (line 7)) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.0 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from spacy->-r requirements.txt (line 7)) (8.3.4)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from spacy->-r requirements.txt (line 7)) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from spacy->-r requirements.txt (line 7)) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from spacy->-r requirements.txt (line 7)) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from spacy->-r requirements.txt (line 7)) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from spacy->-r requirements.txt (line 7)) (0.15.2)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from spacy->-r requirements.txt (line 7)) (2.11.3)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from spacy->-r requirements.txt (line 7)) (3.1.6)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from spacy->-r requirements.txt (line 7)) (75.8.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from spacy->-r requirements.txt (line 7)) (24.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from spacy->-r requirements.txt (line 7)) (3.5.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from scikit-learn->-r requirements.txt (line 9)) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from scikit-learn->-r requirements.txt (line 9)) (3.6.0)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from transformers->-r requirements.txt (line 10)) (3.18.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from transformers->-r requirements.txt (line 10)) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from transformers->-r requirements.txt (line 10)) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from transformers->-r requirements.txt (line 10)) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from transformers->-r requirements.txt (line 10)) (0.5.3)\n",
      "Requirement already satisfied: async-lru>=1.0.0 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from jupyterlab->-r requirements.txt (line 11)) (2.0.5)\n",
      "Requirement already satisfied: httpx>=0.25.0 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from jupyterlab->-r requirements.txt (line 11)) (0.28.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.3 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from jupyterlab->-r requirements.txt (line 11)) (8.6.1)\n",
      "Requirement already satisfied: ipykernel>=6.5.0 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from jupyterlab->-r requirements.txt (line 11)) (6.29.5)\n",
      "Requirement already satisfied: jupyter-core in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from jupyterlab->-r requirements.txt (line 11)) (5.7.2)\n",
      "Requirement already satisfied: jupyter-lsp>=2.0.0 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from jupyterlab->-r requirements.txt (line 11)) (2.2.5)\n",
      "Requirement already satisfied: jupyter-server<3,>=2.4.0 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from jupyterlab->-r requirements.txt (line 11)) (2.15.0)\n",
      "Requirement already satisfied: jupyterlab-server<3,>=2.27.1 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from jupyterlab->-r requirements.txt (line 11)) (2.27.3)\n",
      "Requirement already satisfied: notebook-shim>=0.2 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from jupyterlab->-r requirements.txt (line 11)) (0.2.4)\n",
      "Requirement already satisfied: tomli>=1.2.2 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from jupyterlab->-r requirements.txt (line 11)) (2.2.1)\n",
      "Requirement already satisfied: tornado>=6.2.0 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from jupyterlab->-r requirements.txt (line 11)) (6.4.2)\n",
      "Requirement already satisfied: traitlets in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from jupyterlab->-r requirements.txt (line 11)) (5.14.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from matplotlib->-r requirements.txt (line 13)) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from matplotlib->-r requirements.txt (line 13)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from matplotlib->-r requirements.txt (line 13)) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from matplotlib->-r requirements.txt (line 13)) (1.4.7)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from matplotlib->-r requirements.txt (line 13)) (3.2.1)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from matplotlib->-r requirements.txt (line 13)) (6.5.2)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.45 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from langchain-community->-r requirements.txt (line 15)) (0.3.51)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from langchain-community->-r requirements.txt (line 15)) (2.0.39)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from langchain-community->-r requirements.txt (line 15)) (3.11.14)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from langchain-community->-r requirements.txt (line 15)) (9.0.0)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from langchain-community->-r requirements.txt (line 15)) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from langchain-community->-r requirements.txt (line 15)) (2.8.1)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from langchain-community->-r requirements.txt (line 15)) (0.3.19)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from langchain-community->-r requirements.txt (line 15)) (0.4.0)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from langchain->-r requirements.txt (line 16)) (0.3.8)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from langchain->-r requirements.txt (line 16)) (4.0.3)\n",
      "Requirement already satisfied: click in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from nltk->-r requirements.txt (line 19)) (8.1.8)\n",
      "Requirement already satisfied: clickhouse-connect==0.6.6 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from chroma-migrate->-r requirements.txt (line 20)) (0.6.6)\n",
      "Requirement already satisfied: duckdb==0.7.1 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from chroma-migrate->-r requirements.txt (line 20)) (0.7.1)\n",
      "Requirement already satisfied: chroma-bullet in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from chroma-migrate->-r requirements.txt (line 20)) (2.2.0)\n",
      "Requirement already satisfied: more-itertools>=9.1.0 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from chroma-migrate->-r requirements.txt (line 20)) (10.6.0)\n",
      "Requirement already satisfied: zstandard in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from clickhouse-connect==0.6.6->chroma-migrate->-r requirements.txt (line 20)) (0.23.0)\n",
      "Requirement already satisfied: lz4 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from clickhouse-connect==0.6.6->chroma-migrate->-r requirements.txt (line 20)) (4.4.4)\n",
      "Requirement already satisfied: build>=1.0.3 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from chromadb->-r requirements.txt (line 22)) (1.2.2.post1)\n",
      "Requirement already satisfied: chroma-hnswlib==0.7.6 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from chromadb->-r requirements.txt (line 22)) (0.7.6)\n",
      "Requirement already satisfied: fastapi==0.115.9 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from chromadb->-r requirements.txt (line 22)) (0.115.9)\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from uvicorn[standard]>=0.18.3->chromadb->-r requirements.txt (line 22)) (0.34.0)\n",
      "Requirement already satisfied: posthog>=2.4.0 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from chromadb->-r requirements.txt (line 22)) (3.23.0)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from chromadb->-r requirements.txt (line 22)) (4.12.2)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from chromadb->-r requirements.txt (line 22)) (1.19.2)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from chromadb->-r requirements.txt (line 22)) (1.31.1)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from chromadb->-r requirements.txt (line 22)) (1.31.1)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from chromadb->-r requirements.txt (line 22)) (0.52b1)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from chromadb->-r requirements.txt (line 22)) (1.31.1)\n",
      "Requirement already satisfied: pypika>=0.48.9 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from chromadb->-r requirements.txt (line 22)) (0.48.9)\n",
      "Requirement already satisfied: overrides>=7.3.1 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from chromadb->-r requirements.txt (line 22)) (7.7.0)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from chromadb->-r requirements.txt (line 22)) (1.71.0)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from chromadb->-r requirements.txt (line 22)) (4.3.0)\n",
      "Requirement already satisfied: kubernetes>=28.1.0 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from chromadb->-r requirements.txt (line 22)) (32.0.1)\n",
      "Requirement already satisfied: mmh3>=4.0.1 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from chromadb->-r requirements.txt (line 22)) (5.1.0)\n",
      "Requirement already satisfied: orjson>=3.9.12 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from chromadb->-r requirements.txt (line 22)) (3.10.16)\n",
      "Requirement already satisfied: rich>=10.11.0 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from chromadb->-r requirements.txt (line 22)) (13.9.4)\n",
      "Requirement already satisfied: jsonschema>=4.19.0 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from chromadb->-r requirements.txt (line 22)) (4.23.0)\n",
      "Requirement already satisfied: starlette<0.46.0,>=0.40.0 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from fastapi==0.115.9->chromadb->-r requirements.txt (line 22)) (0.45.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community->-r requirements.txt (line 15)) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community->-r requirements.txt (line 15)) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community->-r requirements.txt (line 15)) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community->-r requirements.txt (line 15)) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community->-r requirements.txt (line 15)) (6.2.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community->-r requirements.txt (line 15)) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community->-r requirements.txt (line 15)) (1.18.3)\n",
      "Requirement already satisfied: pyproject_hooks in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from build>=1.0.3->chromadb->-r requirements.txt (line 22)) (1.2.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community->-r requirements.txt (line 15)) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community->-r requirements.txt (line 15)) (0.9.0)\n",
      "Requirement already satisfied: anyio in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from httpx>=0.25.0->jupyterlab->-r requirements.txt (line 11)) (4.9.0)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from httpx>=0.25.0->jupyterlab->-r requirements.txt (line 11)) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from httpcore==1.*->httpx>=0.25.0->jupyterlab->-r requirements.txt (line 11)) (0.14.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from huggingface-hub>=0.20.0->sentence-transformers->-r requirements.txt (line 6)) (2025.3.0)\n",
      "Requirement already satisfied: zipp>=3.20 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from importlib-metadata>=4.8.3->jupyterlab->-r requirements.txt (line 11)) (3.21.0)\n",
      "Requirement already satisfied: appnope in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from ipykernel>=6.5.0->jupyterlab->-r requirements.txt (line 11)) (0.1.4)\n",
      "Requirement already satisfied: comm>=0.1.1 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from ipykernel>=6.5.0->jupyterlab->-r requirements.txt (line 11)) (0.2.2)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from ipykernel>=6.5.0->jupyterlab->-r requirements.txt (line 11)) (1.8.11)\n",
      "Requirement already satisfied: ipython>=7.23.1 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from ipykernel>=6.5.0->jupyterlab->-r requirements.txt (line 11)) (8.18.1)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from ipykernel>=6.5.0->jupyterlab->-r requirements.txt (line 11)) (8.6.3)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from ipykernel>=6.5.0->jupyterlab->-r requirements.txt (line 11)) (0.1.7)\n",
      "Requirement already satisfied: nest-asyncio in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from ipykernel>=6.5.0->jupyterlab->-r requirements.txt (line 11)) (1.6.0)\n",
      "Requirement already satisfied: psutil in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from ipykernel>=6.5.0->jupyterlab->-r requirements.txt (line 11)) (5.8.0)\n",
      "Requirement already satisfied: pyzmq>=24 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from ipykernel>=6.5.0->jupyterlab->-r requirements.txt (line 11)) (26.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from jinja2->spacy->-r requirements.txt (line 7)) (3.0.2)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from jsonschema>=4.19.0->chromadb->-r requirements.txt (line 22)) (2024.10.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from jsonschema>=4.19.0->chromadb->-r requirements.txt (line 22)) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from jsonschema>=4.19.0->chromadb->-r requirements.txt (line 22)) (0.23.1)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from jupyter-core->jupyterlab->-r requirements.txt (line 11)) (4.3.7)\n",
      "Requirement already satisfied: argon2-cffi>=21.1 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 11)) (23.1.0)\n",
      "Requirement already satisfied: jupyter-events>=0.11.0 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 11)) (0.12.0)\n",
      "Requirement already satisfied: jupyter-server-terminals>=0.4.4 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 11)) (0.5.3)\n",
      "Requirement already satisfied: nbconvert>=6.4.4 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 11)) (7.16.6)\n",
      "Requirement already satisfied: nbformat>=5.3.0 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 11)) (5.10.4)\n",
      "Requirement already satisfied: prometheus-client>=0.9 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 11)) (0.21.1)\n",
      "Requirement already satisfied: send2trash>=1.8.2 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 11)) (1.8.3)\n",
      "Requirement already satisfied: terminado>=0.8.3 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 11)) (0.18.1)\n",
      "Requirement already satisfied: websocket-client>=1.7 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 11)) (1.8.0)\n",
      "Requirement already satisfied: babel>=2.10 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab->-r requirements.txt (line 11)) (2.17.0)\n",
      "Requirement already satisfied: json5>=0.9.0 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab->-r requirements.txt (line 11)) (0.10.0)\n",
      "Requirement already satisfied: six>=1.9.0 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from kubernetes>=28.1.0->chromadb->-r requirements.txt (line 22)) (1.17.0)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from kubernetes>=28.1.0->chromadb->-r requirements.txt (line 22)) (2.38.0)\n",
      "Requirement already satisfied: requests-oauthlib in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from kubernetes>=28.1.0->chromadb->-r requirements.txt (line 22)) (2.0.0)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from kubernetes>=28.1.0->chromadb->-r requirements.txt (line 22)) (3.2.2)\n",
      "Requirement already satisfied: durationpy>=0.7 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from kubernetes>=28.1.0->chromadb->-r requirements.txt (line 22)) (0.9)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from langchain-core<1.0.0,>=0.3.45->langchain-community->-r requirements.txt (line 15)) (1.33)\n",
      "Requirement already satisfied: language-data>=1.2 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from langcodes<4.0.0,>=3.2.0->spacy->-r requirements.txt (line 7)) (1.3.0)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from langsmith<0.4,>=0.1.125->langchain-community->-r requirements.txt (line 15)) (1.0.0)\n",
      "Requirement already satisfied: coloredlogs in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from onnxruntime>=1.14.1->chromadb->-r requirements.txt (line 22)) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from onnxruntime>=1.14.1->chromadb->-r requirements.txt (line 22)) (25.2.10)\n",
      "Requirement already satisfied: protobuf in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from onnxruntime>=1.14.1->chromadb->-r requirements.txt (line 22)) (5.29.4)\n",
      "Requirement already satisfied: sympy in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from onnxruntime>=1.14.1->chromadb->-r requirements.txt (line 22)) (1.13.1)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from opentelemetry-api>=1.2.0->chromadb->-r requirements.txt (line 22)) (1.2.18)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb->-r requirements.txt (line 22)) (1.69.2)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.31.1 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb->-r requirements.txt (line 22)) (1.31.1)\n",
      "Requirement already satisfied: opentelemetry-proto==1.31.1 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb->-r requirements.txt (line 22)) (1.31.1)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-asgi==0.52b1 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb->-r requirements.txt (line 22)) (0.52b1)\n",
      "Requirement already satisfied: opentelemetry-instrumentation==0.52b1 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb->-r requirements.txt (line 22)) (0.52b1)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.52b1 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb->-r requirements.txt (line 22)) (0.52b1)\n",
      "Requirement already satisfied: opentelemetry-util-http==0.52b1 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb->-r requirements.txt (line 22)) (0.52b1)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from opentelemetry-instrumentation==0.52b1->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb->-r requirements.txt (line 22)) (1.17.2)\n",
      "Requirement already satisfied: asgiref~=3.0 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from opentelemetry-instrumentation-asgi==0.52b1->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb->-r requirements.txt (line 22)) (3.8.1)\n",
      "Requirement already satisfied: monotonic>=1.5 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from posthog>=2.4.0->chromadb->-r requirements.txt (line 22)) (1.6)\n",
      "Requirement already satisfied: backoff>=1.10.0 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from posthog>=2.4.0->chromadb->-r requirements.txt (line 22)) (2.2.1)\n",
      "Requirement already satisfied: distro>=1.5.0 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from posthog>=2.4.0->chromadb->-r requirements.txt (line 22)) (1.9.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy->-r requirements.txt (line 7)) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.1 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy->-r requirements.txt (line 7)) (2.33.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy->-r requirements.txt (line 7)) (0.4.0)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community->-r requirements.txt (line 15)) (1.1.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from rich>=10.11.0->chromadb->-r requirements.txt (line 22)) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from rich>=10.11.0->chromadb->-r requirements.txt (line 22)) (2.19.1)\n",
      "Requirement already satisfied: blis<1.3.0,>=1.2.0 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from thinc<8.4.0,>=8.3.0->spacy->-r requirements.txt (line 7)) (1.2.0)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from thinc<8.4.0,>=8.3.0->spacy->-r requirements.txt (line 7)) (0.1.5)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from torch>=1.11.0->sentence-transformers->-r requirements.txt (line 6)) (3.2.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from sympy->onnxruntime>=1.14.1->chromadb->-r requirements.txt (line 22)) (1.3.0)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from typer<1.0.0,>=0.3.0->spacy->-r requirements.txt (line 7)) (1.5.4)\n",
      "Requirement already satisfied: httptools>=0.6.3 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from uvicorn[standard]>=0.18.3->chromadb->-r requirements.txt (line 22)) (0.6.4)\n",
      "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from uvicorn[standard]>=0.18.3->chromadb->-r requirements.txt (line 22)) (0.21.0)\n",
      "Requirement already satisfied: watchfiles>=0.13 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from uvicorn[standard]>=0.18.3->chromadb->-r requirements.txt (line 22)) (1.0.4)\n",
      "Requirement already satisfied: websockets>=10.4 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from uvicorn[standard]>=0.18.3->chromadb->-r requirements.txt (line 22)) (15.0.1)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from weasel<0.5.0,>=0.1.0->spacy->-r requirements.txt (line 7)) (0.21.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from weasel<0.5.0,>=0.1.0->spacy->-r requirements.txt (line 7)) (7.1.0)\n",
      "Requirement already satisfied: prov>=1.5.2 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from nipype->fitz->-r requirements.txt (line 2)) (2.0.1)\n",
      "Requirement already satisfied: pydot>=1.2.3 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from nipype->fitz->-r requirements.txt (line 2)) (3.0.4)\n",
      "Requirement already satisfied: rdflib>=5.0.0 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from nipype->fitz->-r requirements.txt (line 2)) (6.3.2)\n",
      "Requirement already satisfied: simplejson>=3.8.0 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from nipype->fitz->-r requirements.txt (line 2)) (3.20.1)\n",
      "Requirement already satisfied: traits>=6.2 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from nipype->fitz->-r requirements.txt (line 2)) (7.0.2)\n",
      "Requirement already satisfied: acres in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from nipype->fitz->-r requirements.txt (line 2)) (0.3.0)\n",
      "Requirement already satisfied: etelemetry>=0.3.1 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from nipype->fitz->-r requirements.txt (line 2)) (0.3.1)\n",
      "Requirement already satisfied: looseversion!=1.2 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from nipype->fitz->-r requirements.txt (line 2)) (1.3.0)\n",
      "Requirement already satisfied: puremagic in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from nipype->fitz->-r requirements.txt (line 2)) (1.28)\n",
      "Requirement already satisfied: lxml>=4.3 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from pyxnat->fitz->-r requirements.txt (line 2)) (5.3.1)\n",
      "Requirement already satisfied: pathlib>=1.0 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from pyxnat->fitz->-r requirements.txt (line 2)) (1.0.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from anyio->httpx>=0.25.0->jupyterlab->-r requirements.txt (line 11)) (1.2.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from anyio->httpx>=0.25.0->jupyterlab->-r requirements.txt (line 11)) (1.3.1)\n",
      "Requirement already satisfied: argon2-cffi-bindings in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 11)) (21.2.0)\n",
      "Requirement already satisfied: ci-info>=0.2 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from etelemetry>=0.3.1->nipype->fitz->-r requirements.txt (line 2)) (0.3.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb->-r requirements.txt (line 22)) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb->-r requirements.txt (line 22)) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb->-r requirements.txt (line 22)) (4.9)\n",
      "Requirement already satisfied: decorator in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab->-r requirements.txt (line 11)) (5.2.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab->-r requirements.txt (line 11)) (0.19.2)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab->-r requirements.txt (line 11)) (3.0.50)\n",
      "Requirement already satisfied: stack-data in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab->-r requirements.txt (line 11)) (0.6.3)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab->-r requirements.txt (line 11)) (4.9.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.45->langchain-community->-r requirements.txt (line 15)) (3.0.0)\n",
      "Requirement already satisfied: python-json-logger>=2.0.4 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 11)) (3.3.0)\n",
      "Requirement already satisfied: rfc3339-validator in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 11)) (0.1.4)\n",
      "Requirement already satisfied: rfc3986-validator>=0.1.1 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 11)) (0.1.1)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy->-r requirements.txt (line 7)) (1.2.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb->-r requirements.txt (line 22)) (0.1.2)\n",
      "Requirement already satisfied: beautifulsoup4 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 11)) (4.13.3)\n",
      "Requirement already satisfied: bleach!=5.0.0 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from bleach[css]!=5.0.0->nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 11)) (6.2.0)\n",
      "Requirement already satisfied: defusedxml in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 11)) (0.7.1)\n",
      "Requirement already satisfied: jupyterlab-pygments in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 11)) (0.3.0)\n",
      "Requirement already satisfied: mistune<4,>=2.0.3 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 11)) (3.1.3)\n",
      "Requirement already satisfied: nbclient>=0.5.0 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 11)) (0.10.2)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 11)) (1.5.1)\n",
      "Requirement already satisfied: fastjsonschema>=2.15 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from nbformat>=5.3.0->jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 11)) (2.21.1)\n",
      "Requirement already satisfied: isodate<0.7.0,>=0.6.0 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from rdflib>=5.0.0->nipype->fitz->-r requirements.txt (line 2)) (0.6.1)\n",
      "Requirement already satisfied: ptyprocess in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from terminado>=0.8.3->jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 11)) (0.7.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community->-r requirements.txt (line 15)) (1.0.0)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb->-r requirements.txt (line 22)) (10.0)\n",
      "Requirement already satisfied: webencodings in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from bleach!=5.0.0->bleach[css]!=5.0.0->nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 11)) (0.5.1)\n",
      "Requirement already satisfied: tinycss2<1.5,>=1.1.0 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from bleach[css]!=5.0.0->nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 11)) (1.4.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab->-r requirements.txt (line 11)) (0.8.4)\n",
      "Requirement already satisfied: fqdn in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 11)) (1.5.1)\n",
      "Requirement already satisfied: isoduration in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 11)) (20.11.0)\n",
      "Requirement already satisfied: uri-template in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 11)) (1.3.0)\n",
      "Requirement already satisfied: webcolors>=24.6.0 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 11)) (24.11.1)\n",
      "Requirement already satisfied: wcwidth in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab->-r requirements.txt (line 11)) (0.2.13)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb->-r requirements.txt (line 22)) (0.6.1)\n",
      "Requirement already satisfied: cffi>=1.0.1 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 11)) (1.17.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from beautifulsoup4->nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 11)) (2.6)\n",
      "Requirement already satisfied: executing>=1.2.0 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from stack-data->ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab->-r requirements.txt (line 11)) (2.1.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from stack-data->ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab->-r requirements.txt (line 11)) (3.0.0)\n",
      "Requirement already satisfied: pure_eval in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from stack-data->ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab->-r requirements.txt (line 11)) (0.2.3)\n",
      "Requirement already satisfied: pycparser in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 11)) (2.22)\n",
      "Requirement already satisfied: arrow>=0.15.0 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 11)) (1.3.0)\n",
      "Requirement already satisfied: types-python-dateutil>=2.8.10 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from arrow>=0.15.0->isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 11)) (2.9.0.20241206)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chunking Lesson plan with Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n1. Hierarchically chunked by markers and sections.\\n2. Embedded with metadata including grade, subject, topic, and section type.\\n3. Queried using metadata filters that match fields such as grade and topic.\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# High Level explanation for lesson plan chunking: \n",
    "\"\"\"\n",
    "1. Hierarchically chunked by markers and sections.\n",
    "2. Embedded with metadata including grade, subject, topic, and section type.\n",
    "3. Queried using metadata filters that match fields such as grade and topic.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/myenv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "import uuid\n",
    "from typing import List\n",
    "from langchain.text_splitter import TextSplitter\n",
    "from langchain.docstore.document import Document\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "\n",
    "# -------------------------------\n",
    "# 1. Our own Custom TextSplitter \n",
    "# note: we are using Hierarchical Chunking\n",
    "# -------------------------------\n",
    "class LessonPlanTextSplitter(TextSplitter):\n",
    "    def split_text(self, text: str) -> List[Document]:\n",
    "        stop_marker = r\"Note: The following pages are intended for classroom use for students as a visual aid to learning\\.\"\n",
    "        split_block = re.split(stop_marker, text, maxsplit=1)\n",
    "        content = split_block[0].strip() if split_block else text.strip()\n",
    "\n",
    "        # Split into intro and remainder\n",
    "        parts = re.split(r\"Student/Teacher Actions:\\s*\", content, flags=re.IGNORECASE, maxsplit=1)\n",
    "        if len(parts) == 2:\n",
    "            intro, remainder = parts[0].strip(), parts[1].strip()\n",
    "        else:\n",
    "            intro, remainder = content, \"\"\n",
    "\n",
    "        docs = []\n",
    "        docs.append(Document(page_content=intro, metadata={\"section\": \"intro_context\"}))\n",
    "\n",
    "        combined_pattern = r\"(Assessment\\s*\\n|Extensions(?: and Connections)?\\s*\\n|Strategies for Differentiation\\s*\\n)\"\n",
    "        split_sec = re.split(combined_pattern, remainder)\n",
    "\n",
    "        # Instructional steps (first segment)\n",
    "        instr = split_sec[0].strip()\n",
    "        if instr:\n",
    "            docs.append(Document(page_content=instr, metadata={\"section\": \"instructional_steps\"}))\n",
    "\n",
    "        # Subsequent header/content pairs\n",
    "        for i in range(1, len(split_sec) - 1, 2):\n",
    "            header = split_sec[i].strip().lower()\n",
    "            content = split_sec[i + 1].strip()\n",
    "            if header.startswith(\"assessment\"):\n",
    "                docs.append(Document(page_content=content, metadata={\"section\": \"assessment\"}))\n",
    "            elif header.startswith(\"extensions\"):\n",
    "                docs.append(Document(page_content=content, metadata={\"section\": \"extensions\"}))\n",
    "            elif header.startswith(\"strategies for differentiation\"):\n",
    "                docs.append(Document(page_content=content, metadata={\"section\": \"differentiation\"}))\n",
    "\n",
    "        return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# 2. Chunk the Lesson Plans \n",
    "# note: here our main file is test_with_notes.txt where all our lesson plan is stored\n",
    "# -------------------------------\n",
    "with open(\"test_with_notes_modified.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    full_text = f.read()\n",
    "\n",
    "# Split into individual lesson plan blocks\n",
    "lesson_plan_blocks = re.findall(\n",
    "    r\"--- Start of Lesson Plan(.*?)--- End of Lesson Plan\",\n",
    "    full_text,\n",
    "    re.DOTALL\n",
    ")\n",
    "\n",
    "splitter = LessonPlanTextSplitter()\n",
    "all_docs: List[Document] = []\n",
    "\n",
    "for lesson_index, block in enumerate(lesson_plan_blocks):\n",
    "    docs = splitter.split_text(block)\n",
    "    # Tag each chunk with its lesson index\n",
    "    for doc in docs:\n",
    "        doc.metadata[\"lesson_index\"] = lesson_index\n",
    "    all_docs.extend(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# 3. Extract Lesson-Level Metadata (Updated with strand + cleaned subject)\n",
    "# -------------------------------\n",
    "\n",
    "def extract_metadata_from_intro(intro: str):\n",
    "    special_grades = [\n",
    "    \"Kindergarten\",\n",
    "    \"Algebra I\",\n",
    "    \"Geometry\",\n",
    "    \"Algebra 2\",\n",
    "    \"Algebra, Functions & Data Analysis\"\n",
    "]\n",
    "\n",
    "    # First try to match \"Grade X\"\n",
    "    grade_match = re.search(r\"Grade\\s*(\\d+)\", intro)\n",
    "    grade = grade_match.group(1) if grade_match else None\n",
    "\n",
    "    # If no \"Grade X\" found, check for special labels\n",
    "    if not grade:\n",
    "        for sg in special_grades:\n",
    "            if sg.lower() in intro.lower():\n",
    "                grade = sg\n",
    "                break\n",
    "\n",
    "    # Try to extract subject between \"Subject:\" and \"Strand:\", if available\n",
    "    subject_match = re.search(r\"Subject:\\s*(.*?)\\s*Strand:\", intro, flags=re.DOTALL)\n",
    "\n",
    "    # If not found, try to extract subject from \"Subject:\" alone\n",
    "    if not subject_match:\n",
    "        subject_match = re.search(r\"Subject:\\s*(.+)\", intro)\n",
    "\n",
    "    strand = re.search(r\"Strand:\\s*(.+)\", intro)\n",
    "    topic = re.search(r\"Topic:\\s*(.+)\", intro)\n",
    "    title = intro.split(\"\\n\")[0].strip()\n",
    "\n",
    "    # Clean subject text\n",
    "    subject_text = subject_match.group(1).strip() if subject_match else None\n",
    "    if subject_text:\n",
    "        subject_text = re.sub(r'\\s+', ' ', subject_text)\n",
    "\n",
    "    return {\n",
    "        \"grade\": grade,\n",
    "        \"subject\": subject_text,\n",
    "        \"strand\": strand.group(1).strip() if strand else None,\n",
    "        \"topic\": topic.group(1).strip() if topic else None,\n",
    "        \"lesson_title\": title\n",
    "    }\n",
    "\n",
    "\n",
    "# Build map: lesson_index -> metadata\n",
    "lesson_metadata_map = {}\n",
    "for doc in all_docs:\n",
    "    if doc.metadata[\"section\"] == \"intro_context\":\n",
    "        lesson_metadata_map[doc.metadata[\"lesson_index\"]] = extract_metadata_from_intro(doc.page_content)\n",
    "# # -------------------------------\n",
    "# # Print out the metadata for each lesson\n",
    "# # -------------------------------\n",
    "# for lesson_index, metadata in lesson_metadata_map.items():\n",
    "#     print(f\"Lesson Index: {lesson_index}\")\n",
    "#     print(f\"  Lesson Title: {metadata.get('lesson_title')}\")\n",
    "#     print(f\"  Grade: {metadata.get('grade')}\")\n",
    "#     print(f\"  Strand: {metadata.get('strand')}\")\n",
    "#     print(f\"  Subject: {metadata.get('subject')}\")\n",
    "#     print(f\"  Topic: {metadata.get('topic')}\")\n",
    "#     print(\"-\" * 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔁 Re-embedding complete with updated metadata.\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from chromadb import PersistentClient\n",
    "\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "client = PersistentClient(path=\"./chroma_store\")\n",
    "# client.delete_collection(\"lesson_plans\")\n",
    "collection = client.get_or_create_collection(\"lesson_plans\")\n",
    "# Optional: clear existing collection to avoid duplicates\n",
    "# collection.delete()  # Uncomment this only if you want a fresh start\n",
    "\n",
    "existing_ids = set(collection.get()[\"ids\"])\n",
    "\n",
    "for doc in all_docs:\n",
    "    # Update metadata from lesson_metadata_map\n",
    "    lesson_meta = lesson_metadata_map.get(doc.metadata[\"lesson_index\"], {})\n",
    "    doc.metadata.update(lesson_meta)\n",
    "\n",
    "    # Clean nulls\n",
    "    cleaned_metadata = {k: v for k, v in doc.metadata.items() if v is not None}\n",
    "\n",
    "    doc_id = f\"lesson{cleaned_metadata['lesson_index']}_{cleaned_metadata['section']}\"\n",
    "    if doc_id in existing_ids:\n",
    "        continue\n",
    "\n",
    "    embedding = model.encode(doc.page_content).tolist()\n",
    "    collection.add(\n",
    "        documents=[doc.page_content],\n",
    "        metadatas=[cleaned_metadata],\n",
    "        embeddings=[embedding],\n",
    "        ids=[doc_id]\n",
    "    )\n",
    "    existing_ids.add(doc_id)\n",
    "\n",
    "print(\"🔁 Re-embedding complete with updated metadata.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"ids\": [\n",
      "        \"lesson36_intro_context\",\n",
      "        \"lesson36_instructional_steps\",\n",
      "        \"lesson36_extensions\",\n",
      "        \"lesson36_assessment\",\n",
      "        \"lesson36_differentiation\"\n",
      "    ],\n",
      "    \"documents\": [\n",
      "        \"Algebra 2-Equations and Inequalities-AII.3a - Absolute Value Equations and Inequalities.pdf ---\\nSubject:\\nAbsolute Value Equations and Inequalities\\nStrand:\\nEquations and Inequalities\\nTopic:\\nSolving absolute value equations and inequalities\\nPrimary SOL:\\nAII.3 The student will solve\\na) The student will solve absolute value equations and\\ninequalities.\\nRelated SOL:\\nAII.6\\nMaterials:\\n\\uf0b7 Absolute Value Matching Cards activity sheet (attached)\\n\\uf0b7 Absolute Value Equations/Inequalities activity sheet (attached)\\n\\uf0b7 Absolute Value Stations Review activity sheet (attached)\\n\\uf0b7 Graphing utility\\n\\uf0b7 Colored pencils or highlighters.\\nVocabulary\\nabsolute value inequality, compound inequality, compound statement, intersection, linear\\nequation, linear inequality, interval notation, union, set-builder notation, solution set\",\n",
      "        \"What should students be doing? What should teachers be doing?\\nTime: 90 minutes\\n1. Review graphing and writing inequalities with students. Have students work to fill in the\\ntable following in cooperative groups.\\nIn words\\nNumber Line\\nSet Notation\\nInterval\\nNotation\\nAll real numbers\\nless than 2\\n(-\\u221e, 2)\\n\\uf07b\\n\\uf07d\\n|\\nx x \\uf0a3\\n[2, 5)\\nAll real numbers\\ngreater than 1 or\\nless than \\u2013 5.\\n2. Invoke students\\u2019 prior learning by asking questions about absolute value. Use questions\\nand equations such as the following:\\n\\uf0b7 \\u201cWhat is absolute value?\\u201d\\n\\uf0b7 \\u201cHow can the absolute value of a number be modeled using a number line or another\\nrepresentation?\\u201d  Demonstrate representation(s) of absolute value.\\n\\uf0b7 \\u201cWhat are the solutions to\\nx \\uf03d?\\u201d\\n\\uf0b7 \\u201cWhat are some solutions to\\nx \\uf03c?\\u201d\\n\\uf0b7 \\u201cWhat are some solutions to\\nx \\uf02d\\n\\uf0b3\\n?\\u201d\\nExplain to students that the objective of the lesson is to learn two methods for solving\\nabsolute value equations and inequalities, which will give them greater insight into the\\nprocess of finding the solution set.\\nMethod 1:  Absolute Value as Distance\\n3. Discuss the statement, \\u201cAbsolute value represents distance.\\u201d Ask students whether this\\nis true, and if so, what it means. Have the class discuss\\n\\uf02d\\uf03d\\nin terms of distance.\\n4. Ask what values make\\n\\uf03d\\nx\\ntrue, and discuss this in terms of distance. Summarize\\nresponses by saying, \\u201cWe can describe the solution set as the set of points whose\\ndistance from the origin is equal to 3.\\u201d Show the solution set on a number line.\\n5. Ask what changes in the solution set when we want to solve\\n1 \\uf03d\\n\\uf02d\\nx\\n. Summarize\\nresponses by saying, \\u201cWe can describe the solution set as the set of points whose\\ndistance from 1, rather than 0, is equal to 3.\\u201d Discuss that when we have two points on a\\nnumber line, we subtract them to find the distance. So, the general form for an absolute\\nvalue uses that subtraction. It is important to show the solution set on a number line.\\n6. Discuss how the process changes if we want to solve\\nx \\uf02b\\n\\uf03d. Summarize responses by\\nsaying, \\u201cThis equations is saying, \\u2018The distance between x and -1 is 3,\\u2019 so now we\\u2019re\\nstarting at -1 and going out 3 from there.\\u201d\\n7. Ask, \\u201cIf the solution to\\n\\uf03d\\nx\\nis the set of points whose distance from the origin is \\u2018equal\\u2019\\nto 3, then what is the solution set to\\n\\uf03c\\nx\\n?\\u201d Discuss, and ask students to identify values\\nof x that make\\n\\uf03c\\nx\\ntrue. They should come to see that the solution set is the set of\\npoints whose distance from the origin is less than 3. Show the solution set on a number\\nline.\\n\\u20132\\n\\u20133\\n\\u22123\\n-1\\n\\u20134\\nAsk students how we might represent these points as a solution set. Discuss that we\\nwant the points between 3 and -3.  So, we\\u2019d want the points where\\nx \\uf03e\\uf02d and\\nx \\uf03c\\n.\\nWe can write this as the compound inequality \\uf07b\\n\\uf07d\\nx\\nx\\n\\uf02d\\uf03c\\n\\uf03c\\n, or we can use interval\\nnotation to write the solution set as (-3, 3).\\n8. Ask how the graph of this solution set would be different if you were solving\\n\\uf0a3\\nx\\n? If\\nyou were solving\\n\\uf03e\\nx\\n? If you were solving\\n\\uf0b3\\nx\\n?\\n9. Instruct students to use the distance method to solve the following examples:\\n2 \\uf0a3\\n\\uf02d\\nx\\n5 \\uf03e\\n\\uf02b\\nx\\n4 \\uf0b3\\n\\uf02d\\nx\\nb\\na\\nx\\n\\uf03c\\n\\uf02d\\nThen, have students state the solution set of each example, using the sentence frame in\\nthe box below. Finally, have students graph each solution set on a number line.\\n10. Ask how the solution set might change if we were trying to solve 2\\nx \\uf02d\\n\\uf03c. Summarize\\nthe responses by saying, \\u201cNow the equation says that the distance between 2x and 1 is\\n3.\\u201d  So, we can start at 1, go out three in each direction and these solutions would\\nrepresent 2x.\\nDivide each of the terms by two to find the solution.\\nTherefore, we can write the solution set in set-builder notation as \\uf07b\\n\\uf07d\\nx\\nx\\n\\uf02d\\uf03c\\n\\uf03c\\nor in\\ninterval notation as \\uf028\\n\\uf029\\n1,2\\n\\uf02d\\n.\\nMethod 2:  Absolute Value as Compound Inequality\\n11. Explain the method of writing absolute value equations and inequalities as compound\\nstatements. Students should make the connection quickly if this is the second method\\ndiscussed.\\n\\uf03d\\nx\\n\\uf0de\\n\\uf03d\\nx\\nOR\\n\\uf02d\\n\\uf03d\\nx\\n\\u20132\\n\\u20131\\n\\u22125\\nSentence Frame for Solution Set\\n\\u201c The solution set to a given equation or inequality is the set of points whose distance\\nfrom _____________ is (equal to, less than, etc.).\\u201d\\n\\uf03c\\nx\\n\\uf0de\\n\\uf03c\\nx\\nand\\n\\uf02d\\n\\uf03e\\nx\\n\\uf03e\\nx\\n\\uf0de\\n\\uf03e\\nx\\nor\\n\\uf02d\\n\\uf03c\\nx\\n12. Have students write the following as compound statements, solve each branch, and\\nthen graph the solution set to each:\\n2 \\uf03d\\n\\uf02b\\nx\\n\\uf0a3\\n\\uf02d\\nx\\n\\uf0b3\\n\\uf02d\\nx\\nMethod 3:  Using Graphs of Absolute Value Equations\\n13. Give students the equation\\n\\uf02d\\n\\uf02d\\uf03d\\nx\\n. Have the students graph the absolute value\\nand discuss what the solutions to the equation would be using the graph. Review the\\nidea of the zero or root of a function using the\\nequation and the graph.\\n14. Give the students the equation\\n\\uf02d\\n\\uf02d\\uf03d\\n3 1\\nx\\n.\\nDiscuss how the equation might be transformed\\nto apply the idea of the roots to solve the\\nequation.\\n15. Give the students the inequality\\n\\uf02d\\n\\uf02d\\uf03c\\n3 1\\nx\\n.\\nTransform the equation so that it would be less\\nthan zero. Have students graph\\n\\uf03d\\n\\uf02d\\n\\uf02d\\ny\\nx\\nand\\n\\uf03d0\\ny\\n. Ask, \\u201cWhere is the graph of the absolute\\nvalue less than 0?\\u201d Have students use colored\\npencils or highlighters to shade the area below the axis. Ask, \\u201cWhat are the values of x\\nthat make that true?\\u201d Students should respond with the x-values between \\u20132 and 6. So,\\n\\uf02d\\n\\uf02d\\n\\uf03c\\nx\\nor\\n\\uf02d\\n\\uf02d\\uf03c\\n3 1\\nx\\nwhen \\uf02d\\uf03c\\n\\uf03c\\nx\\n.\\n16. Give pairs of students a set of Absolute Value Matching Cards. Explain that student pairs\\nwill play a game to match an inequality with the graph of its solution set, the statement\\ndescribing its solution set, and the corresponding compound statement.\\nHave pairs shuffle their cards and deal eight cards each. In turn, each player places one\\ncard on the table. If there is a corresponding card already on the table, the player places\\nhis/her card on top of that card to create a stack; if not, the player starts a new stack. At\\nthe end of the game, six stacks should have been created. When a student places the\\nfourth card on any stack, that student collects that stack. The player with the most\\nstacks wins.\\nOnce the game becomes too repetitive with these cards, have students create their own\\ngame cards in the same manner by writing inequalities, accompanying statements\\ndescribing the solution sets, accompanying graphs, and accompanying compound\\ninequalities.\\n\\u22125\\n\\u22125\\n17. Distribute copies of the Absolute Value Equations/Inequalities activity sheet. Have the\\nstudents complete the problems using whichever method they choose. Questions 16\\u201318\\nprovide an opportunity for students to apply their understanding of absolute value\\nequations and inequalities by writing equations and inequalities for a given solution set.\\nAdditional discussion within the class may be necessary with these questions. Or, the\\nquestions can be assigned to small groups. Have groups share and explain their\\nproblems to other groups.\\n18. The Absolute Value Stations Review is designed as a review and practice session on\\nthese topics, with students assisting each other through the review process. Set up six\\nworkstations, with each station containing multiple sheets showing one set of problems.\\nBe sure that each station has enough sheets for every student to have one. Place a\\nposter at each station with the answers and full solution sets to the problems. Put\\nstudents into groups at the six stations. Give each group seven to 10 minutes per station\\nto complete the problems found there, working together and checking answers. Set a\\ntimer so that the groups know when to stop working and rotate to the next station.\",\n",
      "        \"\\uf0b7 Have students solve problems where the argument of the absolute value is quadratic.\\n\\uf0b7 Have students work in groups to reteach one of the methods, and record a video for\\nstudent review.\\n\\uf0b7 Have students develop their own mnemonic to remember the connecting compound\\nword, and or or, for each absolute value equation or inequality type.\\n\\uf0b7 Have students solve\\n\\uf0a3\\n\\uf0a3x\\n, using a distance argument.\",\n",
      "        \"\\uf0b7 Questions\\no You have noticed when you graph certain absolute value inequalities that the\\nsolution set is everything within an interval between two points, as\\ndemonstrated below:\\nIn other absolute value inequalities, the solution set is everything on the\\nnumber line outside of an interval, as demonstrated below:\\nWhat types of absolute value inequalities lead to each of these kinds of solution\\nsets? Why?\\no Given any interval between two given numbers, how can you write an absolute\\nvalue inequality to produce that interval as its solution set?\\n\\uf0b7 Journal/writing prompts\\no Discuss the two ways of solving absolute value equations and inequalities,\\nincluding which one you think makes the most sense and why.\\n\\u22125\\n\\u22125\\no Explain the steps you would use to solve an absolute value inequality, using your\\nmethod of choice.\\no Write a fictitious story about the history of absolute value.\",\n",
      "        \"\\uf0b7 When teaching the compound statement method, consider\\nusing a template like the one shown at right.\\n\\uf0b7 Provide additional sentence frames to help students\\narticulate solution sets to absolute value inequalities.\\n\\uf0b7 Restrict the number of game cards to two representations for\\nstudents who struggle with multiple representations.\\n\\uf0b7 Construct a number line on the floor, and have students\\nphysically represent distances from zero and solution sets to\\nabsolute value equations and inequalities.\\n\\uf0b7 Allow students to use talking graphing utilities as they\\ninterpret graphs and points of intersection.\"\n",
      "    ],\n",
      "    \"metadatas\": [\n",
      "        {\n",
      "            \"subject\": \"Absolute Value Equations and Inequalities\",\n",
      "            \"lesson_index\": 36,\n",
      "            \"strand\": \"Equations and Inequalities\",\n",
      "            \"topic\": \"Solving absolute value equations and inequalities\",\n",
      "            \"section\": \"intro_context\",\n",
      "            \"lesson_title\": \"Algebra 2-Equations and Inequalities-AII.3a - Absolute Value Equations and Inequalities.pdf ---\",\n",
      "            \"grade\": \"Algebra 2\"\n",
      "        },\n",
      "        {\n",
      "            \"lesson_index\": 36,\n",
      "            \"section\": \"instructional_steps\",\n",
      "            \"topic\": \"Solving absolute value equations and inequalities\",\n",
      "            \"lesson_title\": \"Algebra 2-Equations and Inequalities-AII.3a - Absolute Value Equations and Inequalities.pdf ---\",\n",
      "            \"strand\": \"Equations and Inequalities\",\n",
      "            \"subject\": \"Absolute Value Equations and Inequalities\",\n",
      "            \"grade\": \"Algebra 2\"\n",
      "        },\n",
      "        {\n",
      "            \"section\": \"extensions\",\n",
      "            \"lesson_title\": \"Algebra 2-Equations and Inequalities-AII.3a - Absolute Value Equations and Inequalities.pdf ---\",\n",
      "            \"grade\": \"Algebra 2\",\n",
      "            \"topic\": \"Solving absolute value equations and inequalities\",\n",
      "            \"subject\": \"Absolute Value Equations and Inequalities\",\n",
      "            \"strand\": \"Equations and Inequalities\",\n",
      "            \"lesson_index\": 36\n",
      "        },\n",
      "        {\n",
      "            \"section\": \"assessment\",\n",
      "            \"subject\": \"Absolute Value Equations and Inequalities\",\n",
      "            \"lesson_index\": 36,\n",
      "            \"strand\": \"Equations and Inequalities\",\n",
      "            \"grade\": \"Algebra 2\",\n",
      "            \"topic\": \"Solving absolute value equations and inequalities\",\n",
      "            \"lesson_title\": \"Algebra 2-Equations and Inequalities-AII.3a - Absolute Value Equations and Inequalities.pdf ---\"\n",
      "        },\n",
      "        {\n",
      "            \"strand\": \"Equations and Inequalities\",\n",
      "            \"subject\": \"Absolute Value Equations and Inequalities\",\n",
      "            \"topic\": \"Solving absolute value equations and inequalities\",\n",
      "            \"section\": \"differentiation\",\n",
      "            \"lesson_index\": 36,\n",
      "            \"grade\": \"Algebra 2\",\n",
      "            \"lesson_title\": \"Algebra 2-Equations and Inequalities-AII.3a - Absolute Value Equations and Inequalities.pdf ---\"\n",
      "        }\n",
      "    ],\n",
      "    \"distances\": [\n",
      "        0.5131648778915405,\n",
      "        0.6332236528396606,\n",
      "        0.6898674368858337,\n",
      "        0.7776682376861572,\n",
      "        1.0108494758605957\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# STEP 5: Querying the Lesson Plan\n",
    "# note: trial sample query to check if its working\n",
    "# -------------------------------\n",
    "import json\n",
    "from chromadb import PersistentClient\n",
    "\n",
    "query_text = \"How do I teach Absolute Value Equations and Inequalities for Algebra 2?\"\n",
    "\n",
    "metadata_filter = {\n",
    "    \"$and\": [\n",
    "        {\"grade\": \"Algebra 2\"},\n",
    "        # {\"strand\": \"Number and Number Sense\"},\n",
    "        {\"subject\": \"Absolute Value Equations and Inequalities\"}\n",
    "    ]\n",
    "}\n",
    "\n",
    "results = collection.query(\n",
    "    query_texts=[query_text],\n",
    "    n_results=5,\n",
    "    where=metadata_filter,\n",
    "    include=[\"documents\", \"metadatas\", \"distances\"]\n",
    ")\n",
    "\n",
    "flat = {\n",
    "    \"ids\": results[\"ids\"][0],\n",
    "    \"documents\": results[\"documents\"][0],\n",
    "    \"metadatas\": results[\"metadatas\"][0],\n",
    "    \"distances\": results[\"distances\"][0]\n",
    "}\n",
    "\n",
    "# Pretty-print for verification\n",
    "print(json.dumps(flat, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📚 Document count by Grade:\n",
      "\n",
      "Grade: 1                                             → Count: 144\n",
      "Grade: 2                                             → Count: 156\n",
      "Grade: 3                                             → Count: 170\n",
      "Grade: 4                                             → Count: 190\n",
      "Grade: 5                                             → Count: 112\n",
      "Grade: 6                                             → Count: 122\n",
      "Grade: 7                                             → Count: 106\n",
      "Grade: 8                                             → Count: 112\n",
      "Grade: Algebra 2                                     → Count: 112\n",
      "Grade: Algebra I                                     → Count: 146\n",
      "Grade: Algebra, Functions & Data Analysis            → Count: 78\n",
      "Grade: Geometry                                      → Count: 96\n",
      "Grade: Kindergarten                                  → Count: 128\n",
      "\n",
      "🔎 Checking for expected grades:\n",
      "⚠️  Missing: Grade 1\n",
      "⚠️  Missing: Grade 2\n",
      "⚠️  Missing: Grade 3\n",
      "⚠️  Missing: Grade 4\n",
      "⚠️  Missing: Grade 5\n",
      "⚠️  Missing: Grade 6\n",
      "⚠️  Missing: Grade 7\n",
      "⚠️  Missing: Grade 8\n"
     ]
    }
   ],
   "source": [
    "# Verify grade distribution\n",
    "sample = collection.get(include=[\"metadatas\"])\n",
    "grade_counts = {}\n",
    "\n",
    "for meta in sample[\"metadatas\"]:\n",
    "    grade = meta.get(\"grade\")\n",
    "    if grade:\n",
    "        grade_counts[grade] = grade_counts.get(grade, 0) + 1\n",
    "\n",
    "print(\"📚 Document count by Grade:\\n\")\n",
    "for grade in sorted(grade_counts):\n",
    "    print(f\"Grade: {grade:<45} → Count: {grade_counts[grade]}\")\n",
    "\n",
    "# Check if any expected grade is missing\n",
    "expected_grades = [\n",
    "    \"Kindergarten\",\n",
    "    \"Grade 1\",\n",
    "    \"Grade 2\",\n",
    "    \"Grade 3\",\n",
    "    \"Grade 4\",\n",
    "    \"Grade 5\",\n",
    "    \"Grade 6\",\n",
    "    \"Grade 7\",\n",
    "    \"Grade 8\",\n",
    "    \"Algebra I\",\n",
    "    \"Geometry\",\n",
    "    \"Algebra 2\",\n",
    "    \"Algebra, Functions & Data Analysis\"\n",
    "]\n",
    "\n",
    "print(\"\\n🔎 Checking for expected grades:\")\n",
    "for g in expected_grades:\n",
    "    if g not in grade_counts:\n",
    "        print(f\"⚠️  Missing: {g}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Algebra 2-Equations and Inequalities-AII.3a - Absolute Value Equations and Inequalities.pdf ---\n",
      "Subject:\n",
      "Absolute Value Equations and Inequalities\n",
      "Strand:\n",
      "Equations and Inequalities\n",
      "Topic:\n",
      "Solving absolute value equations and inequalities\n",
      "Primary SOL:\n",
      "AII.3 The student will solve\n",
      "a) The student will solve absolute value equations and\n",
      "inequalities.\n",
      "Related SOL:\n",
      "AII.6\n",
      "Materials:\n",
      " Absolute Value Matching Cards activity sheet (attached)\n",
      " Absolute Value Equations/Inequalities activity sheet (attached)\n",
      " Absolute Value Stations Review activity sheet (attached)\n",
      " Graphing utility\n",
      " Colored pencils or highlighters.\n",
      "Vocabulary\n",
      "absolute value inequality, compound inequality, compound statement, intersection, linear\n",
      "equation, linear inequality, interval notation, union, set-builder notation, solution set\n",
      "\n",
      "What should students be doing? What should teachers be doing?\n",
      "Time: 90 minutes\n",
      "1. Review graphing and writing inequalities with students. Have students work to fill in the\n",
      "table following in cooperative groups.\n",
      "In words\n",
      "Number Line\n",
      "Set Notation\n",
      "Interval\n",
      "Notation\n",
      "All real numbers\n",
      "less than 2\n",
      "(-∞, 2)\n",
      "\n",
      "\n",
      "|\n",
      "x x \n",
      "[2, 5)\n",
      "All real numbers\n",
      "greater than 1 or\n",
      "less than – 5.\n",
      "2. Invoke students’ prior learning by asking questions about absolute value. Use questions\n",
      "and equations such as the following:\n",
      " “What is absolute value?”\n",
      " “How can the absolute value of a number be modeled using a number line or another\n",
      "representation?”  Demonstrate representation(s) of absolute value.\n",
      " “What are the solutions to\n",
      "x ?”\n",
      " “What are some solutions to\n",
      "x ?”\n",
      " “What are some solutions to\n",
      "x \n",
      "\n",
      "?”\n",
      "Explain to students that the objective of the lesson is to learn two methods for solving\n",
      "absolute value equations and inequalities, which will give them greater insight into the\n",
      "process of finding the solution set.\n",
      "Method 1:  Absolute Value as Distance\n",
      "3. Discuss the statement, “Absolute value represents distance.” Ask students whether this\n",
      "is true, and if so, what it means. Have the class discuss\n",
      "\n",
      "in terms of distance.\n",
      "4. Ask what values make\n",
      "\n",
      "x\n",
      "true, and discuss this in terms of distance. Summarize\n",
      "responses by saying, “We can describe the solution set as the set of points whose\n",
      "distance from the origin is equal to 3.” Show the solution set on a number line.\n",
      "5. Ask what changes in the solution set when we want to solve\n",
      "1 \n",
      "\n",
      "x\n",
      ". Summarize\n",
      "responses by saying, “We can describe the solution set as the set of points whose\n",
      "distance from 1, rather than 0, is equal to 3.” Discuss that when we have two points on a\n",
      "number line, we subtract them to find the distance. So, the general form for an absolute\n",
      "value uses that subtraction. It is important to show the solution set on a number line.\n",
      "6. Discuss how the process changes if we want to solve\n",
      "x \n",
      ". Summarize responses by\n",
      "saying, “This equations is saying, ‘The distance between x and -1 is 3,’ so now we’re\n",
      "starting at -1 and going out 3 from there.”\n",
      "7. Ask, “If the solution to\n",
      "\n",
      "x\n",
      "is the set of points whose distance from the origin is ‘equal’\n",
      "to 3, then what is the solution set to\n",
      "\n",
      "x\n",
      "?” Discuss, and ask students to identify values\n",
      "of x that make\n",
      "\n",
      "x\n",
      "true. They should come to see that the solution set is the set of\n",
      "points whose distance from the origin is less than 3. Show the solution set on a number\n",
      "line.\n",
      "–2\n",
      "–3\n",
      "−3\n",
      "-1\n",
      "–4\n",
      "Ask students how we might represent these points as a solution set. Discuss that we\n",
      "want the points between 3 and -3.  So, we’d want the points where\n",
      "x  and\n",
      "x \n",
      ".\n",
      "We can write this as the compound inequality \n",
      "\n",
      "x\n",
      "x\n",
      "\n",
      "\n",
      ", or we can use interval\n",
      "notation to write the solution set as (-3, 3).\n",
      "8. Ask how the graph of this solution set would be different if you were solving\n",
      "\n",
      "x\n",
      "? If\n",
      "you were solving\n",
      "\n",
      "x\n",
      "? If you were solving\n",
      "\n",
      "x\n",
      "?\n",
      "9. Instruct students to use the distance method to solve the following examples:\n",
      "2 \n",
      "\n",
      "x\n",
      "5 \n",
      "\n",
      "x\n",
      "4 \n",
      "\n",
      "x\n",
      "b\n",
      "a\n",
      "x\n",
      "\n",
      "\n",
      "Then, have students state the solution set of each example, using the sentence frame in\n",
      "the box below. Finally, have students graph each solution set on a number line.\n",
      "10. Ask how the solution set might change if we were trying to solve 2\n",
      "x \n",
      ". Summarize\n",
      "the responses by saying, “Now the equation says that the distance between 2x and 1 is\n",
      "3.”  So, we can start at 1, go out three in each direction and these solutions would\n",
      "represent 2x.\n",
      "Divide each of the terms by two to find the solution.\n",
      "Therefore, we can write the solution set in set-builder notation as \n",
      "\n",
      "x\n",
      "x\n",
      "\n",
      "\n",
      "or in\n",
      "interval notation as \n",
      "\n",
      "1,2\n",
      "\n",
      ".\n",
      "Method 2:  Absolute Value as Compound Inequality\n",
      "11. Explain the method of writing absolute value equations and inequalities as compound\n",
      "statements. Students should make the connection quickly if this is the second method\n",
      "discussed.\n",
      "\n",
      "x\n",
      "\n",
      "\n",
      "x\n",
      "OR\n",
      "\n",
      "\n",
      "x\n",
      "–2\n",
      "–1\n",
      "−5\n",
      "Sentence Frame for Solution Set\n",
      "“ The solution set to a given equation or inequality is the set of points whose distance\n",
      "from _____________ is (equal to, less than, etc.).”\n",
      "\n",
      "x\n",
      "\n",
      "\n",
      "x\n",
      "and\n",
      "\n",
      "\n",
      "x\n",
      "\n",
      "x\n",
      "\n",
      "\n",
      "x\n",
      "or\n",
      "\n",
      "\n",
      "x\n",
      "12. Have students write the following as compound statements, solve each branch, and\n",
      "then graph the solution set to each:\n",
      "2 \n",
      "\n",
      "x\n",
      "\n",
      "\n",
      "x\n",
      "\n",
      "\n",
      "x\n",
      "Method 3:  Using Graphs of Absolute Value Equations\n",
      "13. Give students the equation\n",
      "\n",
      "\n",
      "x\n",
      ". Have the students graph the absolute value\n",
      "and discuss what the solutions to the equation would be using the graph. Review the\n",
      "idea of the zero or root of a function using the\n",
      "equation and the graph.\n",
      "14. Give the students the equation\n",
      "\n",
      "\n",
      "3 1\n",
      "x\n",
      ".\n",
      "Discuss how the equation might be transformed\n",
      "to apply the idea of the roots to solve the\n",
      "equation.\n",
      "15. Give the students the inequality\n",
      "\n",
      "\n",
      "3 1\n",
      "x\n",
      ".\n",
      "Transform the equation so that it would be less\n",
      "than zero. Have students graph\n",
      "\n",
      "\n",
      "\n",
      "y\n",
      "x\n",
      "and\n",
      "0\n",
      "y\n",
      ". Ask, “Where is the graph of the absolute\n",
      "value less than 0?” Have students use colored\n",
      "pencils or highlighters to shade the area below the axis. Ask, “What are the values of x\n",
      "that make that true?” Students should respond with the x-values between –2 and 6. So,\n",
      "\n",
      "\n",
      "\n",
      "x\n",
      "or\n",
      "\n",
      "\n",
      "3 1\n",
      "x\n",
      "when \n",
      "\n",
      "x\n",
      ".\n",
      "16. Give pairs of students a set of Absolute Value Matching Cards. Explain that student pairs\n",
      "will play a game to match an inequality with the graph of its solution set, the statement\n",
      "describing its solution set, and the corresponding compound statement.\n",
      "Have pairs shuffle their cards and deal eight cards each. In turn, each player places one\n",
      "card on the table. If there is a corresponding card already on the table, the player places\n",
      "his/her card on top of that card to create a stack; if not, the player starts a new stack. At\n",
      "the end of the game, six stacks should have been created. When a student places the\n",
      "fourth card on any stack, that student collects that stack. The player with the most\n",
      "stacks wins.\n",
      "Once the game becomes too repetitive with these cards, have students create their own\n",
      "game cards in the same manner by writing inequalities, accompanying statements\n",
      "describing the solution sets, accompanying graphs, and accompanying compound\n",
      "inequalities.\n",
      "−5\n",
      "−5\n",
      "17. Distribute copies of the Absolute Value Equations/Inequalities activity sheet. Have the\n",
      "students complete the problems using whichever method they choose. Questions 16–18\n",
      "provide an opportunity for students to apply their understanding of absolute value\n",
      "equations and inequalities by writing equations and inequalities for a given solution set.\n",
      "Additional discussion within the class may be necessary with these questions. Or, the\n",
      "questions can be assigned to small groups. Have groups share and explain their\n",
      "problems to other groups.\n",
      "18. The Absolute Value Stations Review is designed as a review and practice session on\n",
      "these topics, with students assisting each other through the review process. Set up six\n",
      "workstations, with each station containing multiple sheets showing one set of problems.\n",
      "Be sure that each station has enough sheets for every student to have one. Place a\n",
      "poster at each station with the answers and full solution sets to the problems. Put\n",
      "students into groups at the six stations. Give each group seven to 10 minutes per station\n",
      "to complete the problems found there, working together and checking answers. Set a\n",
      "timer so that the groups know when to stop working and rotate to the next station.\n",
      "\n",
      " Have students solve problems where the argument of the absolute value is quadratic.\n",
      " Have students work in groups to reteach one of the methods, and record a video for\n",
      "student review.\n",
      " Have students develop their own mnemonic to remember the connecting compound\n",
      "word, and or or, for each absolute value equation or inequality type.\n",
      " Have students solve\n",
      "\n",
      "x\n",
      ", using a distance argument.\n",
      "\n",
      " Questions\n",
      "o You have noticed when you graph certain absolute value inequalities that the\n",
      "solution set is everything within an interval between two points, as\n",
      "demonstrated below:\n",
      "In other absolute value inequalities, the solution set is everything on the\n",
      "number line outside of an interval, as demonstrated below:\n",
      "What types of absolute value inequalities lead to each of these kinds of solution\n",
      "sets? Why?\n",
      "o Given any interval between two given numbers, how can you write an absolute\n",
      "value inequality to produce that interval as its solution set?\n",
      " Journal/writing prompts\n",
      "o Discuss the two ways of solving absolute value equations and inequalities,\n",
      "including which one you think makes the most sense and why.\n",
      "−5\n",
      "−5\n",
      "o Explain the steps you would use to solve an absolute value inequality, using your\n",
      "method of choice.\n",
      "o Write a fictitious story about the history of absolute value.\n",
      "\n",
      " When teaching the compound statement method, consider\n",
      "using a template like the one shown at right.\n",
      " Provide additional sentence frames to help students\n",
      "articulate solution sets to absolute value inequalities.\n",
      " Restrict the number of game cards to two representations for\n",
      "students who struggle with multiple representations.\n",
      " Construct a number line on the floor, and have students\n",
      "physically represent distances from zero and solution sets to\n",
      "absolute value equations and inequalities.\n",
      " Allow students to use talking graphing utilities as they\n",
      "interpret graphs and points of intersection.\n"
     ]
    }
   ],
   "source": [
    "# note: Just for confirmation we are printing full lesson plan\n",
    "# Concatenate all 5 documents into a full lesson text if we want to use it as a single lesson\n",
    "full_lesson = \"\\n\\n\".join(flat[\"documents\"])\n",
    "\n",
    "print(full_lesson)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chunking with Subject - OLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/myenv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "import uuid\n",
    "from typing import List\n",
    "from langchain.text_splitter import TextSplitter\n",
    "from langchain.docstore.document import Document\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "\n",
    "# -------------------------------\n",
    "# 1. Our own Custom TextSplitter \n",
    "# note: we are using Hierarchical Chunking\n",
    "# -------------------------------\n",
    "class LessonPlanTextSplitter(TextSplitter):\n",
    "    def split_text(self, text: str) -> List[Document]:\n",
    "        stop_marker = r\"Note: The following pages are intended for classroom use for students as a visual aid to learning\\.\"\n",
    "        split_block = re.split(stop_marker, text, maxsplit=1)\n",
    "        content = split_block[0].strip() if split_block else text.strip()\n",
    "\n",
    "        # Split into intro and remainder\n",
    "        parts = re.split(r\"Student/Teacher Actions:\\s*\", content, flags=re.IGNORECASE, maxsplit=1)\n",
    "        if len(parts) == 2:\n",
    "            intro, remainder = parts[0].strip(), parts[1].strip()\n",
    "        else:\n",
    "            intro, remainder = content, \"\"\n",
    "\n",
    "        docs = []\n",
    "        docs.append(Document(page_content=intro, metadata={\"section\": \"intro_context\"}))\n",
    "\n",
    "        combined_pattern = r\"(Assessment\\s*\\n|Extensions(?: and Connections)?\\s*\\n|Strategies for Differentiation\\s*\\n)\"\n",
    "        split_sec = re.split(combined_pattern, remainder)\n",
    "\n",
    "        # Instructional steps (first segment)\n",
    "        instr = split_sec[0].strip()\n",
    "        if instr:\n",
    "            docs.append(Document(page_content=instr, metadata={\"section\": \"instructional_steps\"}))\n",
    "\n",
    "        # Subsequent header/content pairs\n",
    "        for i in range(1, len(split_sec) - 1, 2):\n",
    "            header = split_sec[i].strip().lower()\n",
    "            content = split_sec[i + 1].strip()\n",
    "            if header.startswith(\"assessment\"):\n",
    "                docs.append(Document(page_content=content, metadata={\"section\": \"assessment\"}))\n",
    "            elif header.startswith(\"extensions\"):\n",
    "                docs.append(Document(page_content=content, metadata={\"section\": \"extensions\"}))\n",
    "            elif header.startswith(\"strategies for differentiation\"):\n",
    "                docs.append(Document(page_content=content, metadata={\"section\": \"differentiation\"}))\n",
    "\n",
    "        return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# 2. Chunk the Lesson Plans \n",
    "# note: here our main file is test_with_notes.txt where all our lesson plan is stored\n",
    "# -------------------------------\n",
    "with open(\"test_with_notes.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    full_text = f.read()\n",
    "\n",
    "# Split into individual lesson plan blocks\n",
    "lesson_plan_blocks = re.findall(\n",
    "    r\"--- Start of Lesson Plan(.*?)--- End of Lesson Plan\",\n",
    "    full_text,\n",
    "    re.DOTALL\n",
    ")\n",
    "\n",
    "splitter = LessonPlanTextSplitter()\n",
    "all_docs: List[Document] = []\n",
    "\n",
    "for lesson_index, block in enumerate(lesson_plan_blocks):\n",
    "    docs = splitter.split_text(block)\n",
    "    # Tag each chunk with its lesson index\n",
    "    for doc in docs:\n",
    "        doc.metadata[\"lesson_index\"] = lesson_index\n",
    "    all_docs.extend(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# 3. Extract Lesson-Level Metadata original\n",
    "# note: here our metda data is split based on lesson index and then we have grade, subject, topic and lesson title\n",
    "# -------------------------------\n",
    "def extract_metadata_from_intro(intro: str):\n",
    "    grade = re.search(r\"Grade\\s*(\\d+)\", intro)\n",
    "    subject = re.search(r\"Strand:\\s*(.+)\", intro)\n",
    "    topic = re.search(r\"Topic:\\s*(.+)\", intro)\n",
    "    title = intro.split(\"\\n\")[0].strip()\n",
    "    return {\n",
    "        \"grade\": grade.group(1) if grade else None,\n",
    "        \"subject\": subject.group(1).strip() if subject else None,\n",
    "        \"topic\": topic.group(1).strip() if topic else None,\n",
    "        \"lesson_title\": title\n",
    "    }\n",
    "\n",
    "# Build map: lesson_index -> metadata\n",
    "lesson_metadata_map = {}\n",
    "for doc in all_docs:\n",
    "    if doc.metadata[\"section\"] == \"intro_context\":\n",
    "        lesson_metadata_map[doc.metadata[\"lesson_index\"]] = extract_metadata_from_intro(doc.page_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding complete. All chunks stored with full metadata.\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from chromadb import PersistentClient\n",
    "\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# client = PersistentClient(path=\"./chroma_store1\")\n",
    "client = PersistentClient(path=\"./chroma_store\")\n",
    "collection = client.get_or_create_collection(\"lesson_plans\")\n",
    "\n",
    "# Fetch existing IDs\n",
    "existing_ids = set(collection.get()[\"ids\"])\n",
    "\n",
    "for doc in all_docs:\n",
    "    lm = lesson_metadata_map.get(doc.metadata[\"lesson_index\"], {})\n",
    "    doc.metadata.update(lm)\n",
    "    cleaned_metadata = {k: v for k, v in doc.metadata.items() if v is not None}\n",
    "\n",
    "    doc_id = f\"lesson{cleaned_metadata['lesson_index']}_{cleaned_metadata['section']}\"\n",
    "    if doc_id in existing_ids:\n",
    "        continue\n",
    "\n",
    "    embedding = model.encode(doc.page_content).tolist()\n",
    "    collection.add(\n",
    "        documents=[doc.page_content],\n",
    "        metadatas=[cleaned_metadata],\n",
    "        embeddings=[embedding],\n",
    "        ids=[doc_id]\n",
    "    )\n",
    "    existing_ids.add(doc_id)\n",
    "\n",
    "print(\"Embedding complete. All chunks stored with full metadata.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"ids\": [\n",
      "        \"lesson123_assessment\",\n",
      "        \"lesson123_intro_context\",\n",
      "        \"lesson123_differentiation\",\n",
      "        \"lesson123_instructional_steps\"\n",
      "    ],\n",
      "    \"documents\": [\n",
      "        \"\\uf0b7 Questions\\no Why do I have more groups when I count by twos than when I count by tens?\\no What do you notice about the numbers when we count by \\u2026\\n\\uf0a7 Twos? (It is every other number)\\n\\uf0a7 Fives? (Ends in a zero or a 5)\\n\\uf0a7 Tens (Ends in a zero)\\u201d\\n\\uf0b7 Journal/writing prompts\\no Given a collection of 50 objects, how many groups of twos, fives, and tens would\\nI have? Show your work.\\no How are counting by fives and tens alike and different.  Is it faster to count by\\nfives or tens? Why?\\u201d\\n\\uf0b7 Other Assessments\\no Give students a collection of 110 objects and ask them to count them in a way\\nthat would be the fastest.\\no Using a blank 110 chart, ask students to fill in only the spots for the twos.  Have\\nstudents erase and repeat for fives and tens.\\nExtensions and Connections (for all students)\\n\\uf0b7 Show students how to skip count on a calculator.  To skip count by twos, clear the\\ncalculator and then enter + 2 = and continue pressing =.  Students can color the\\nnumbers shown on the calculator display on a 110 chart.\\n\\uf0b7 Place the count-by-two numbers on cards.  Mix up the cards and have students put\\nthem in order.\\n\\uf0b7 Play \\u201cCatch the Mistake.\\u201d  Count out a collection of objects by twos, fives, or tens but\\nmake a mistake.  Students raise their hands as soon as they hear your mistake and\\nexplain the mistake that was made.  Include mistakes such as counting by twos but only\\ntouching or moving one object at a time.\\n\\uf0b7 Involve students in whole-class counting exercises, where each student says a number in\\nturn.  Begin with zero and ask students to count by twos, fives, or tens.\",\n",
      "        \"Grade 1-Number and Number Sense-1.1d - Grouping and Counting\\u2013Part 2.pdf ---\\nSubject:\\nGrouping and Counting\\u2013Part 2\\nStrand:\\nNumber and Number Sense\\nTopic:\\nCounting and skip counting\\nPrimary SOL:\\n1.1 The student will\\nd) count forward orally by ones, twos, fives and tens to determine\\nthe total number of objects up to 110.\\nRelated SOL:\\n1.1a, 1.1b, 1.5a\\nMaterials\\n\\uf0b7\\n4\\u20136 bags, each containing 63 small objects\\n\\uf0b7\\nMultiple collections of manipulatives (e.g., shells, beans, buttons, jewels, beads,\\nmarbles, small erasers, plastic linking cubes) small enough to fit on the counting mats\\n\\uf0b7\\nTeacher Center Recording Sheet (attached)\\n\\uf0b7\\nGrouping Collections task cards for student center (attached)\\n\\uf0b7\\nGrouping Collections Recording Sheet (attached)\\n\\uf0b7\\nCounting mats, with and without numbers (attached)\\nVocabulary\\ncount, fives, forward, group, skip count, tens, total, twos\",\n",
      "        \"\\uf0b7 Use the counting mats with the numbers to provide support for students who are not\\ntotally comfortable with the rote counting sequence.\\n\\uf0b7 Level task cards based on students\\u2019 instructional levels.\\n\\uf0b7 Provide students with sentence starters to use during small-group discussions with the\\nteacher (e.g., \\u201cI noticed \\u2026,\\u201d \\u201cWe counted this way \\u2026,\\u201d \\u201cMy strategy was similar to yours\\nbecause \\u2026\\u201d).\",\n",
      "        \"What should students be doing? What should teachers be doing?\\nNote: You may want to teach this lesson over two days, so you have time to meet with each\\nstudent in a small group to assess their understanding.   Divide your class into four groups so\\nthat you can meet with two groups each day. While you are working with each small group,\\nother students will be working on the task cards.  Before beginning this activity, prepare the\\nbags to be used at the teacher center. You will need 4\\u20136 bags for your teacher center depending\\non how many students you have in your groups.  Each bag should contain 63 objects.\\n1. Remind students about what they learned in the previous lesson. Ask, \\u201cHow did we use\\nskip counting to help us count a large collection of objects?\\u201d  Engage students to skip\\ncount along with the teacher by twos, fives, and tens.\\n2. Show students the Group Collections task cards and model how to use them at the\\nstudent center.  Task cards should be placed in a shared basket, and each student\\nshould have their own Group Collections Recording Sheet.  Collections of manipulatives\\nshould be available for student use. Model how to select one card from the basket and\\nchoose a set of manipulatives.  Model counting out the number of objects by ones, and\\nthen grouping and counting according to the directions on the task card. Show how to\\nuse the attached counting mats, if needed.  Students can place their objects into groups\\nonto the counting mat to help organize their material.  The counting mats with the\\nnumbers can provide support for students who are not totally comfortable with the rote\\ncounting sequence.  Finally, model how to fill in the recording sheet. Show how to take\\nthe task card back to the basket and choose a different one that has not already been\\ncompleted. Explain that students will complete as many task cards as they can during\\nyour rotation time.\\n3. Inform students that a small group will work at the teacher\\u2019s table.  All students will\\neventually have a chance to work at the teacher\\u2019s table. Plan to work with two groups\\non the first day and two groups on the second day.\\n4. Assign students not working at the teacher\\u2019s table to work on the task cards and record\\ntheir answers on the recording sheet.  The teacher may choose to have several sets of\\nthe task cards placed around the room for groups of students to share.\\n5. At the teacher\\u2019s table, give each student a brown bag filled with 63 manipulatives.  Ask\\nthe students to empty the bag on the table and assign some students to group and\\ncount their objects by twos, some to group and count by fives, and some to group and\\ncount by tens.  Allow students to use the counting mats to help organize their\\ngroups.   Instruct students to count their groups and record their answers on the\\nTeacher Center Recording Sheet.  This is an opportunity for formative assessment. Take\\nnote of the students who are having trouble keeping track of counting each group.  Note\\nwho has trouble with the rote counting sequence. Note who has trouble with counting\\nthe leftovers.  If time allows, let students group and count a different way.\\n6. Engage the students at the teacher\\u2019s group in a small-group discussion about how many\\nwere counted, the groups that were counted by, and the number of groups that were\\nmade.  Be sure to discuss that even though different groupings were used, the total\\nnumber of objects stayed the same. Especially discuss the leftovers and how they were\\ncounted.\\n7. Inform students that if they did not get a chance to work with the teacher today, they\\nwill work with the teacher the next day.  Bring students back together for a class\\ndiscussion.  Ask: \\u201cWhen you were skip counting, did all of your groups come out without\\nany leftovers?  What should we do with the leftovers when we are skip counting by twos,\\nfives, or tens?\\u201d\"\n",
      "    ],\n",
      "    \"metadatas\": [\n",
      "        {\n",
      "            \"lesson_title\": \"Grade 1-Number and Number Sense-1.1d - Grouping and Counting\\u2013Part 2.pdf ---\",\n",
      "            \"subject\": \"Grouping and Counting\\u2013Part 2\",\n",
      "            \"section\": \"assessment\",\n",
      "            \"topic\": \"Counting and skip counting\",\n",
      "            \"lesson_index\": 123,\n",
      "            \"strand\": \"Number and Number Sense\",\n",
      "            \"grade\": \"1\"\n",
      "        },\n",
      "        {\n",
      "            \"lesson_index\": 123,\n",
      "            \"topic\": \"Counting and skip counting\",\n",
      "            \"grade\": \"1\",\n",
      "            \"subject\": \"Grouping and Counting\\u2013Part 2\",\n",
      "            \"section\": \"intro_context\",\n",
      "            \"lesson_title\": \"Grade 1-Number and Number Sense-1.1d - Grouping and Counting\\u2013Part 2.pdf ---\",\n",
      "            \"strand\": \"Number and Number Sense\"\n",
      "        },\n",
      "        {\n",
      "            \"grade\": \"1\",\n",
      "            \"lesson_title\": \"Grade 1-Number and Number Sense-1.1d - Grouping and Counting\\u2013Part 2.pdf ---\",\n",
      "            \"topic\": \"Counting and skip counting\",\n",
      "            \"subject\": \"Grouping and Counting\\u2013Part 2\",\n",
      "            \"lesson_index\": 123,\n",
      "            \"section\": \"differentiation\",\n",
      "            \"strand\": \"Number and Number Sense\"\n",
      "        },\n",
      "        {\n",
      "            \"topic\": \"Counting and skip counting\",\n",
      "            \"lesson_title\": \"Grade 1-Number and Number Sense-1.1d - Grouping and Counting\\u2013Part 2.pdf ---\",\n",
      "            \"strand\": \"Number and Number Sense\",\n",
      "            \"section\": \"instructional_steps\",\n",
      "            \"lesson_index\": 123,\n",
      "            \"subject\": \"Grouping and Counting\\u2013Part 2\",\n",
      "            \"grade\": \"1\"\n",
      "        }\n",
      "    ],\n",
      "    \"distances\": [\n",
      "        1.0274051427841187,\n",
      "        1.0730910301208496,\n",
      "        1.078939437866211,\n",
      "        1.3241400718688965\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# STEP 5: Querying the Lesson Plan\n",
    "# note: trial sample query to check if its working\n",
    "# -------------------------------\n",
    "import json\n",
    "from chromadb import PersistentClient\n",
    "\n",
    "query_text = \"How do I teach decimal rounding in number and number sense for grade 5?\"\n",
    "\n",
    "metadata_filter = {\n",
    "    \"$and\": [\n",
    "        {\"grade\": \"1\"},\n",
    "        {\"strand\": \"Number and Number Sense\"},\n",
    "        {\"subject\": \"Grouping and Counting–Part 2\"}\n",
    "    ]\n",
    "}\n",
    "\n",
    "results = collection.query(\n",
    "    query_texts=[query_text],\n",
    "    n_results=5,\n",
    "    where=metadata_filter,\n",
    "    include=[\"documents\", \"metadatas\", \"distances\"]\n",
    ")\n",
    "\n",
    "flat = {\n",
    "    \"ids\": results[\"ids\"][0],\n",
    "    \"documents\": results[\"documents\"][0],\n",
    "    \"metadatas\": results[\"metadatas\"][0],\n",
    "    \"distances\": results[\"distances\"][0]\n",
    "}\n",
    "\n",
    "# Pretty-print for verification\n",
    "print(json.dumps(flat, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Questions\n",
      "o Why do I have more groups when I count by twos than when I count by tens?\n",
      "o What do you notice about the numbers when we count by …\n",
      " Twos? (It is every other number)\n",
      " Fives? (Ends in a zero or a 5)\n",
      " Tens (Ends in a zero)”\n",
      " Journal/writing prompts\n",
      "o Given a collection of 50 objects, how many groups of twos, fives, and tens would\n",
      "I have? Show your work.\n",
      "o How are counting by fives and tens alike and different.  Is it faster to count by\n",
      "fives or tens? Why?”\n",
      " Other Assessments\n",
      "o Give students a collection of 110 objects and ask them to count them in a way\n",
      "that would be the fastest.\n",
      "o Using a blank 110 chart, ask students to fill in only the spots for the twos.  Have\n",
      "students erase and repeat for fives and tens.\n",
      "Extensions and Connections (for all students)\n",
      " Show students how to skip count on a calculator.  To skip count by twos, clear the\n",
      "calculator and then enter + 2 = and continue pressing =.  Students can color the\n",
      "numbers shown on the calculator display on a 110 chart.\n",
      " Place the count-by-two numbers on cards.  Mix up the cards and have students put\n",
      "them in order.\n",
      " Play “Catch the Mistake.”  Count out a collection of objects by twos, fives, or tens but\n",
      "make a mistake.  Students raise their hands as soon as they hear your mistake and\n",
      "explain the mistake that was made.  Include mistakes such as counting by twos but only\n",
      "touching or moving one object at a time.\n",
      " Involve students in whole-class counting exercises, where each student says a number in\n",
      "turn.  Begin with zero and ask students to count by twos, fives, or tens.\n",
      "\n",
      "Grade 1-Number and Number Sense-1.1d - Grouping and Counting–Part 2.pdf ---\n",
      "Subject:\n",
      "Grouping and Counting–Part 2\n",
      "Strand:\n",
      "Number and Number Sense\n",
      "Topic:\n",
      "Counting and skip counting\n",
      "Primary SOL:\n",
      "1.1 The student will\n",
      "d) count forward orally by ones, twos, fives and tens to determine\n",
      "the total number of objects up to 110.\n",
      "Related SOL:\n",
      "1.1a, 1.1b, 1.5a\n",
      "Materials\n",
      "\n",
      "4–6 bags, each containing 63 small objects\n",
      "\n",
      "Multiple collections of manipulatives (e.g., shells, beans, buttons, jewels, beads,\n",
      "marbles, small erasers, plastic linking cubes) small enough to fit on the counting mats\n",
      "\n",
      "Teacher Center Recording Sheet (attached)\n",
      "\n",
      "Grouping Collections task cards for student center (attached)\n",
      "\n",
      "Grouping Collections Recording Sheet (attached)\n",
      "\n",
      "Counting mats, with and without numbers (attached)\n",
      "Vocabulary\n",
      "count, fives, forward, group, skip count, tens, total, twos\n",
      "\n",
      " Use the counting mats with the numbers to provide support for students who are not\n",
      "totally comfortable with the rote counting sequence.\n",
      " Level task cards based on students’ instructional levels.\n",
      " Provide students with sentence starters to use during small-group discussions with the\n",
      "teacher (e.g., “I noticed …,” “We counted this way …,” “My strategy was similar to yours\n",
      "because …”).\n",
      "\n",
      "What should students be doing? What should teachers be doing?\n",
      "Note: You may want to teach this lesson over two days, so you have time to meet with each\n",
      "student in a small group to assess their understanding.   Divide your class into four groups so\n",
      "that you can meet with two groups each day. While you are working with each small group,\n",
      "other students will be working on the task cards.  Before beginning this activity, prepare the\n",
      "bags to be used at the teacher center. You will need 4–6 bags for your teacher center depending\n",
      "on how many students you have in your groups.  Each bag should contain 63 objects.\n",
      "1. Remind students about what they learned in the previous lesson. Ask, “How did we use\n",
      "skip counting to help us count a large collection of objects?”  Engage students to skip\n",
      "count along with the teacher by twos, fives, and tens.\n",
      "2. Show students the Group Collections task cards and model how to use them at the\n",
      "student center.  Task cards should be placed in a shared basket, and each student\n",
      "should have their own Group Collections Recording Sheet.  Collections of manipulatives\n",
      "should be available for student use. Model how to select one card from the basket and\n",
      "choose a set of manipulatives.  Model counting out the number of objects by ones, and\n",
      "then grouping and counting according to the directions on the task card. Show how to\n",
      "use the attached counting mats, if needed.  Students can place their objects into groups\n",
      "onto the counting mat to help organize their material.  The counting mats with the\n",
      "numbers can provide support for students who are not totally comfortable with the rote\n",
      "counting sequence.  Finally, model how to fill in the recording sheet. Show how to take\n",
      "the task card back to the basket and choose a different one that has not already been\n",
      "completed. Explain that students will complete as many task cards as they can during\n",
      "your rotation time.\n",
      "3. Inform students that a small group will work at the teacher’s table.  All students will\n",
      "eventually have a chance to work at the teacher’s table. Plan to work with two groups\n",
      "on the first day and two groups on the second day.\n",
      "4. Assign students not working at the teacher’s table to work on the task cards and record\n",
      "their answers on the recording sheet.  The teacher may choose to have several sets of\n",
      "the task cards placed around the room for groups of students to share.\n",
      "5. At the teacher’s table, give each student a brown bag filled with 63 manipulatives.  Ask\n",
      "the students to empty the bag on the table and assign some students to group and\n",
      "count their objects by twos, some to group and count by fives, and some to group and\n",
      "count by tens.  Allow students to use the counting mats to help organize their\n",
      "groups.   Instruct students to count their groups and record their answers on the\n",
      "Teacher Center Recording Sheet.  This is an opportunity for formative assessment. Take\n",
      "note of the students who are having trouble keeping track of counting each group.  Note\n",
      "who has trouble with the rote counting sequence. Note who has trouble with counting\n",
      "the leftovers.  If time allows, let students group and count a different way.\n",
      "6. Engage the students at the teacher’s group in a small-group discussion about how many\n",
      "were counted, the groups that were counted by, and the number of groups that were\n",
      "made.  Be sure to discuss that even though different groupings were used, the total\n",
      "number of objects stayed the same. Especially discuss the leftovers and how they were\n",
      "counted.\n",
      "7. Inform students that if they did not get a chance to work with the teacher today, they\n",
      "will work with the teacher the next day.  Bring students back together for a class\n",
      "discussion.  Ask: “When you were skip counting, did all of your groups come out without\n",
      "any leftovers?  What should we do with the leftovers when we are skip counting by twos,\n",
      "fives, or tens?”\n"
     ]
    }
   ],
   "source": [
    "# note: Just for confirmation we are printing full lesson plan\n",
    "# Concatenate all 5 documents into a full lesson text if we want to use it as a single lesson\n",
    "full_lesson = \"\\n\\n\".join(flat[\"documents\"])\n",
    "\n",
    "print(full_lesson)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chunking textbook using Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n1. Processed using both recursive (and optionally semantic) chunking approaches.\\n\\n2. Chunked further into smaller text pieces with overlap to preserve context.\\n\\n3.Embedded and stored with metadata (e.g., executive_skill and section_title).\\n\\n4.Retrieval is performed using metadata filters such as \"executive_skill\": \"Working Memory\"\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # High Level explanation\n",
    "\"\"\"\n",
    "1. Processed using both recursive (and optionally semantic) chunking approaches.\n",
    "\n",
    "2. Chunked further into smaller text pieces with overlap to preserve context.\n",
    "\n",
    "3.Embedded and stored with metadata (e.g., executive_skill and section_title).\n",
    "\n",
    "4.Retrieval is performed using metadata filters such as \"executive_skill\": \"Working Memory\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (0.3.23)\n",
      "Requirement already satisfied: langchain-experimental in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (0.3.4)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.51 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from langchain) (0.3.51)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from langchain) (0.3.8)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from langchain) (0.3.19)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from langchain) (2.11.3)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from langchain) (2.0.39)\n",
      "Requirement already satisfied: requests<3,>=2 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from langchain) (4.0.3)\n",
      "Requirement already satisfied: langchain-community<0.4.0,>=0.3.0 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from langchain-experimental) (0.3.20)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental) (3.11.14)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental) (9.0.0)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental) (2.8.1)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental) (0.4.0)\n",
      "Requirement already satisfied: numpy<3,>=1.26.2 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental) (2.0.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from langchain-core<1.0.0,>=0.3.51->langchain) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from langchain-core<1.0.0,>=0.3.51->langchain) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from langchain-core<1.0.0,>=0.3.51->langchain) (4.12.2)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.16)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.1 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from requests<3,>=2->langchain) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from requests<3,>=2->langchain) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from requests<3,>=2->langchain) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from requests<3,>=2->langchain) (2025.1.31)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (6.2.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (1.18.3)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (0.9.0)\n",
      "Requirement already satisfied: anyio in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (4.9.0)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.51->langchain) (3.0.0)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (1.1.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (1.0.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.2.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.3.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U langchain langchain-experimental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sections: 24\n",
      "Section 1: introduction\n",
      "Section 2: end of introduction\n",
      "Section 3: Building Response Inhibition\n",
      "Section 4: end of Building Response Inhibition\n",
      "Section 5: Enhancing Working Memory\n",
      "Section 6: end of Enhancing Working Memory\n",
      "Section 7: Improving Emotional Control\n",
      "Section 8: end of Improving Emotional Control\n",
      "Section 9: Strengthening Sustained Attention\n",
      "Section 10: end of Strengthening Sustained Attention\n",
      "Section 11: Teaching Task Initiation\n",
      "Section 12: end of Teaching Task Initiation\n",
      "Section 13: Promoting, Planning, and Prioritizing\n",
      "Section 14: end of Promoting, Planning, and Prioritizing\n",
      "Section 15: Fostering Organization\n",
      "Section 16: end of Fostering Organization\n",
      "Section 17: Instilling Time Management\n",
      "Section 18: end of Instilling Time Management\n",
      "Section 19: Encouraging Flexibility\n",
      "Section 20: end of Encouraging Flexibility\n",
      "Section 21: Increasing Goal-Directed Persistence\n",
      "Section 22: end of Increasing Goal-Directed Persistence\n",
      "Section 23: Cultivating Metacognition\n",
      "Section 24: end of Cultivating Metacognition\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "# --- Step 1: Load and split the text file by markers ---\n",
    "def split_by_markers(text: str):\n",
    "    \"\"\"\n",
    "    Splits the text into sections based on markers of the form \"---- <section title> ----\"\n",
    "    Returns a list of tuples: (section_title, content)\n",
    "    \"\"\"\n",
    "    pattern = r\"----\\s*(.*?)\\s*----\\n(.*?)(?=----|$)\"\n",
    "    matches = re.findall(pattern, text, re.DOTALL)\n",
    "    sections = [(title.strip(), content.strip()) for title, content in matches]\n",
    "    return sections\n",
    "\n",
    "with open(\"Smartbutscattered.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    book_text = f.read()\n",
    "\n",
    "sections = split_by_markers(book_text)\n",
    "print(f\"Total sections: {len(sections)}\")\n",
    "for i, (title, _) in enumerate(sections):\n",
    "    print(f\"Section {i+1}: {title}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total semantic chunks created: 119\n"
     ]
    }
   ],
   "source": [
    "# --- Step 2: Semantic chunking using LangChain's SemanticChunker ---\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "chunker = SemanticChunker(embeddings=embedding_model)\n",
    "\n",
    "docs = []\n",
    "for section_title, content in sections:\n",
    "    metadata = {\"executive_skill\": section_title, \"section_title\": section_title}\n",
    "    chunks = chunker.split_text(content)\n",
    "    for chunk in chunks:\n",
    "        docs.append(Document(page_content=chunk, metadata=metadata))\n",
    "\n",
    "print(f\"Total semantic chunks created: {len(docs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 119 new chunks to the collection.\n"
     ]
    }
   ],
   "source": [
    "# --- Step 3: Store chunks into Chroma DB with duplicate checking ---\n",
    "import chromadb\n",
    "\n",
    "# Initialize the Chroma client and create/get the \"exec_skills\" collection.\n",
    "# client = chromadb.Client()\n",
    "collection = client.get_or_create_collection(\"exec_skills\")\n",
    "\n",
    "existing_result = collection.get() \n",
    "existing_ids = set(existing_result.get(\"ids\", []))\n",
    "\n",
    "new_ids = []\n",
    "new_docs = []\n",
    "new_metadatas = []\n",
    "new_embeddings = []\n",
    "\n",
    "for i, doc in enumerate(docs):\n",
    "    # Create a deterministic ID using a sequential number and the executive_skill (with spaces removed)\n",
    "    doc_id = f\"exec_{i}_{doc.metadata['executive_skill'].replace(' ', '_')}\"\n",
    "    if doc_id in existing_ids:\n",
    "        continue\n",
    "\n",
    "    embedding = embedding_model.embed_documents([doc.page_content])[0]\n",
    "    new_ids.append(doc_id)\n",
    "    new_docs.append(doc.page_content)\n",
    "    new_metadatas.append(doc.metadata)\n",
    "    new_embeddings.append(embedding)\n",
    "\n",
    "if new_ids:\n",
    "    collection.add(\n",
    "        ids=new_ids,\n",
    "        documents=new_docs,\n",
    "        metadatas=new_metadatas,\n",
    "        embeddings=new_embeddings\n",
    "    )\n",
    "    print(f\"Added {len(new_ids)} new chunks to the collection.\")\n",
    "else:\n",
    "    print(\"No new chunks to add.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ids': ['exec_0_introduction', 'exec_1_introduction', 'exec_2_introduction', 'exec_3_introduction', 'exec_4_introduction', 'exec_5_introduction', 'exec_6_introduction', 'exec_7_introduction', 'exec_8_introduction', 'exec_9_introduction'], 'embeddings': array([[-0.0433003 ,  0.05404672,  0.10007481, ...,  0.04402868,\n",
      "        -0.02650417,  0.01566317],\n",
      "       [ 0.030204  , -0.02159898,  0.06438272, ...,  0.04969868,\n",
      "        -0.00508368,  0.04048944],\n",
      "       [ 0.01783722, -0.01273656,  0.02721443, ...,  0.1407281 ,\n",
      "        -0.01282246,  0.01987847],\n",
      "       ...,\n",
      "       [-0.01212036,  0.04398706, -0.04357916, ...,  0.04279802,\n",
      "        -0.06416749,  0.05369508],\n",
      "       [ 0.09652939,  0.03130122,  0.01609589, ...,  0.10366048,\n",
      "         0.03726649, -0.00696636],\n",
      "       [ 0.0324617 , -0.09679312, -0.01684162, ...,  0.1221268 ,\n",
      "         0.02831617, -0.01210798]]), 'documents': ['How Did Such a Smart Kid End Up So Scattered? Katie is 8 years old. It’s Saturday morning, and her mother has sent her to clean her room, with the admonition that she can’t go across the street to play with her girlfriend until everything is picked up. Katie reluctantly leaves the living room where her younger brother is engrossed in Saturday morning cartoons and climbs the stairs. She stands in the doorway and surveys the scene: Her Barbie dolls are scattered in one corner, a tangle of dolls and outfits and accessories that look from a distance like a colorful gypsy ragbag. Books are piled every which way in her bookcase, with some spilling out on the floor. Her closet door is open, and she sees that clothes have fallen off hangers and drifted to the floor of her closet, covering several pairs of shoes and some board games and puzzles she hasn’t played with recently. Some dirty clothes have been kicked under her bed but are visible in the space between the bedspread and the floor. And there’s a pile of clean clothes strewn around the floor by her bureau, left there after a mad search for a favorite sweater she wanted to wear to school yesterday. Katie sighs and goes to the doll corner. She places a couple of dolls on her toy shelf, then picks up a third doll and holds it at arm’s length to inspect the outfit she’s wearing. She remembers she was getting the doll ready for the prom and decides she doesn’t like the dress she chose. She scrabbles around in the pile of miniature clothing to find a dress she likes better. She’s just snapping the last fastener on the dress when her mother pops her head in the door. “Katie!” she says, a note of impatience in her voice. “It’s been half an hour and you haven’t done a thing!” Her mother comes over to the doll corner and together she and Katie pick up dolls and clothes, placing the dolls on toy shelves and the clothes in the plastic bin that serves as a clothes chest. The work goes quickly.', 'Mom stands up to leave. “Now, see what you can do with those books,” she says. Katie walks to the bookshelf and begins organizing her books. In the midst of the pile on the floor, she finds the latest in the Boxcar Children series, the one she’s in the middle of reading. She opens the book to the bookmarked page and begins reading. “I’ll just finish this chapter,” she tells herself. When she’s finished, she closes the book and looks around the room. “Mom!” she cries out plaintively. “This is way too much work! Can I go play and finish this later? Please?!” Downstairs, Katie’s mother sighs heavily. This happens every time she asks her daughter to get something done: she gets distracted, discouraged, and off track, and the job doesn’t get done unless Mom sticks around and walks her through each and every little step—or caves in and does it all herself. How can her daughter be so unfocused and irresponsible? Why can’t she put off just a little of what she’d prefer to do until she finishes what she hasto do? Shouldn’t a third grader be expected to take care of some things on her own? Katie has been in the 90th percentile on the Iowa achievement tests since she began taking them. Her teachers report that she’s imaginative, a whiz at math, and has a good vocabulary. She’s a nice girl, too. That’s why they hate to keep reporting to Katie’s parents that their daughter can be disruptive in class because she can’t stay on task during a group activity or that the teacher has to keep reminding her during quiet reading time to get back to the book and stop rummaging around in her desk, fiddling with her shoelaces, or whispering to her neighbors. Katie’s teachers have suggested more than once that it might help if her parents tried to impress upon her the importance of following directions and sticking to assigned activities. At this point her parents can only reply sheepishly that they’ve tried every way they know to get through to their daughter and that Katie sincerely promises to try but then can’t seem to hold on to her vow any more than she can follow through on cleaning her room or setting the table. Katie’s parents are at their wits’ end, and their daughter is at risk of falling behind at school. How can someone so smart be so scattered? As we mentioned in the Introduction, kids who are smart often end up scattered because they lack the brain-based skills we all need to plan and direct activities and to regulate behavior. It’s not that they have any problem receiving and organizing the input they get from their senses—what we might ordinarily consider “intelligence.” When it comes to smarts, they’ve got plenty. This is why they may have little trouble comprehending division or fractions or learning how to spell. The trouble shows up when they need to organize output—deciding what to do when and then controlling their own behavior to get there. Because they have what it takes to absorb information and learn math and language and other school subjects, you may assume that much simpler tasks like making a bed or taking turns12 WHAT MAKES YOUR CHILD SMART BUT SCATTERED should be a no-brainer. But that’s not the case because your child may have intelligence but lack the executive skills to put it to best use. What Are Executive Skills?', 'Let’s correct one possible misunderstanding right off the bat. When people hear the term executive skills , they assume it refers to the set of skills required of good business executives—skills like financial management, communication, strategic planning, and decision making. There issome overlap—executive skills definitely include decision making, planning, and management of all kinds of data, and like the skills used by a business executive, executive skills help kids get done what needs to get done—but in fact the term executive skills comes from the neurosciences literature and refers to the brain-based skills that are required for humans to execute ,o rp e r - form, tasks. Your child (like you) needs executive skills to formulate even the most fundamental plan to initiate a task. For something as simple as getting a glass of milk from the kitchen, he needs to decide to get up and go into the kitchen when he’s thirsty, get a glass from the cabinet, put it down on the counter, open the refrigerator and retrieve the milk, close the refrigerator, pour the milk, return the milk to the refrigerator, and then drink it either on the spot or back in the family room where he started out. To carry out this simple task he has to resist the impulse to grab and eat the chips he spots in the cabinet first—they’ll only make him thirstier—and to choose a sugar-loaded soda instead of milk. If he finds none of the usual glasses in the cabinet, he has to think to check the dishwasher instead of opting for one of his parents’ best crystal goblets. When he finds the milk is almost gone, he has to soothe his own frustration and resist starting a fight with his little sister when he’s sure she drank most of the milk. And he has to be sure not to leave a milk ring on the coffee table if he doesn’t want to be banned from having his snacks in the family room in the future. A child with executive skill weaknesses may be able to get a glass of milk without trouble—or he may get distracted, make poor choices, and demonstrate little emotional or behavioral control, leaving the fridge wide open, leaving a trail of milk droplets across the counter and the floor, leaving the milk out on the counter to spoil, and leaving his little sister in tears. But even if he can get himself a glass of milk without incident, you can bet that he will have trouble with the tasks in his life that are more complicated and more demanding of his ability to plan, sustain attention, organize, and regulate his feelings and how he acts on them. Executive skills are, in fact, what your child needs to make any of your hopes and dreams for his future—or his own hopes and dreams—come true. By late adolescence, our children must meet one fundamental condition: They must function with a reasonable degree of independence. That does not mean that they don’t askHow Did Such a Smart Kid End Up So Scattered? for help or seek advice at times. But it does mean that they no longer rely on us to plan or organize their day for them, tell them when to start tasks, bring them items when they forget them, or remind them to pay attention at school. When our children reach this point, our parenting role is coming to an end.', 'We speak of our children as being “on their own,” accept this at some level of comfort, and hope for the best for them. Social institutions do the same, defining them as “adult” for most legal purposes. To reach this stage of independence, the child must develop executive skills. You’ve probably seen an infant watch his mother leave the room, wait for a short time, and then begin to cry for his mother’s return. Or maybe you’ve listened to your -year-old tell herself, in a voice that sounds suspiciously like your own, not to do something. Or how about watching the 9-year-old who actually stops and looks before he races into the street after a ball? In all these cases you’re witnessing the development of executive skills.', 'Our Model Our initial work in executive skills dates to the 1980s. In evaluating and treating children with traumatic brain injuries, we saw that the source of many cognitive and behavioral difficulties was deficits in executive skills. Although less severe, we noted similar types of problems in children with significant attention disorders. From these origins, we began investigating the development of executive skills for a broad range of children. While there are other systems of executive skills (the Resource section includes references for these systems), our model has been designed to achieve a specific goal: to help us come up with ways that parents and teachers can promote the development of executive skills in kids who have demonstrated weaknesses. We’ve based our model on two premises: .Most individuals have an array of executive skill strengths as well as executive skill weaknesses . In fact, we’ve found that there seem to be common profiles of strengths and weaknesses. Kids (and adults) who are strong in some specific skills are often weak in other particular skills, and the patterns are predictable. We wanted a model that would enable people to identify those patterns so that kids could be encouraged to draw on their strengths and work to enhance or bypass their weaknesses to improve overall functioning. We also found that it made sense to help parents identify their own strengths and weaknesses so they could be of the greatest help to their kids. .The primary purpose of identifying areas of weakness is to be able to design and implement interventions to address those weaknesses . We wanted to be able to help chil-14 WHAT MAKES YOUR CHILD SMART BUT SCATTERED dren build the skills they need or manipulate the environment to minimize or prevent the problems associated with the skill weaknesses. The more discrete the skills are, the easier it is to develop operational definitions of them.', 'When the skills can be operationalized, it’s easier to create interventions to improve those operations. For example, let’s take the term scattered . It’s great for a book title because as a parent you read the word and know immediately that it describes your child. But scattered could mean forgetful or disorganized, lacking persistence, or distracted. Each one of those problems suggests a different solution.', 'The more specific we can be in our problem definition, the more likely we are to come up with a strategy that actually solves the problem. The scheme we arrived at consists of 11 skills: •Response inhibition •Working memory •Emotional control •Sustained attention •Task initiation •Planning/prioritization •Organization •Time management •Goal-directed persistence •Flexibility •Metacognition These skills can be organized in two different ways, developmentally (the order in which they develop in kids) and functionally (what they help the child do). Knowing the order in which the skills emerge during infancy, toddlerhood, and beyond, as mentioned earlier, helps you and your child’s teachers understand what to expect from a child at a particular age. In a workshop we conducted several years ago with teachers in kindergarten through grade 8, we asked teachers to identify those two or three executive skills in their students that were of greatest concern to them. Teachers in the lower elementary grades focused on task initiation and sustained attention, while middle school teachers stressed time management, organization, and planning/prioritization. Interestingly enough, teachers at all levels selected response inhibition as a skill that they saw lacking in many of their students! The main point, though, is that if you know the order in which skills are expected to develop, you won’t end up wasting your time trying to bolster a skill in your 7-year-old that is typically not mastered before age 11. You have enough battles already, you don’t need to add beating your head against a brick wall. The table on pages 16–17 lists the skills in order of emergence, defines each skill, and provides examples of what the skill looks like in younger and older children.How Did Such a Smart Kid End Up So Scattered? WHAT MAKES YOUR CHILD SMART BUT SCATTERED Developmental Progression of Executive Skills Executive skill Definition Examples Response inhibitionThe capacity to think before you act— this ability to resist the urge to say or do something allows your child the time to evaluate a situation and how his or her behavior might impact it.Ayoung child can wait for a short period without being disruptive. An adolescent can accept a referee’s call without an argument. Working memoryThe ability to hold information in memory while performing complex tasks. It incorporates the ability to draw on past learning or experience to apply to the situation at hand or to project into the future.Ayoung child can hold in mind and follow oneor two-step directions. The middle school child can remember the expectations of multiple teachers. Emotional controlThe ability to manage emotions to achieve goals, complete tasks, or control and direct behavior.Ayoung child with this skill can recover from a disappointment in a short time. Ateenager can manage the anxiety of a game or test and still perform. Sustained attentionThe capacity to keep paying attention to a situation or task in spite of distractibility, fatigue, or boredom.Completing a 5-minute chore with occasional supervision is an example of sustained attention in the younger child. Ateenager can pay attention to homework, with short breaks, for 1 to hours. Task initiation The ability to begin projects without undue procrastination, in an efficient or timely fashion.Ayoung child is able to start a chore or assignment right after instructions are given. A teenager does not wait until the last minute to begin a project. Planning/ prioritizationThe ability to create a roadmap to reach a goal or to complete a task. It also involves being able to make decisions about what’s important to focus on and what’s not important.Ayoung child, with coaching, can think of options to settle a peer conflict. Ateenager can formulate a plan to get a job. Organization The ability to create and maintain systems to keep track of information or materials.Ayoung child can, with a reminder, put toys in a designated place. Ateenager can organize and locate sports equipment. Infant research tells us that response inhibition, working memory, emotional control, and attention all develop early, in the first 6 to 12 months of life. We see the beginnings of planning when the child finds a way to get a desired object. This is more evident when the child walks. Flexibility shows in the child’s reaction to change and can be seen between 12 and 24 months. The other skills, such as task initiation, organization, time management, and goal-directed persistence, come later, ranging from preschool to early elementary school. Knowing how each skill functions—whether it contributes to your child’s thinking or doing—tells you whether the goal of your intervention is to help your child think differently or to help your child behave differently. If your child has a weak working memory, for instance, you will be working to give the child strategies to help her retrieve critical information (such as what she has to bring home from school for homework) more reliably. If your child has weak emotional control, you will be working to help him use words rather than fists when he discovers that his little brother sat on his model airplane. In fact, though, thinking and doing go handHow Did Such a Smart Kid End Up So Scattered? Executive skill Definition Examples Time managementThe capacity to estimate how much time one has, how to allocate it, and how to stay within time limits and deadlines. It also involves a sense that time is important.Ayoung child can complete a short job within a time limit set by an adult.', 'Ateenager can establish a schedule to meet task deadlines. Goal-directed persistenceThe capacity to have a goal, follow through to the completion of the goal, and not be put off by or distracted by competing interests.Afirst grader can complete a job to get to recess. Ateenager can earn and save money over time to buy something of importance. Flexibility The ability to revise plans in the face of obstacles, setbacks, new information, or mistakes. It relates to an adaptability to changing conditions.Ayoung child can adjust to a change in plans without major distress. Ateenager can accept an alternative such as a different job when the first choice is not available. Metacognition The ability to stand back and take a bird’s-eye view of yourself in a situation, to observe how you problem solve. It also includes self-monitoring and self-evaluative skills (e.g., asking yourself, “How am I doing?” or “How did I do?”).Ayoung child can change behavior in response to feedback from an adult. A teenager can monitor and critique her performance and improve it by observing others who are more skilled. in hand. Very often, we’re teaching kids how to use their thoughts to control their behaviors. The thinking skills are designed to select and achieve goals or to develop solutions to problems. They help children create a picture of a goal and a path to that goal, and they give them the resources they’ll need to access along the way to achieve the goal. They also help your child remember the picture, even though the goal may be far away and other events come along to occupy the child’s attention and take up space in his or her memory. But to reach the goal, your child needs to use the second set of skills, ones that enable the child to do what he needs to do to accomplish the tasks he has set for himself. The second set of skills incorporates behaviors that guide the child’s actions as he moves along the path. This organizing scheme is depicted in the table below. When all goes as planned, beginning in early childhood, we come up with ideas for things we want or need to do, plan or organize the task, squelch thoughts or feelings that interfere with our plans, cheer ourselves on, keep the goal in mind even when obstacles, distractions, or temptations arise, change course as the situation requires, and persist with our efforts until the goal is achieved. This may be as time limited as completing a 10-piece puzzle or as extensive as remodeling our house. Whether we’re 3 years old or 30, we use the same set of brain-based executive skills to help us reach our goal. During much of your child’s growth, you can see those executive skills improving. You probably remember having to hold your child’s hand on the sidewalk constantly at age 2, then recall being able to walk side by side when your daughter was , and then letting her cross the street on her own a few years later. At each stage you were aware that your child’s executive skills—her ability to be independent— were growing yet were not developed enough for the child to manage her behavior or solve all the problems she faces without guidance. Everything you teach your child reflects your instinctive understanding that you play a role in helping your child develop and refine these executive skills. So, if parents are playing this role, how do some kids end up off track?18 WHAT MAKES YOUR CHILD SMART BUT SCATTERED Two Dimensions of Executive Skills: Thinking and Doing Executive skillsinvolving thinking (cognition)Executive skillsinvolving doing(behavior) Working memory Response inhibition Planning/prioritization Emotional control Organization Sustained attention Time management Task initiation Metacognition Goal-directed persistence Flexibility How Executive Skills Develop in the Brain: Biology and Experience How do children come by executive skills? As is the case with many of the abilities we have, there are two main contributors: biology and experience. In terms of the biological contribution, the potential for executive skills is innate, already hardwired into the brain at birth. This is similar to the way that language develops. Of course at birth executive skills, like language, exist only as potential. This means that the brain has within it the biological equipment for these skills to develop.', 'But there are a number of biological factors that can influence how these skills develop. Major trauma or physical insult to the child’s brain, particularly to the frontal lobes, will affect skill development. The genes that the child inherits from you two can also impact these skills. If you don’t have good attention or organization skills, chances are your child will have problems in these areas as well. As to the environment, if it’s biologically or physically toxic, the likelihood that the child’s executive skills will suffer is increased. Environmental “poisons” can include anything from lead exposure to child abuse. However, assuming reasonably normal biological equipment and the absence of genetic or environmental traumas, brain development can proceed as it’s supposed to. Biology: Growth + Pruning = Executive Skills At birth, a child’s brain weighs about 13 ounces. By the late teenage years brain weight has increased to nearly 3 pounds. A number of changes account for this increase.', 'First, there’s rapid growth in the number of nerve cells in the brain. These nerve cells must communicate if the child is to think, feel, or act. So they can “talk” to each other, the nerve cells develop branches that allow them to send and receive information from other nerve cells. The growth of these branches, called axons and dendrites, is especially fast during the infant and toddler years. Also during these earliest stages of development, a substance known as myelin begins to form a fatty sheath around the axons. This process of myelination insulates the branches that carry the nerve signals, making the “conversations” between nerve cells faster and more efficient. Myelination continues well into the late stages of adolescence and early adulthood and is responsible for the development of what is often called the white matter of the brain. The white matter consists of bundles of axons that connect different brain regions and allow them to communicate. Then there’s gray matter. There’s a reason that this is the term often used as a metaphor for the learning, thinking power of the brain itself. The gray matter is made up of the nerve cells, or neurons, and the connections between them called thesynapses, and the development of this type of brain matter is a bit more complex. Five months into pregnancy, the brain of an unborn child is estimated to haveHow Did Such a Smart Kid End Up So Scattered? about one hundred billion neurons. This is comparable to what the average adult brain has. Early in childhood, the number of synapses in the brain (about a quadrillion) greatly exceeds the number in the adult brain. If development of gray matter continued at this pace, the adult brain would be enormous. Instead, a different phenomenon occurs. The increase in gray matter—neurons and particularly synapses— peaks before age 5 and is followed by a gradual reduction or “pruning” of the neuron connections. The initial increase happens during a period of rapid learning and experience in early childhood. Recent brain research suggests that as this learning and skill development become more efficient, additional increases in gray matter could actually undermine new learning. Through pruning, the child consolidates mental skills, with the gray matter connections that are not needed or used dropping away. This period of consolidation continues until a second period of significant growth in gray matter that begins around age 11 or 12, the onset of another period recognized as one of rapid learning and development. This increase is again followed by a period of reduction through pruning over the course of adolescence. Of significance to what we know about the development of executive skills, research shows that this growth spurt prior to adolescence occurs primarily in the frontal lobes. Considering that scientists generally agree that the frontal brain systems play a key role in the development of executive skills, we can safely say that these areas, which include the frontal and prefrontal cortex, along with connections to adjacent areas, make up the brain base for executive skills. It’s as if during the preteen years the brain is preparing itself for the development of executive skills and the significant demands that will be made on those executive skills during adolescence. The diagram on the facing page shows the human brain with the approximate location of major functions, including executive skills, in the prefrontal cortex. Researchers at the National Institute of Mental Health also suggest that a “use it or lose it” process may be occurring in the frontal lobes during this time. Neural connections that are used are retained, while those that are not exercised are lost. If this is the case, then the practice of executive skills is critical. It means that kids who practice executive skills are not only learning self-management— independence—but in the process developing brain structures that will support their executive skills into later adolescence and adulthood. Practice is important to acquisition of executive skills for another reason. Researchers who study the brain using fMRI (functional magnetic resonance imaging) have found that when children and teenagers perform tasks that require these skills, they rely on the prefrontal cortex to do all the work rather than distributing the workload to other specialized regions of the brain, such as the amygdala and the insula, two parts of the brain that are activated when making quick decisions that affect safety and survival (the fight-or-flight response). Adults, in contrast, can spread out the workload in part because they’ve had years of practice to develop the neural pathways to make this possible. Activating executive skills takes more conscious effort with children and with teenagers than it does with adults, which may help20 WHAT MAKES YOUR CHILD SMART BUT SCATTERED explain why they are less inclined to engage their working memory to perform tasks of daily living. This is where you—and your child’s teachers—come in. Clearly childhood offers parents and teachers a critical opportunity to enhance the learning and development of executive skills in a child. We don’t mean to oversimplify. The brain is a very complex organ, and evidence from brain-imaging studies continues to suggest that areas other than the prefrontal cortex are involved in the development of executive skills. But the prefrontal systems are among the last areas of the brain to develop fully, in late adoescence or young adulthood, and are the final, common pathway for managing information and deciding how we will behave. When you consider the critical functions of the frontal lobes, it’s easy to see how important these brain structures must be to the development of executive skills: . The frontal lobes direct our behavior, helping us decide what we should pay attention to and what actions we should take. Example: A 7-year-old sees his brother watching television.'], 'uris': None, 'included': ['embeddings', 'metadatas', 'documents'], 'data': None, 'metadatas': [{'executive_skill': 'introduction', 'section_title': 'introduction'}, {'executive_skill': 'introduction', 'section_title': 'introduction'}, {'executive_skill': 'introduction', 'section_title': 'introduction'}, {'section_title': 'introduction', 'executive_skill': 'introduction'}, {'section_title': 'introduction', 'executive_skill': 'introduction'}, {'executive_skill': 'introduction', 'section_title': 'introduction'}, {'executive_skill': 'introduction', 'section_title': 'introduction'}, {'executive_skill': 'introduction', 'section_title': 'introduction'}, {'section_title': 'introduction', 'executive_skill': 'introduction'}, {'section_title': 'introduction', 'executive_skill': 'introduction'}]}\n"
     ]
    }
   ],
   "source": [
    "print(collection.peek())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query results:\n",
      "{\n",
      "    \"documents\": [\n",
      "        \"Enhancing Working Memory Working memory is the capacity to hold information in mind while performing complex tasks. We rely on working memory all the time. It\\u2019s the ability to run out to the store to buy a few things and remember what they are without having to write them down. When you remember to stop by the dry cleaner on your way home from work, you\\u2019re using working memory. When you look up a phone number in the phone book and remember it long enough to make the call, you\\u2019re using working memory. When your spouse asks you to do something and you say, \\u201cI\\u2019ll do it as soon as I finish loading the dishwasher,\\u201d and then you actually remember to do it, chances are your working memory is pretty good. Odds are it\\u2019s not so good, however, if you can\\u2019t remember anyone\\u2019s birthday, you tend to return home with only half your errands done unless you have a written agenda, and you\\u2019ll do anything to avoid having to introduce people at a cocktail party because you can\\u2019t remember anyone\\u2019s name. In that case, be sure to use the tips in Chapter 3 to help you enhance your child\\u2019s working memory when you have the same weakness.\",\n",
      "        \"How Working Memory Develops Working memory begins to develop fairly early in infancy. When you\\u2019re playing with a baby and you hide a favorite toy under a blanket, you know the baby is using working memory if he lifts the blanket to retrieve the toy. This is because the baby is able to hold an image of the toy in mind as well as the memory of what you did to hide it. Children develop nonverbal working memory before they develop verbal working memory because this skill begins to emerge before language does. When children develop language, however, their working memory skills expand, because now they can draw on visual imagery and language to retrieve information. As we told you in Chapter 1, when children and teenagers perform tasks that require executive skills such as working memory, they rely on the prefrontal cortex to do all the work rather than distributing the workload to other specialized regions of the brain, as adults are able to do. Thus, activating working memory takes more conscious effort with children and with teenagers than it does with adults, which may help explain why they are less inclined to use their working memory to complete daily routine tasks. We tend naturally to limit our expectations for working memory in very young children. Before the age of 3, we generally expect children to remember only things that are in close proximity\\u2014either in time or in space. If we want them to do something, we don\\u2019t say, \\u201cWould you mind putting your toys away after you finish watchingBarney ?\\u201d (unless we also expect to cue them once Barney is over). And while we might ask them to put all their blocks in the toy box while we\\u2019re standing in the playroom with them, we generally don\\u2019t instruct them to go to their bedroom and do a similar task all by themselves. Gradually, we\\u2019re able to stretch both time and distance in terms of what we expect our children to be able to remember. In the questionnaire below, you can evaluate where your child might fall on the developmental ladder, based on the kinds of tasks children are capable of carrying out independently at various childhood stages. Using this scale will give you a closer look than the scales in Chapter did at how well your child uses the skill of working memory. HOW GOOD ISYOUR CHILD\\u2019SWORKING MEMORY? Use the following scale to rate how well your child performs e ach of the tasks listed. At each level, children can be expected to perform all the tasks listed fairly well to very well. Scale \\u2014 Never or rarely \\u2014 Does but not well (about 25%of the time) \\u2014 Does fairly well (about 75%of the time) \\u2014 Does very well (always or almost always) Preschool/kindergarten Runs simple errands (e.g., gets shoes from bedroom when aske d) Remembers instructions that were just given Follows a routine with only one prompt per step (e.g., brushi ng teeth after breakfast)198 PUTTING IT ALL TOGETHER Lower elementary (grades1\\u20133) Able to run an errand with two to three steps Remembers instructions that were given a couple of minutes e arlier Follows two steps of a routine with one prompt Upper elementary (grades4\\u20135) Remembers to perform a routine chore after school without re minder Takes books, papers, assignments to and from school Keeps track of changing daily schedule (e.g., different act ivities after school) Middle school (grades6\\u20138) Able to keep track of assignments and classroom expectation s of multiple teachers Remembers events or responsibilities that deviate from the norm (e.g., permission slips for field trips, special instructions regard ing extracurricular activities, etc.) Remembers multistep directions, given sufficient time or p ractice From Smart but Scattered by Peg Dawson and Richard Guare. Copyright 2009 by The Guilfo rd Press.\",\n",
      "        \"Building Working Memory in Everyday Situations \\u2022Make eye contact with your child before telling him something you want him to remember. \\u2022Keep external distractions to a minimum if you want your child\\u2019s full attention (for example, turn off the television or turn down the volume). \\u2022Have the child repeat back to you what you just said so you know she has heard you. \\u2022Use written reminders \\u2014picture schedules, lists, and schedules, depending on the age of the child. Prompt the child at each step to \\u201ccheck your schedule\\u201d or \\u201clook at your list.\\u201d \\u2022Rehearse with the child what you expect him to remember just before the situation (for example, \\u201cWhat do you need to say to Aunt Mary after she gives you your birthday present?\\u201d). \\u2022Help the child think about ways to help her remember something important that she thinks will work for her. \\u2022With children in middle school, use cell phones, text messages, or instant messages (IMs) to remind them of important things they have to do.Enhancing Working Memory \\u2022Consider using a reward for remembering key information or imposing a penalty for forgetting. For example, a child might be allowed to rent a video game on the weekend if he goes a whole week without forgetting to bring home all his homework materials. Rewards and penalties are useful when your child\\u2019s working memory is only mildly underdeveloped. Ending the Waiting Game: Teaching Y our Child to Get Dressed without Dawdling Annie is a bright 8-year-old second grader who can be absent-minded at times but is one of the more advanced students in her class. She has a variety of interests and is a good friend to her peers.\",\n",
      "        \"They are satisfied that the cutout will work. Step 1: Establish Behavioral Goal Target executive skill(s): Working memory Specific behavioral objective: Jake will organize his sports equipment before each game and have the equipment he needs for each game with no more than one a dult prompt Step 2: Design Intervention What environmental supports will be provided to help reach the target goal? \\u2022 A cutout of Jake that will be labeled with equipment needed for pract ice and games. \\u2022 Reminder from his dad the night before a game to check and pack his equip - ment. What specific skill will be taught, who will teach the skill, and what procedure will be used to teach it? Skill: Working memory (remember all required sports equipment for practice and games)Enhancing Working Memory (cont.) Who will teach the skill? Father Procedure: \\u2022 Jake and Dad meet and agree on a plan for organizing the equipment.\",\n",
      "        \"\\u2022 Jake, with Dad\\u2019s help, makes a cutout. \\u2022 Jake makes labels and hooks for all equipment and puts them on the cutou t. \\u2022 He tries one practice run with his father watching. \\u2022 Dad agrees to cue him to get equipment ready the night before. \\u2022 For two weeks, Dad checks with him after he has given the cue to ensure that he has followed through. What incentives will be used to help motivate the child to use/practice the skill? \\u2022 Jake will be able to participate in sports without experiencing cons equences from coaches for not having equipment. Keysto Success \\u2022Don\\u2019t rely on your child\\u2019s statement that he or she has acted on your cue. In this example the cutout served as a reminder and an organizing too l for Jake. While this may be enough in most cases, children with working memor y weaknesses, when asked about or cued to remember something, will often in dicate that they have done what they need to do or will take care of it and then pr oceed to forget. Therefore you\\u2019ll need to follow up the cue with a check to see if your child has in fact acted on the cue. Acting at the time that the cue is g iven is key, which may involve your checking on a more frequent basis until the d esired behavior is established.204 PUTTING IT ALL TOGETHER\"\n",
      "    ],\n",
      "    \"metadatas\": [\n",
      "        {\n",
      "            \"executive_skill\": \"Enhancing Working Memory\",\n",
      "            \"section_title\": \"Enhancing Working Memory\"\n",
      "        },\n",
      "        {\n",
      "            \"executive_skill\": \"Enhancing Working Memory\",\n",
      "            \"section_title\": \"Enhancing Working Memory\"\n",
      "        },\n",
      "        {\n",
      "            \"executive_skill\": \"Enhancing Working Memory\",\n",
      "            \"section_title\": \"Enhancing Working Memory\"\n",
      "        },\n",
      "        {\n",
      "            \"executive_skill\": \"Enhancing Working Memory\",\n",
      "            \"section_title\": \"Enhancing Working Memory\"\n",
      "        },\n",
      "        {\n",
      "            \"section_title\": \"Enhancing Working Memory\",\n",
      "            \"executive_skill\": \"Enhancing Working Memory\"\n",
      "        }\n",
      "    ],\n",
      "    \"distances\": [\n",
      "        0.8731342554092407,\n",
      "        0.9037927389144897,\n",
      "        0.9836471080780029,\n",
      "        1.238317847251892,\n",
      "        1.3526577949523926\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    " #--- Step 4: Query test using metadata filtering ---\n",
    "# For this example, we query for chunks where the executive_skill is \"Enhancing Working Memory\".\n",
    "query_text = \"What strategies can help improve working memory?\"\n",
    "metadata_filter = {\"executive_skill\": \"Enhancing Working Memory\"}\n",
    "\n",
    "results = collection.query(\n",
    "    query_texts=[query_text],\n",
    "    n_results=5,\n",
    "    where=metadata_filter,\n",
    "    include=[\"documents\", \"metadatas\", \"distances\"]\n",
    ")\n",
    "\n",
    "if results[\"documents\"]:\n",
    "    flat = {\n",
    "        \"documents\": results[\"documents\"][0],\n",
    "        \"metadatas\": results[\"metadatas\"][0],\n",
    "        \"distances\": results[\"distances\"][0]\n",
    "    }\n",
    "    print(\"Query results:\")\n",
    "    print(json.dumps(flat, indent=4))\n",
    "else:\n",
    "    print(\"No documents retrieved for the given query filter.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra things done - different methods used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## parabased chunking for tb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import uuid\n",
    "from typing import List, Tuple\n",
    "from langchain.docstore.document import Document\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "\n",
    "# -------------------------------\n",
    "# 1. Top-Level Splitting by Explicit Section Markers\n",
    "# -------------------------------\n",
    "def split_by_markers(text: str) -> List[Tuple[str, str]]:\n",
    "    \"\"\"\n",
    "    Splits text into sections based on markers that look like:\n",
    "      ---- section title ----\n",
    "    Ignores markers with \"end\" (case-insensitive).\n",
    "    \n",
    "    Returns a list of tuples: (section_title, section_content)\n",
    "    \"\"\"\n",
    "    marker_pattern = r\"(?m)^----\\s*(.*?)\\s*----\\s*$\"\n",
    "    markers = list(re.finditer(marker_pattern, text))\n",
    "    sections = []\n",
    "    for i, m in enumerate(markers):\n",
    "        title = m.group(1).strip()\n",
    "        if \"end\" in title.lower():\n",
    "            continue\n",
    "        start_index = m.end()\n",
    "        end_index = len(text)\n",
    "        # Look for the next marker that is not an \"end\" marker or take rest of text.\n",
    "        for j in range(i+1, len(markers)):\n",
    "            next_title = markers[j].group(1).strip()\n",
    "            if \"end\" not in next_title.lower():\n",
    "                end_index = markers[j].start()\n",
    "                break\n",
    "            else:\n",
    "                end_index = markers[j].start()\n",
    "                break\n",
    "        content = text[start_index:end_index].strip()\n",
    "        sections.append((title, content))\n",
    "    return sections\n",
    "\n",
    "# -------------------------------\n",
    "# 2. Second-Level Splitting by Paragraphs\n",
    "# -------------------------------\n",
    "def split_section_paragraphs(sections: List[Tuple[str, str]]) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Splits each section by paragraphs (using two or more newlines) and creates a Document for each chunk.\n",
    "    Applies a simple mapping rule for sections representing executive skills.\n",
    "    \"\"\"\n",
    "    docs = []\n",
    "    \n",
    "    def map_title(title: str) -> str:\n",
    "        lower = title.lower()\n",
    "        if lower.startswith(\"building response inhibition\"):\n",
    "            return \"Response Inhibition\"\n",
    "        elif lower.startswith(\"enhancing working memory\"):\n",
    "            return \"Working Memory\"\n",
    "        # Add additional mappings if needed.\n",
    "        return title  # Default\n",
    "    \n",
    "    for title, content in sections:\n",
    "        mapped_title = map_title(title)\n",
    "        # Split by paragraph (two or more newlines)\n",
    "        paragraphs = re.split(r\"\\n\\s*\\n\", content)\n",
    "        for para in paragraphs:\n",
    "            para = para.strip()\n",
    "            if para:\n",
    "                docs.append(Document(\n",
    "                    page_content=para,\n",
    "                    metadata={\"executive_skill\": mapped_title, \"section_title\": title}\n",
    "                ))\n",
    "    return docs\n",
    "\n",
    "# -------------------------------\n",
    "# 3. Load \"Smartbutscattered.txt\" and Process\n",
    "# -------------------------------\n",
    "with open(\"Smartbutscattered.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    book_text = f.read()\n",
    "\n",
    "sections = split_by_markers(book_text)\n",
    "exec_docs = split_section_paragraphs(sections)\n",
    "\n",
    "# Optional: Save structured chunks to a JSON file for review\n",
    "with open(\"exec_skill_chunks_parabased.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump([{\"executive_skill\": doc.metadata[\"executive_skill\"], \"section_title\": doc.metadata[\"section_title\"], \"content\": doc.page_content} for doc in exec_docs], f, indent=4)\n",
    "\n",
    "# -------------------------------\n",
    "# 4. Embed & Store in ChromaDB Collection \"exec_skills\"\n",
    "# -------------------------------\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "client = chromadb.Client()\n",
    "exec_collection = client.get_or_create_collection(\"exec_skills\")\n",
    "\n",
    "# Use a deterministic ID: based on executive_skill and the chunk index\n",
    "existing_exec_ids = set(exec_collection.get()[\"ids\"])\n",
    "\n",
    "for idx, doc in enumerate(exec_docs):\n",
    "    skill = doc.metadata[\"executive_skill\"].replace(\" \", \"_\")\n",
    "    doc_id = f\"exec_{skill}_{idx}\"\n",
    "    if doc_id in existing_exec_ids:\n",
    "        continue\n",
    "    embedding = model.encode(doc.page_content).tolist()\n",
    "    exec_collection.add(\n",
    "        documents=[doc.page_content],\n",
    "        metadatas=[doc.metadata],\n",
    "        embeddings=[embedding],\n",
    "        ids=[doc_id]\n",
    "    )\n",
    "    existing_exec_ids.add(doc_id)\n",
    "\n",
    "print(\"Executive skill chunks embedded and stored in the 'exec_skills' collection.\")\n",
    "\n",
    "# -------------------------------\n",
    "# 5. (Optional) Retrieval Example\n",
    "# -------------------------------\n",
    "query_text = \"What are some strategies to improve Working Memory?\"\n",
    "\n",
    "# Use a single filter expression directly (do not wrap in $and if there's only one condition)\n",
    "# metadata_filter = {\"executive_skill\": \"Working Memory\"}\n",
    "\n",
    "results = exec_collection.query(\n",
    "    query_texts=[query_text],\n",
    "    n_results=5,\n",
    "    # where=metadata_filter,\n",
    "    include=[\"documents\", \"metadatas\", \"distances\"]\n",
    ")\n",
    "\n",
    "# Unpack results from the first query batch.\n",
    "flat = {\n",
    "    \"ids\": results[\"ids\"][0],\n",
    "    \"documents\": results[\"documents\"][0],\n",
    "    \"metadatas\": results[\"metadatas\"][0],\n",
    "    \"distances\": results[\"distances\"][0]\n",
    "}\n",
    "\n",
    "import json\n",
    "print(json.dumps(flat, indent=4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking how are meta data is for tb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All metadata from the 'exec_skills' collection:\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import chromadb\n",
    "\n",
    "# Initialize ChromaDB client and connect to your \"exec_skills\" collection.\n",
    "client = chromadb.Client()\n",
    "exec_collection = client.get_or_create_collection(\"exec_skills\")\n",
    "\n",
    "# Retrieve all records from the collection.\n",
    "records = exec_collection.get()\n",
    "\n",
    "# The \"metadatas\" field is expected to be a list of lists.\n",
    "# We want to flatten that to a list of dictionaries.\n",
    "flat_metadatas = []\n",
    "\n",
    "# Loop over each batch (each batch is expected to be a list)\n",
    "for batch in records.get(\"metadatas\", []):\n",
    "    if isinstance(batch, list):\n",
    "        # Check each element in the batch:\n",
    "        for meta in batch:\n",
    "            # If the element is a dictionary, append it directly.\n",
    "            if isinstance(meta, dict):\n",
    "                flat_metadatas.append(meta)\n",
    "            else:\n",
    "                # If it is not a dict, it might be a string (i.e. a key),\n",
    "                # so we skip or wrap it as needed.\n",
    "                flat_metadatas.append({\"value\": meta})\n",
    "    elif isinstance(batch, dict):\n",
    "        # In case the batch itself is a dictionary, add it.\n",
    "        flat_metadatas.append(batch)\n",
    "    else:\n",
    "        # Otherwise, add as a simple string wrapped in a dict.\n",
    "        flat_metadatas.append({\"value\": batch})\n",
    "\n",
    "# Print the metadata in a readable JSON format.\n",
    "print(\"All metadata from the 'exec_skills' collection:\")\n",
    "print(json.dumps(flat_metadatas, indent=4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recursive Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install nltk langchain chromadb sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "from typing import List, Tuple\n",
    "from langchain.docstore.document import Document\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# -------------------------------\n",
    "# Step 1: Top-Level Splitting by Explicit Markers\n",
    "# -------------------------------\n",
    "def split_by_markers(text: str) -> List[Tuple[str, str]]:\n",
    "    \"\"\"\n",
    "    Splits the text using markers of the form:\n",
    "       ---- Section Title ----\n",
    "    Ignores markers that have \"end\" (case-insensitive) in the title.\n",
    "    Returns a list of (section_title, section_content) tuples.\n",
    "    \"\"\"\n",
    "    marker_pattern = r\"(?m)^----\\s*(.*?)\\s*----\\s*$\"\n",
    "    markers = list(re.finditer(marker_pattern, text))\n",
    "    sections = []\n",
    "    for i, m in enumerate(markers):\n",
    "        title = m.group(1).strip()\n",
    "        if \"end\" in title.lower():\n",
    "            continue\n",
    "        start_index = m.end()\n",
    "        end_index = len(text)\n",
    "        for j in range(i+1, len(markers)):\n",
    "            next_title = markers[j].group(1).strip()\n",
    "            # If next marker is an \"end\" marker or a new section, stop here.\n",
    "            if \"end\" in next_title.lower() or next_title:\n",
    "                end_index = markers[j].start()\n",
    "                break\n",
    "        content = text[start_index:end_index].strip()\n",
    "        sections.append((title, content))\n",
    "    return sections\n",
    "\n",
    "# -------------------------------\n",
    "# Step 2: Sentence-Based Chunking with Overlap\n",
    "# -------------------------------\n",
    "def group_sentences(sentences: List[str], max_chunk_size: int = 1000, overlap_sent_count: int = 2) -> List[str]:\n",
    "    \"\"\"\n",
    "    Groups sentences into chunks that do not exceed max_chunk_size (by character count)\n",
    "    and adds overlap of the last few sentences to the next chunk.\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    current_sentences = []\n",
    "    current_length = 0\n",
    "\n",
    "    for sentence in sentences:\n",
    "        # +1 for a space between sentences.\n",
    "        if current_length + len(sentence) + (1 if current_sentences else 0) <= max_chunk_size:\n",
    "            current_sentences.append(sentence)\n",
    "            current_length = len(\" \".join(current_sentences))\n",
    "        else:\n",
    "            # Current chunk is complete; join it.\n",
    "            chunk = \" \".join(current_sentences)\n",
    "            chunks.append(chunk)\n",
    "            # Now, determine overlap: use the last overlap_sent_count sentences.\n",
    "            overlap_sentences = current_sentences[-overlap_sent_count:] if len(current_sentences) >= overlap_sent_count else current_sentences\n",
    "            # Start a new chunk with the overlap and the current sentence.\n",
    "            current_sentences = overlap_sentences + [sentence]\n",
    "            current_length = len(\" \".join(current_sentences))\n",
    "    if current_sentences:\n",
    "        chunks.append(\" \".join(current_sentences))\n",
    "    return chunks\n",
    "\n",
    "def split_section_paragraphs(sections: List[Tuple[str, str]], max_chunk_size: int = 1000, overlap_sent_count: int = 2) -> List[Document]:\n",
    "    \"\"\"\n",
    "    For each section (tuple of title and content), first split into paragraphs using blank lines,\n",
    "    then for each paragraph split into sentences (using nltk.sent_tokenize) and group them into chunks.\n",
    "    Each resulting chunk is saved as a Document with metadata.\n",
    "    \"\"\"\n",
    "    docs = []\n",
    "\n",
    "    def map_title(title: str) -> str:\n",
    "        lower = title.lower()\n",
    "        if lower.startswith(\"building response inhibition\"):\n",
    "            return \"Response Inhibition\"\n",
    "        elif lower.startswith(\"enhancing working memory\"):\n",
    "            return \"Working Memory\"\n",
    "        # Extend mappings for additional executive skill sections as needed.\n",
    "        return title  # fallback: use the raw title\n",
    "\n",
    "    for title, content in sections:\n",
    "        canonical_skill = map_title(title)\n",
    "        # First, split the section into paragraphs (using two or more newlines)\n",
    "        paragraphs = re.split(r\"\\n\\s*\\n\", content)\n",
    "        for para in paragraphs:\n",
    "            para = para.strip()\n",
    "            if not para:\n",
    "                continue\n",
    "            # Use nltk to split into sentences.\n",
    "            sentences = sent_tokenize(para)\n",
    "            # Group sentences into chunks with our function.\n",
    "            chunk_texts = group_sentences(sentences, max_chunk_size=max_chunk_size, overlap_sent_count=overlap_sent_count)\n",
    "            for chunk in chunk_texts:\n",
    "                docs.append(Document(\n",
    "                    page_content=chunk,\n",
    "                    metadata={\n",
    "                        \"executive_skill\": canonical_skill,\n",
    "                        \"section_title\": title\n",
    "                    }\n",
    "                ))\n",
    "    return docs\n",
    "\n",
    "# -------------------------------\n",
    "# Step 3: Load Text File and Process\n",
    "# -------------------------------\n",
    "with open(\"Smartbutscattered.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    book_text = f.read()\n",
    "\n",
    "sections = split_by_markers(book_text)\n",
    "exec_docs = split_section_paragraphs(sections, max_chunk_size=1000, overlap_sent_count=2)\n",
    "\n",
    "# (Optional) Write the resulting chunks (metadata and content) to a JSON file for inspection.\n",
    "with open(\"exec_skill_chunks.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(\n",
    "        [{\"executive_skill\": doc.metadata[\"executive_skill\"],\n",
    "          \"section_title\": doc.metadata[\"section_title\"],\n",
    "          \"content\": doc.page_content}\n",
    "         for doc in exec_docs],\n",
    "        f,\n",
    "        indent=4\n",
    "    )\n",
    "# -------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Langchain recursive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "from typing import List, Tuple\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "\n",
    "# -------------------------------\n",
    "# Step 1: Hierarchical Splitting by Explicit Markers\n",
    "# -------------------------------\n",
    "def split_by_markers_recursivelangchain(text: str) -> List[Tuple[str, str]]:\n",
    "    \"\"\"\n",
    "    Splits the text using markers of the form:\n",
    "        ---- Section Title ----\n",
    "    Skips markers that have \"end\" in the title (case-insensitive). Returns a list of tuples:\n",
    "    (section_title, section_content)\n",
    "    \"\"\"\n",
    "    marker_pattern = r\"(?m)^----\\s*(.*?)\\s*----\\s*$\"\n",
    "    markers = list(re.finditer(marker_pattern, text))\n",
    "    sections_recursivelangchain = []\n",
    "    for i, m in enumerate(markers):\n",
    "        title = m.group(1).strip()\n",
    "        if \"end\" in title.lower():\n",
    "            continue  # Skip markers that indicate an end section\n",
    "        start_index = m.end()\n",
    "        end_index = len(text)\n",
    "        # Look for the next marker (regardless of its title) to define the end of this section.\n",
    "        for j in range(i + 1, len(markers)):\n",
    "            next_title = markers[j].group(1).strip()\n",
    "            # We stop at the very next marker (even if it is an 'end' marker)\n",
    "            end_index = markers[j].start()\n",
    "            break\n",
    "        content = text[start_index:end_index].strip()\n",
    "        sections_recursivelangchain.append((title, content))\n",
    "    return sections_recursivelangchain\n",
    "\n",
    "# -------------------------------\n",
    "# Step 2: Mapping Section Titles to Canonical Executive Skills\n",
    "# -------------------------------\n",
    "def map_title_to_skill_recursivelangchain(title: str) -> str:\n",
    "    lower = title.lower()\n",
    "    if lower.startswith(\"building response inhibition\"):\n",
    "        return \"Response Inhibition\"\n",
    "    elif lower.startswith(\"enhancing working memory\"):\n",
    "        return \"Working Memory\"\n",
    "    elif lower.startswith(\"improving emotional control\"):\n",
    "        return \"Improving Emotional Control\"\n",
    "    elif lower.startswith(\"strengthening sustained attention\"):\n",
    "        return \"Strengthening Sustained Attention\"\n",
    "    elif lower.startswith(\"teaching task initiation\"):\n",
    "        return \"Teaching Task Initiation\"\n",
    "    elif lower.startswith(\"promoting, planning, and prioritizing\"):\n",
    "        return \"Promoting, Planning, and Prioritizing\"\n",
    "    elif lower.startswith(\"fostering organization\"):\n",
    "        return \"Fostering Organization\"\n",
    "    elif lower.startswith(\"instilling time management\"):\n",
    "        return \"Instilling Time Management\"\n",
    "    elif lower.startswith(\"increasing goal-directed persistence\"):\n",
    "        return \"Increasing Goal-Directed Persistence\"\n",
    "    elif lower.startswith(\"cultivating metacognition\"):\n",
    "        return \"Cultivating Metacognition\"\n",
    "    # Fallback: return the raw title\n",
    "    return title\n",
    "\n",
    "# -------------------------------\n",
    "# Step 3: Recursive Chunking Using RecursiveCharacterTextSplitter\n",
    "# -------------------------------\n",
    "def recursive_chunk_sections_recursivelangchain(\n",
    "    sections: List[Tuple[str, str]], \n",
    "    chunk_size: int = 1000, \n",
    "    chunk_overlap: int = 200\n",
    ") -> List[Document]:\n",
    "    docs_recursivelangchain = []\n",
    "    recursive_splitter_recursivelangchain = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "    )\n",
    "    for title, content in sections:\n",
    "        canonical_skill = map_title_to_skill_recursivelangchain(title)\n",
    "        chunks = recursive_splitter_recursivelangchain.split_text(content)\n",
    "        for chunk in chunks:\n",
    "            docs_recursivelangchain.append(Document(\n",
    "                page_content=chunk,\n",
    "                metadata={\n",
    "                    \"executive_skill\": canonical_skill,\n",
    "                    \"section_title\": title\n",
    "                }\n",
    "            ))\n",
    "    return docs_recursivelangchain\n",
    "\n",
    "# -------------------------------\n",
    "# Step 4: Load the Text File and Process into Documents\n",
    "# -------------------------------\n",
    "with open(\"Smartbutscattered.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    book_text_recursivelangchain = f.read()\n",
    "\n",
    "sections_recursivelangchain = split_by_markers_recursivelangchain(book_text_recursivelangchain)\n",
    "print(f\"Total sections found: {len(sections_recursivelangchain)}\")\n",
    "\n",
    "exec_docs_recursivelangchain = recursive_chunk_sections_recursivelangchain(\n",
    "    sections_recursivelangchain,\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200\n",
    ")\n",
    "print(f\"Total recursive chunks created: {len(exec_docs_recursivelangchain)}\")\n",
    "\n",
    "# (Optional) Write the resulting recursive chunks to a JSON file for inspection.\n",
    "with open(\"exec_skill_chunks_recursive_recursivelangchain.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(\n",
    "        [{\n",
    "            \"executive_skill\": doc.metadata[\"executive_skill\"],\n",
    "            \"section_title\": doc.metadata[\"section_title\"],\n",
    "            \"content\": doc.page_content\n",
    "        } for doc in exec_docs_recursivelangchain],\n",
    "        f,\n",
    "        indent=4\n",
    "    )\n",
    "\n",
    "# -------------------------------\n",
    "# Step 5: Embed & Store Documents in ChromaDB (with duplicate check)\n",
    "# -------------------------------\n",
    "# Initialize the embedding model.\n",
    "embedding_model_recursivelangchain = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Set the persistent directory.\n",
    "persist_dir_recursivelangchain = \"./chroma_exec_skills_db_recursivelangchain\"\n",
    "\n",
    "# Attempt to initialize a new Chroma client with our settings.\n",
    "try:\n",
    "    client_recursivelangchain = chromadb.Client(Settings(\n",
    "        chroma_db_impl=\"duckdb+parquet\",\n",
    "        persist_directory=persist_dir_recursivelangchain\n",
    "    ))\n",
    "except ValueError as e:\n",
    "    # If an instance already exists with different settings, reuse the existing client.\n",
    "    print(\"Chroma client already exists with different settings. Reusing the existing client.\")\n",
    "    client_recursivelangchain = chromadb.Client()  # reusing default ephemeral client\n",
    "\n",
    "# Create (or get) the collection.\n",
    "collection_recursivelangchain = client_recursivelangchain.get_or_create_collection(\n",
    "    name=\"exec_skills_recursivelangchain\"\n",
    ")\n",
    "\n",
    "# Fetch existing IDs from the collection (to avoid duplicates).\n",
    "existing_ids_recursivelangchain = set()\n",
    "try:\n",
    "    for sublist in collection_recursivelangchain.get()[\"ids\"]:\n",
    "        existing_ids_recursivelangchain.update(sublist)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# Loop over each document chunk and add it if not already stored.\n",
    "for idx, doc in enumerate(exec_docs_recursivelangchain):\n",
    "    doc_id_recursivelangchain = f\"skill_{doc.metadata['executive_skill']}_{idx}_recursivelangchain\"\n",
    "    if doc_id_recursivelangchain in existing_ids_recursivelangchain:\n",
    "        continue  # Skip duplicate chunks\n",
    "    embedding_recursivelangchain = embedding_model_recursivelangchain.encode(doc.page_content).tolist()\n",
    "    collection_recursivelangchain.add(\n",
    "        documents=[doc.page_content],\n",
    "        metadatas=[doc.metadata],\n",
    "        embeddings=[embedding_recursivelangchain],\n",
    "        ids=[doc_id_recursivelangchain]\n",
    "    )\n",
    "    existing_ids_recursivelangchain.add(doc_id_recursivelangchain)\n",
    "\n",
    "print(\"Embedding complete. All recursive chunks stored in the 'exec_skills_recursivelangchain' collection.\")\n",
    "\n",
    "# -------------------------------\n",
    "# Step 6: Query Example\n",
    "# -------------------------------\n",
    "query_text_recursivelangchain = \"What are some strategies to improve working memory?\"\n",
    "# Use a metadata filter that selects only the chunks with executive_skill = \"Working Memory\"\n",
    "metadata_filter_recursivelangchain = {\"executive_skill\": \"Working Memory\"}\n",
    "\n",
    "results_recursivelangchain = collection_recursivelangchain.query(\n",
    "    query_texts=[query_text_recursivelangchain],\n",
    "    n_results=5,\n",
    "    where=metadata_filter_recursivelangchain,\n",
    "    include=[\"documents\", \"metadatas\", \"distances\"]\n",
    ")\n",
    "\n",
    "flat_results_recursivelangchain = {\n",
    "    \"documents\": results_recursivelangchain[\"documents\"][0],\n",
    "    \"metadatas\": results_recursivelangchain[\"metadatas\"][0],\n",
    "    \"distances\": results_recursivelangchain[\"distances\"][0]\n",
    "}\n",
    "\n",
    "import pprint\n",
    "print(\"Query Results (for executive_skill 'Working Memory'):\")\n",
    "pprint.pprint(flat_results_recursivelangchain)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual Semantic Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk langchain chromadb sentence_transformers scikit-learn numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "from typing import List, Tuple\n",
    "from langchain.docstore.document import Document\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# -------------------------------\n",
    "# Step 1: Top-Level Splitting by Markers\n",
    "# -------------------------------\n",
    "def split_by_markers(text: str) -> List[Tuple[str, str]]:\n",
    "    \"\"\"\n",
    "    Splits the text using markers (lines starting with \"----\" and ending with \"----\").\n",
    "    Ignores markers that contain \"end\" (case-insensitive).\n",
    "    Returns a list of tuples: (section_title, section_content)\n",
    "    \"\"\"\n",
    "    marker_pattern = r\"(?m)^----\\s*(.*?)\\s*----\\s*$\"\n",
    "    markers = list(re.finditer(marker_pattern, text))\n",
    "    sections = []\n",
    "    for i, m in enumerate(markers):\n",
    "        title = m.group(1).strip()\n",
    "        if \"end\" in title.lower():\n",
    "            continue\n",
    "        start_index = m.end()\n",
    "        end_index = len(text)\n",
    "        for j in range(i+1, len(markers)):\n",
    "            next_title = markers[j].group(1).strip()\n",
    "            # If the next marker is an \"end\" marker or any marker then stop.\n",
    "            if \"end\" in next_title.lower() or next_title:\n",
    "                end_index = markers[j].start()\n",
    "                break\n",
    "        content = text[start_index:end_index].strip()\n",
    "        sections.append((title, content))\n",
    "    return sections\n",
    "\n",
    "# -------------------------------\n",
    "# Step 2: Sentence-Based Chunking with Overlap Using Semantic Grouping\n",
    "# -------------------------------\n",
    "def recursive_split_by_length(text: str, max_chunk_size: int) -> List[str]:\n",
    "    \"\"\"\n",
    "    Fallback: Split a given text (which should already be a coherent chunk)\n",
    "    into smaller pieces (using sentence boundaries) if it exceeds max_chunk_size.\n",
    "    \"\"\"\n",
    "    sentences = sent_tokenize(text)\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_length = 0\n",
    "    for sentence in sentences:\n",
    "        if current_length + len(sentence) + (1 if current_chunk else 0) <= max_chunk_size:\n",
    "            current_chunk.append(sentence)\n",
    "            current_length = len(\" \".join(current_chunk))\n",
    "        else:\n",
    "            if current_chunk:\n",
    "                chunks.append(\" \".join(current_chunk))\n",
    "            current_chunk = [sentence]\n",
    "            current_length = len(sentence) + 1\n",
    "    if current_chunk:\n",
    "        chunks.append(\" \".join(current_chunk))\n",
    "    return chunks\n",
    "\n",
    "def semantic_chunking(text: str, model: SentenceTransformer, similarity_threshold: float = 0.7, \n",
    "                      max_chunk_size: int = 1000) -> List[str]:\n",
    "    \"\"\"\n",
    "    Splits the text into sentences, embeds them, and clusters them using Agglomerative\n",
    "    Clustering based on cosine similarity. Groups adjacent sentences that are semantically similar.\n",
    "    If a group produces a chunk longer than max_chunk_size, further split it.\n",
    "    Returns a list of text chunks.\n",
    "    \"\"\"\n",
    "    sentences = sent_tokenize(text)\n",
    "    if not sentences:\n",
    "        return []\n",
    "    \n",
    "    # Compute embeddings for each sentence\n",
    "    embeddings = model.encode(sentences)\n",
    "    \n",
    "    # Perform clustering using AgglomerativeClustering.\n",
    "    # Replace the \"affinity\" parameter with \"metric\" to suit newer scikit-learn versions.\n",
    "    clustering = AgglomerativeClustering(n_clusters=None, metric='cosine', linkage='average',\n",
    "                                           distance_threshold=1 - similarity_threshold)\n",
    "    clustering.fit(embeddings)\n",
    "    cluster_labels = clustering.labels_\n",
    "    \n",
    "    chunks = []\n",
    "    current_cluster = cluster_labels[0]\n",
    "    current_sentences = []\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        if cluster_labels[i] == current_cluster:\n",
    "            current_sentences.append(sentence)\n",
    "        else:\n",
    "            chunk = \" \".join(current_sentences)\n",
    "            if len(chunk) > max_chunk_size:\n",
    "                sub_chunks = recursive_split_by_length(chunk, max_chunk_size)\n",
    "                chunks.extend(sub_chunks)\n",
    "            else:\n",
    "                chunks.append(chunk)\n",
    "            # Start the new chunk with overlap: include the last sentence of the previous group\n",
    "            overlap = [current_sentences[-1]] if current_sentences else []\n",
    "            current_sentences = overlap + [sentence]\n",
    "            current_cluster = cluster_labels[i]\n",
    "    if current_sentences:\n",
    "        chunk = \" \".join(current_sentences)\n",
    "        if len(chunk) > max_chunk_size:\n",
    "            sub_chunks = recursive_split_by_length(chunk, max_chunk_size)\n",
    "            chunks.extend(sub_chunks)\n",
    "        else:\n",
    "            chunks.append(chunk)\n",
    "    return chunks\n",
    "\n",
    "def split_section_semantically(sections: List[Tuple[str, str]], model: SentenceTransformer,\n",
    "                               max_chunk_size: int = 1000, similarity_threshold: float = 0.7) -> List[Document]:\n",
    "    \"\"\"\n",
    "    For each section, split the content into paragraphs (using blank lines), then apply semantic\n",
    "    chunking to each paragraph. Each resulting chunk becomes a Document with metadata.\n",
    "    \"\"\"\n",
    "    docs = []\n",
    "    \n",
    "    def map_title(title: str) -> str:\n",
    "        lower = title.lower()\n",
    "        if lower.startswith(\"building response inhibition\"):\n",
    "            return \"Response Inhibition\"\n",
    "        elif lower.startswith(\"enhancing working memory\"):\n",
    "            return \"Working Memory\"\n",
    "        # Extend mappings for additional sections as needed.\n",
    "        return title  # fallback using raw title\n",
    "    \n",
    "    for title, content in sections:\n",
    "        canonical_skill = map_title(title)\n",
    "        paragraphs = re.split(r\"\\n\\s*\\n\", content)\n",
    "        for para in paragraphs:\n",
    "            para = para.strip()\n",
    "            if not para:\n",
    "                continue\n",
    "            chunks = semantic_chunking(para, model, similarity_threshold=similarity_threshold,\n",
    "                                       max_chunk_size=max_chunk_size)\n",
    "            for chunk in chunks:\n",
    "                docs.append(Document(\n",
    "                    page_content=chunk,\n",
    "                    metadata={\"executive_skill\": canonical_skill, \"section_title\": title}\n",
    "                ))\n",
    "    return docs\n",
    "\n",
    "# -------------------------------\n",
    "# Step 3: Load the Text File and Process\n",
    "# -------------------------------\n",
    "with open(\"Smartbutscattered.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    book_text = f.read()\n",
    "\n",
    "sections = split_by_markers(book_text)\n",
    "\n",
    "# Initialize SentenceTransformer model once for chunking\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "exec_docs = split_section_semantically(sections, model=model, max_chunk_size=1000, similarity_threshold=0.7)\n",
    "\n",
    "# (Optional) Write the resulting chunks to a JSON file for inspection.\n",
    "with open(\"exec_skill_chunks.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(\n",
    "        [{\"executive_skill\": doc.metadata[\"executive_skill\"],\n",
    "          \"section_title\": doc.metadata[\"section_title\"],\n",
    "          \"content\": doc.page_content}\n",
    "         for doc in exec_docs],\n",
    "        f,\n",
    "        indent=4\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra for lesson plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import uuid\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "\n",
    "# -------------------------------\n",
    "# STEP 1: Read & Chunk the Lesson Plans\n",
    "# -------------------------------\n",
    "\n",
    "# Read the file that contains all lesson plans.\n",
    "with open(\"lessonplans.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    full_text = f.read()\n",
    "\n",
    "lesson_plan_blocks = re.findall(\n",
    "    r\"--- Start of Lesson Plan(.*?)--- End of Lesson Plan\",\n",
    "    full_text,\n",
    "    re.DOTALL\n",
    ")\n",
    "\n",
    "# Define the stop marker (we stop processing at this note)\n",
    "stop_marker = r\"Note: The following pages are intended for classroom use for students as a visual aid to learning\\.\"\n",
    "\n",
    "section_headers = {\n",
    "    \"assessment\": r\"Assessment\\s*\\n\",\n",
    "    \"extensions\": r\"Extensions(?: and Connections)?\\s*\\n\",\n",
    "    \"differentiation\": r\"Strategies for Differentiation\\s*\\n\"\n",
    "}\n",
    "\n",
    "chunked_lesson_plans = []\n",
    "\n",
    "for block in lesson_plan_blocks:\n",
    "    # 1. Limit processing to the content before the stop marker.\n",
    "    split_block = re.split(stop_marker, block, maxsplit=1)\n",
    "    content_to_process = split_block[0].strip() if split_block else block.strip()\n",
    "    \n",
    "    # 2. Split into intro_context and remainder using \"Student/Teacher Actions:\" (case-insensitive)\n",
    "    parts = re.split(r\"Student/Teacher Actions:\\s*\", content_to_process, flags=re.IGNORECASE, maxsplit=1)\n",
    "    if len(parts) == 2:\n",
    "        intro_context = parts[0].strip()\n",
    "        remainder = parts[1].strip()\n",
    "    else:\n",
    "        # If no \"Student/Teacher Actions:\" found, treat entire content as intro_context.\n",
    "        intro_context = content_to_process\n",
    "        remainder = \"\"\n",
    "\n",
    "    combined_pattern = r\"(Assessment\\s*\\n|Extensions(?: and Connections)?\\s*\\n|Strategies for Differentiation\\s*\\n)\"\n",
    "    split_sections = re.split(combined_pattern, remainder)\n",
    "    \n",
    "    # The first part (before any extra header) is the instructional_steps.\n",
    "    instructional_steps = split_sections[0].strip() if split_sections else \"\"\n",
    "    \n",
    "    # Initialize placeholders for extra sections.\n",
    "    assessment_text = \"\"\n",
    "    extensions_text = \"\"\n",
    "    differentiation_text = \"\"\n",
    "    \n",
    "    # Process remaining parts in pairs: header then content.\n",
    "    for i in range(1, len(split_sections) - 1, 2):\n",
    "        header = split_sections[i].strip().lower()\n",
    "        content = split_sections[i+1].strip()\n",
    "        if header.startswith(\"assessment\"):\n",
    "            assessment_text = content\n",
    "        elif header.startswith(\"extensions\"):\n",
    "            extensions_text = content\n",
    "        elif header.startswith(\"strategies for differentiation\"):\n",
    "            differentiation_text = content\n",
    "\n",
    "    # Build the JSON structure for this lesson plan.\n",
    "    lesson_plan_json = {\n",
    "        \"intro_context\": intro_context,\n",
    "        \"instructional_steps\": instructional_steps,\n",
    "        \"assessment\": assessment_text,\n",
    "        \"extensions\": extensions_text,\n",
    "        \"differentiation\": differentiation_text\n",
    "    }\n",
    "    \n",
    "    chunked_lesson_plans.append(lesson_plan_json)\n",
    "\n",
    "# Optionally, save the chunked lesson plans for later reference.\n",
    "with open(\"chunked_lesson_plans.json\", \"w\", encoding=\"utf-8\") as outfile:\n",
    "    json.dump(chunked_lesson_plans, outfile, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# STEP 2: Extract Metadata & Embed Chunks into ChromaDB\n",
    "# -------------------------------\n",
    "\n",
    "def extract_metadata_from_intro(intro_context):\n",
    "    \"\"\"\n",
    "    Extract additional metadata from the intro_context.\n",
    "    Attempts to extract:\n",
    "      - grade (e.g., \"Grade 5\")\n",
    "      - subject (from the \"Strand:\" line)\n",
    "      - topic (from the \"Topic:\" line)\n",
    "      - lesson_title (from the first line)\n",
    "    \"\"\"\n",
    "    grade_match = re.search(r\"Grade\\s*(\\d+)\", intro_context)\n",
    "    grade = grade_match.group(1) if grade_match else None\n",
    "    \n",
    "    subject_match = re.search(r\"Strand:\\s*(.+)\", intro_context)\n",
    "    subject = subject_match.group(1).strip() if subject_match else None\n",
    "    \n",
    "    topic_match = re.search(r\"Topic:\\s*(.+)\", intro_context)\n",
    "    topic = topic_match.group(1).strip() if topic_match else None\n",
    "    \n",
    "    title_line = intro_context.split(\"\\n\")[0].strip() if intro_context else \"\"\n",
    "    lesson_title = title_line if title_line else None\n",
    "    \n",
    "    return {\n",
    "        \"grade\": grade,\n",
    "        \"subject\": subject,\n",
    "        \"topic\": topic,\n",
    "        \"lesson_title\": lesson_title\n",
    "    }\n",
    "\n",
    "# Define a custom embedding function class that meets the new interface.\n",
    "class SentenceTransformerEmbedding:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "    def __call__(self, input):\n",
    "        # 'input' should be a list of strings.\n",
    "        return self.model.encode(input).tolist()\n",
    "\n",
    "# Initialize the embedding model.\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Initialize a ChromaDB client using the new client constructor.\n",
    "client = chromadb.Client()\n",
    "\n",
    "embedding_fn = SentenceTransformerEmbedding(model)\n",
    "collection = client.get_or_create_collection(\"lesson_plans\", embedding_function=embedding_fn)\n",
    "\n",
    "# Iterate over each lesson plan and each section.\n",
    "for lesson_index, lesson in enumerate(chunked_lesson_plans):\n",
    "\n",
    "    additional_metadata = extract_metadata_from_intro(lesson.get(\"intro_context\", \"\"))\n",
    "    \n",
    "\n",
    "    for section in [\"intro_context\", \"instructional_steps\", \"assessment\", \"extensions\", \"differentiation\"]:\n",
    "        text = lesson.get(section, \"\").strip()\n",
    "        if text:\n",
    "            # Create a unique ID for this chunk.\n",
    "            doc_id = f\"lesson{lesson_index}_{section}_{str(uuid.uuid4())[:8]}\"\n",
    "            # Compute the embedding.\n",
    "            embedding = model.encode(text).tolist()  # Convert numpy array to list.\n",
    "        \n",
    "            metadata = {\n",
    "                \"lesson_index\": lesson_index,\n",
    "                \"section\": section,\n",
    "                **additional_metadata  # Merge in grade, subject, topic, lesson_title.\n",
    "            }\n",
    "            collection.add(\n",
    "                documents=[text],\n",
    "                metadatas=[metadata],\n",
    "                embeddings=[embedding],\n",
    "                ids=[doc_id]\n",
    "            )\n",
    "\n",
    "print(\"All lesson plan chunks have been embedded and stored in the vector database.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# -------------------------------\n",
    "# Verification Example 1: Metadata Filter\n",
    "# -------------------------------\n",
    "\n",
    "results_metadata = collection.query(\n",
    "    query_texts=\"\", \n",
    "    n_results=100, \n",
    "    where={\"lesson_index\": 0}\n",
    ")\n",
    "\n",
    "print(\"Results filtered by metadata (lesson_index == 0):\")\n",
    "print(json.dumps(results_metadata, indent=4))\n",
    "\n",
    "query_texts = \"Computation and Estimation with Decimals\"\n",
    "results_similarity = collection.query(\n",
    "    query_texts=query_texts,\n",
    "    n_results=5,\n",
    "    where={\"lesson_index\": 0}\n",
    ")\n",
    "\n",
    "print(\"\\nResults from similarity search with query '{}':\".format(query_texts))\n",
    "print(json.dumps(results_similarity, indent=4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
