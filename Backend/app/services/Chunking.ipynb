{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 1)) (2.32.3)\n",
      "Requirement already satisfied: fitz in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 2)) (0.0.1.dev2)\n",
      "Requirement already satisfied: PyMuPDF in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 3)) (1.25.5)\n",
      "Requirement already satisfied: numpy in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 4)) (1.26.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 5)) (2.2.2)\n",
      "Requirement already satisfied: sentence-transformers in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 6)) (4.0.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 7)) (4.66.5)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 8)) (1.5.1)\n",
      "Requirement already satisfied: transformers in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 9)) (4.46.2)\n",
      "Requirement already satisfied: jupyterlab in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 10)) (4.2.5)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 11)) (3.9.2)\n",
      "Requirement already satisfied: cython in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 12)) (3.0.12)\n",
      "Requirement already satisfied: langchain-community in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 13)) (0.3.21)\n",
      "Requirement already satisfied: langchain in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 14)) (0.3.23)\n",
      "Requirement already satisfied: openai in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 15)) (0.28.1)\n",
      "Requirement already satisfied: tiktoken in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 16)) (0.9.0)\n",
      "Requirement already satisfied: nltk in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 17)) (3.9.1)\n",
      "Requirement already satisfied: chromadb in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 19)) (1.0.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from requests->-r requirements.txt (line 1)) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from requests->-r requirements.txt (line 1)) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from requests->-r requirements.txt (line 1)) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from requests->-r requirements.txt (line 1)) (2025.1.31)\n",
      "Requirement already satisfied: configobj in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from fitz->-r requirements.txt (line 2)) (5.0.9)\n",
      "Requirement already satisfied: configparser in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from fitz->-r requirements.txt (line 2)) (7.2.0)\n",
      "Requirement already satisfied: httplib2 in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from fitz->-r requirements.txt (line 2)) (0.22.0)\n",
      "Requirement already satisfied: nibabel in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from fitz->-r requirements.txt (line 2)) (5.3.2)\n",
      "Requirement already satisfied: nipype in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from fitz->-r requirements.txt (line 2)) (1.10.0)\n",
      "Requirement already satisfied: pyxnat in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from fitz->-r requirements.txt (line 2)) (1.6.3)\n",
      "Requirement already satisfied: scipy in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from fitz->-r requirements.txt (line 2)) (1.13.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from pandas->-r requirements.txt (line 5)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from pandas->-r requirements.txt (line 5)) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from pandas->-r requirements.txt (line 5)) (2023.3)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from sentence-transformers->-r requirements.txt (line 6)) (2.5.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from sentence-transformers->-r requirements.txt (line 6)) (0.26.2)\n",
      "Requirement already satisfied: Pillow in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from sentence-transformers->-r requirements.txt (line 6)) (10.4.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from sentence-transformers->-r requirements.txt (line 6)) (4.12.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from tqdm->-r requirements.txt (line 7)) (0.4.6)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from scikit-learn->-r requirements.txt (line 8)) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from scikit-learn->-r requirements.txt (line 8)) (3.5.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from transformers->-r requirements.txt (line 9)) (3.13.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from transformers->-r requirements.txt (line 9)) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from transformers->-r requirements.txt (line 9)) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from transformers->-r requirements.txt (line 9)) (2024.9.11)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from transformers->-r requirements.txt (line 9)) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from transformers->-r requirements.txt (line 9)) (0.20.3)\n",
      "Requirement already satisfied: async-lru>=1.0.0 in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from jupyterlab->-r requirements.txt (line 10)) (2.0.4)\n",
      "Requirement already satisfied: httpx>=0.25.0 in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from jupyterlab->-r requirements.txt (line 10)) (0.28.1)\n",
      "Requirement already satisfied: ipykernel>=6.5.0 in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from jupyterlab->-r requirements.txt (line 10)) (6.28.0)\n",
      "Requirement already satisfied: jinja2>=3.0.3 in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from jupyterlab->-r requirements.txt (line 10)) (3.1.4)\n",
      "Requirement already satisfied: jupyter-core in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from jupyterlab->-r requirements.txt (line 10)) (5.7.2)\n",
      "Requirement already satisfied: jupyter-lsp>=2.0.0 in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from jupyterlab->-r requirements.txt (line 10)) (2.2.0)\n",
      "Requirement already satisfied: jupyter-server<3,>=2.4.0 in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from jupyterlab->-r requirements.txt (line 10)) (2.14.1)\n",
      "Requirement already satisfied: jupyterlab-server<3,>=2.27.1 in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from jupyterlab->-r requirements.txt (line 10)) (2.27.3)\n",
      "Requirement already satisfied: notebook-shim>=0.2 in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from jupyterlab->-r requirements.txt (line 10)) (0.2.3)\n",
      "Requirement already satisfied: setuptools>=40.1.0 in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from jupyterlab->-r requirements.txt (line 10)) (75.1.0)\n",
      "Requirement already satisfied: tornado>=6.2.0 in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from jupyterlab->-r requirements.txt (line 10)) (6.4.1)\n",
      "Requirement already satisfied: traitlets in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from jupyterlab->-r requirements.txt (line 10)) (5.14.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from matplotlib->-r requirements.txt (line 11)) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from matplotlib->-r requirements.txt (line 11)) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from matplotlib->-r requirements.txt (line 11)) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from matplotlib->-r requirements.txt (line 11)) (1.4.4)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from matplotlib->-r requirements.txt (line 11)) (3.1.2)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.51 in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from langchain-community->-r requirements.txt (line 13)) (0.3.51)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from langchain-community->-r requirements.txt (line 13)) (2.0.34)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from langchain-community->-r requirements.txt (line 13)) (3.10.5)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from langchain-community->-r requirements.txt (line 13)) (8.2.3)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from langchain-community->-r requirements.txt (line 13)) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from langchain-community->-r requirements.txt (line 13)) (2.8.1)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.125 in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from langchain-community->-r requirements.txt (line 13)) (0.3.27)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from langchain-community->-r requirements.txt (line 13)) (0.4.0)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from langchain->-r requirements.txt (line 14)) (0.3.8)\n",
      "Collecting pydantic<3.0.0,>=2.7.4 (from langchain->-r requirements.txt (line 14))\n",
      "  Using cached pydantic-2.11.3-py3-none-any.whl.metadata (65 kB)\n",
      "Requirement already satisfied: click in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from nltk->-r requirements.txt (line 17)) (8.1.8)\n",
      "Requirement already satisfied: build>=1.0.3 in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from chromadb->-r requirements.txt (line 19)) (1.2.2.post1)\n",
      "Requirement already satisfied: chroma-hnswlib==0.7.6 in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from chromadb->-r requirements.txt (line 19)) (0.7.6)\n",
      "Requirement already satisfied: fastapi==0.115.9 in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from chromadb->-r requirements.txt (line 19)) (0.115.9)\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb->-r requirements.txt (line 19)) (0.34.0)\n",
      "Requirement already satisfied: posthog>=2.4.0 in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from chromadb->-r requirements.txt (line 19)) (3.23.0)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from chromadb->-r requirements.txt (line 19)) (1.21.0)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from chromadb->-r requirements.txt (line 19)) (1.31.1)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from chromadb->-r requirements.txt (line 19)) (1.31.1)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from chromadb->-r requirements.txt (line 19)) (0.52b1)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from chromadb->-r requirements.txt (line 19)) (1.31.1)\n",
      "Requirement already satisfied: pypika>=0.48.9 in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from chromadb->-r requirements.txt (line 19)) (0.48.9)\n",
      "Requirement already satisfied: overrides>=7.3.1 in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from chromadb->-r requirements.txt (line 19)) (7.4.0)\n",
      "Requirement already satisfied: importlib-resources in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from chromadb->-r requirements.txt (line 19)) (6.5.2)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from chromadb->-r requirements.txt (line 19)) (1.67.1)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from chromadb->-r requirements.txt (line 19)) (4.3.0)\n",
      "Requirement already satisfied: typer>=0.9.0 in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from chromadb->-r requirements.txt (line 19)) (0.13.1)\n",
      "Requirement already satisfied: kubernetes>=28.1.0 in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from chromadb->-r requirements.txt (line 19)) (32.0.1)\n",
      "Requirement already satisfied: mmh3>=4.0.1 in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from chromadb->-r requirements.txt (line 19)) (5.1.0)\n",
      "Requirement already satisfied: orjson>=3.9.12 in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from chromadb->-r requirements.txt (line 19)) (3.10.16)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from chromadb->-r requirements.txt (line 19)) (13.7.1)\n",
      "Requirement already satisfied: jsonschema>=4.19.0 in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from chromadb->-r requirements.txt (line 19)) (4.23.0)\n",
      "Requirement already satisfied: starlette<0.46.0,>=0.40.0 in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from fastapi==0.115.9->chromadb->-r requirements.txt (line 19)) (0.45.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community->-r requirements.txt (line 13)) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community->-r requirements.txt (line 13)) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community->-r requirements.txt (line 13)) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community->-r requirements.txt (line 13)) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community->-r requirements.txt (line 13)) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community->-r requirements.txt (line 13)) (1.11.0)\n",
      "Requirement already satisfied: pyproject_hooks in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from build>=1.0.3->chromadb->-r requirements.txt (line 19)) (1.2.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community->-r requirements.txt (line 13)) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community->-r requirements.txt (line 13)) (0.9.0)\n",
      "Requirement already satisfied: anyio in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from httpx>=0.25.0->jupyterlab->-r requirements.txt (line 10)) (4.8.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from httpx>=0.25.0->jupyterlab->-r requirements.txt (line 10)) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx>=0.25.0->jupyterlab->-r requirements.txt (line 10)) (0.14.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers->-r requirements.txt (line 6)) (2024.6.1)\n",
      "Requirement already satisfied: comm>=0.1.1 in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from ipykernel>=6.5.0->jupyterlab->-r requirements.txt (line 10)) (0.2.1)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from ipykernel>=6.5.0->jupyterlab->-r requirements.txt (line 10)) (1.6.7)\n",
      "Requirement already satisfied: ipython>=7.23.1 in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from ipykernel>=6.5.0->jupyterlab->-r requirements.txt (line 10)) (8.27.0)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from ipykernel>=6.5.0->jupyterlab->-r requirements.txt (line 10)) (8.6.0)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from ipykernel>=6.5.0->jupyterlab->-r requirements.txt (line 10)) (0.1.6)\n",
      "Requirement already satisfied: nest-asyncio in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from ipykernel>=6.5.0->jupyterlab->-r requirements.txt (line 10)) (1.6.0)\n",
      "Requirement already satisfied: psutil in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from ipykernel>=6.5.0->jupyterlab->-r requirements.txt (line 10)) (5.9.0)\n",
      "Requirement already satisfied: pyzmq>=24 in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from ipykernel>=6.5.0->jupyterlab->-r requirements.txt (line 10)) (25.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from jinja2>=3.0.3->jupyterlab->-r requirements.txt (line 10)) (2.1.3)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from jsonschema>=4.19.0->chromadb->-r requirements.txt (line 19)) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from jsonschema>=4.19.0->chromadb->-r requirements.txt (line 19)) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from jsonschema>=4.19.0->chromadb->-r requirements.txt (line 19)) (0.10.6)\n",
      "Requirement already satisfied: platformdirs>=2.5 in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from jupyter-core->jupyterlab->-r requirements.txt (line 10)) (3.10.0)\n",
      "Requirement already satisfied: pywin32>=300 in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from jupyter-core->jupyterlab->-r requirements.txt (line 10)) (305.1)\n",
      "Requirement already satisfied: argon2-cffi>=21.1 in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 10)) (21.3.0)\n",
      "Requirement already satisfied: jupyter-events>=0.9.0 in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 10)) (0.10.0)\n",
      "Requirement already satisfied: jupyter-server-terminals>=0.4.4 in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 10)) (0.4.4)\n",
      "Requirement already satisfied: nbconvert>=6.4.4 in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 10)) (7.16.4)\n",
      "Requirement already satisfied: nbformat>=5.3.0 in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 10)) (5.10.4)\n",
      "Requirement already satisfied: prometheus-client>=0.9 in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 10)) (0.14.1)\n",
      "Requirement already satisfied: pywinpty>=2.0.1 in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 10)) (2.0.10)\n",
      "Requirement already satisfied: send2trash>=1.8.2 in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 10)) (1.8.2)\n",
      "Requirement already satisfied: terminado>=0.8.3 in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 10)) (0.17.1)\n",
      "Requirement already satisfied: websocket-client>=1.7 in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 10)) (1.8.0)\n",
      "Requirement already satisfied: babel>=2.10 in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab->-r requirements.txt (line 10)) (2.11.0)\n",
      "Requirement already satisfied: json5>=0.9.0 in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab->-r requirements.txt (line 10)) (0.9.6)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromadb->-r requirements.txt (line 19)) (1.17.0)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromadb->-r requirements.txt (line 19)) (2.38.0)\n",
      "Requirement already satisfied: requests-oauthlib in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromadb->-r requirements.txt (line 19)) (2.0.0)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromadb->-r requirements.txt (line 19)) (3.2.2)\n",
      "Requirement already satisfied: durationpy>=0.7 in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromadb->-r requirements.txt (line 19)) (0.9)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.51->langchain-community->-r requirements.txt (line 13)) (1.33)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-community->-r requirements.txt (line 13)) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-community->-r requirements.txt (line 13)) (0.23.0)\n",
      "Requirement already satisfied: coloredlogs in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb->-r requirements.txt (line 19)) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb->-r requirements.txt (line 19)) (24.3.25)\n",
      "Requirement already satisfied: protobuf in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb->-r requirements.txt (line 19)) (5.29.4)\n",
      "Requirement already satisfied: sympy in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb->-r requirements.txt (line 19)) (1.13.1)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from opentelemetry-api>=1.2.0->chromadb->-r requirements.txt (line 19)) (1.2.18)\n",
      "Requirement already satisfied: importlib-metadata<8.7.0,>=6.0 in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from opentelemetry-api>=1.2.0->chromadb->-r requirements.txt (line 19)) (7.0.1)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb->-r requirements.txt (line 19)) (1.69.2)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.31.1 in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb->-r requirements.txt (line 19)) (1.31.1)\n",
      "Requirement already satisfied: opentelemetry-proto==1.31.1 in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb->-r requirements.txt (line 19)) (1.31.1)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-asgi==0.52b1 in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb->-r requirements.txt (line 19)) (0.52b1)\n",
      "Requirement already satisfied: opentelemetry-instrumentation==0.52b1 in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb->-r requirements.txt (line 19)) (0.52b1)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.52b1 in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb->-r requirements.txt (line 19)) (0.52b1)\n",
      "Requirement already satisfied: opentelemetry-util-http==0.52b1 in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb->-r requirements.txt (line 19)) (0.52b1)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from opentelemetry-instrumentation==0.52b1->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb->-r requirements.txt (line 19)) (1.14.1)\n",
      "Requirement already satisfied: asgiref~=3.0 in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from opentelemetry-instrumentation-asgi==0.52b1->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb->-r requirements.txt (line 19)) (3.8.1)\n",
      "Requirement already satisfied: monotonic>=1.5 in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from posthog>=2.4.0->chromadb->-r requirements.txt (line 19)) (1.6)\n",
      "Requirement already satisfied: backoff>=1.10.0 in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from posthog>=2.4.0->chromadb->-r requirements.txt (line 19)) (2.2.1)\n",
      "Requirement already satisfied: distro>=1.5.0 in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from posthog>=2.4.0->chromadb->-r requirements.txt (line 19)) (1.9.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain->-r requirements.txt (line 14)) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.1 in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain->-r requirements.txt (line 14)) (2.33.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain->-r requirements.txt (line 14)) (0.4.0)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community->-r requirements.txt (line 13)) (1.0.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from rich>=10.11.0->chromadb->-r requirements.txt (line 19)) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from rich>=10.11.0->chromadb->-r requirements.txt (line 19)) (2.15.1)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain-community->-r requirements.txt (line 13)) (3.0.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers->-r requirements.txt (line 6)) (3.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from sympy->onnxruntime>=1.14.1->chromadb->-r requirements.txt (line 19)) (1.3.0)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from typer>=0.9.0->chromadb->-r requirements.txt (line 19)) (1.5.4)\n",
      "Requirement already satisfied: httptools>=0.6.3 in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb->-r requirements.txt (line 19)) (0.6.4)\n",
      "Requirement already satisfied: watchfiles>=0.13 in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb->-r requirements.txt (line 19)) (1.0.5)\n",
      "Requirement already satisfied: websockets>=10.4 in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb->-r requirements.txt (line 19)) (15.0.1)\n",
      "Requirement already satisfied: prov>=1.5.2 in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from nipype->fitz->-r requirements.txt (line 2)) (2.0.1)\n",
      "Requirement already satisfied: pydot>=1.2.3 in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from nipype->fitz->-r requirements.txt (line 2)) (3.0.4)\n",
      "Requirement already satisfied: rdflib>=5.0.0 in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from nipype->fitz->-r requirements.txt (line 2)) (6.3.2)\n",
      "Requirement already satisfied: simplejson>=3.8.0 in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from nipype->fitz->-r requirements.txt (line 2)) (3.20.1)\n",
      "Requirement already satisfied: traits>=6.2 in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from nipype->fitz->-r requirements.txt (line 2)) (7.0.2)\n",
      "Requirement already satisfied: acres in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from nipype->fitz->-r requirements.txt (line 2)) (0.3.0)\n",
      "Requirement already satisfied: etelemetry>=0.3.1 in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from nipype->fitz->-r requirements.txt (line 2)) (0.3.1)\n",
      "Requirement already satisfied: looseversion!=1.2 in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from nipype->fitz->-r requirements.txt (line 2)) (1.3.0)\n",
      "Requirement already satisfied: puremagic in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from nipype->fitz->-r requirements.txt (line 2)) (1.28)\n",
      "Requirement already satisfied: lxml>=4.3 in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from pyxnat->fitz->-r requirements.txt (line 2)) (5.2.1)\n",
      "Requirement already satisfied: pathlib>=1.0 in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from pyxnat->fitz->-r requirements.txt (line 2)) (1.0.1)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from anyio->httpx>=0.25.0->jupyterlab->-r requirements.txt (line 10)) (1.3.1)\n",
      "Requirement already satisfied: argon2-cffi-bindings in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 10)) (21.2.0)\n",
      "Requirement already satisfied: ci-info>=0.2 in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from etelemetry>=0.3.1->nipype->fitz->-r requirements.txt (line 2)) (0.3.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb->-r requirements.txt (line 19)) (5.3.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb->-r requirements.txt (line 19)) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb->-r requirements.txt (line 19)) (4.9)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from importlib-metadata<8.7.0,>=6.0->opentelemetry-api>=1.2.0->chromadb->-r requirements.txt (line 19)) (3.17.0)\n",
      "Requirement already satisfied: decorator in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab->-r requirements.txt (line 10)) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab->-r requirements.txt (line 10)) (0.19.1)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab->-r requirements.txt (line 10)) (3.0.43)\n",
      "Requirement already satisfied: stack-data in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab->-r requirements.txt (line 10)) (0.2.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.51->langchain-community->-r requirements.txt (line 13)) (2.1)\n",
      "Requirement already satisfied: python-json-logger>=2.0.4 in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 10)) (2.0.7)\n",
      "Requirement already satisfied: rfc3339-validator in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 10)) (0.1.4)\n",
      "Requirement already satisfied: rfc3986-validator>=0.1.1 in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 10)) (0.1.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb->-r requirements.txt (line 19)) (0.1.0)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 10)) (4.12.3)\n",
      "Requirement already satisfied: bleach!=5.0.0 in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 10)) (4.1.0)\n",
      "Requirement already satisfied: defusedxml in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 10)) (0.7.1)\n",
      "Requirement already satisfied: jupyterlab-pygments in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 10)) (0.1.2)\n",
      "Requirement already satisfied: mistune<4,>=2.0.3 in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 10)) (2.0.4)\n",
      "Requirement already satisfied: nbclient>=0.5.0 in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 10)) (0.8.0)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 10)) (1.5.0)\n",
      "Requirement already satisfied: tinycss2 in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 10)) (1.2.1)\n",
      "Requirement already satisfied: fastjsonschema>=2.15 in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from nbformat>=5.3.0->jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 10)) (2.16.2)\n",
      "Requirement already satisfied: isodate<0.7.0,>=0.6.0 in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from rdflib>=5.0.0->nipype->fitz->-r requirements.txt (line 2)) (0.6.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community->-r requirements.txt (line 13)) (1.0.0)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb->-r requirements.txt (line 19)) (10.0)\n",
      "Requirement already satisfied: webencodings in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from bleach!=5.0.0->nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 10)) (0.5.1)\n",
      "Requirement already satisfied: pyreadline3 in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from humanfriendly>=9.1->coloredlogs->onnxruntime>=1.14.1->chromadb->-r requirements.txt (line 19)) (3.5.4)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab->-r requirements.txt (line 10)) (0.8.3)\n",
      "Requirement already satisfied: fqdn in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 10)) (1.5.1)\n",
      "Requirement already satisfied: isoduration in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 10)) (20.11.0)\n",
      "Requirement already satisfied: uri-template in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 10)) (1.3.0)\n",
      "Requirement already satisfied: webcolors>=24.6.0 in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 10)) (24.11.1)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab->-r requirements.txt (line 10)) (0.2.5)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb->-r requirements.txt (line 19)) (0.4.8)\n",
      "Requirement already satisfied: cffi>=1.0.1 in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 10)) (1.17.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from beautifulsoup4->nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 10)) (2.5)\n",
      "Requirement already satisfied: executing in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from stack-data->ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab->-r requirements.txt (line 10)) (0.8.3)\n",
      "Requirement already satisfied: asttokens in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from stack-data->ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab->-r requirements.txt (line 10)) (2.0.5)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from stack-data->ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab->-r requirements.txt (line 10)) (0.2.2)\n",
      "Requirement already satisfied: pycparser in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 10)) (2.22)\n",
      "Requirement already satisfied: arrow>=0.15.0 in c:\\users\\sheth\\anaconda3\\lib\\site-packages (from isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->-r requirements.txt (line 10)) (1.2.3)\n",
      "Using cached pydantic-2.11.3-py3-none-any.whl (443 kB)\n",
      "Installing collected packages: pydantic\n",
      "  Attempting uninstall: pydantic\n",
      "    Found existing installation: pydantic 1.10.21\n",
      "    Uninstalling pydantic-1.10.21:\n",
      "      Successfully uninstalled pydantic-1.10.21\n",
      "Successfully installed pydantic-2.11.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\sheth\\anaconda3\\Lib\\site-packages\\~~dantic'.\n",
      "  You can safely remove it manually.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "chroma-migrate 0.0.7 requires duckdb==0.7.1, but you have duckdb 1.2.1 which is incompatible.\n",
      "thinc 8.3.2 requires numpy<2.1.0,>=2.0.0; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip uninstall duckdb chroma-migrate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chunking Lesson plan with Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n1. Hierarchically chunked by markers and sections.\\n2. Embedded with metadata including grade, subject, topic, and section type.\\n3. Queried using metadata filters that match fields such as grade and topic.\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# High Level explanation for lesson plan chunking: \n",
    "\"\"\"\n",
    "1. Hierarchically chunked by markers and sections.\n",
    "2. Embedded with metadata including grade, subject, topic, and section type.\n",
    "3. Queried using metadata filters that match fields such as grade and topic.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/myenv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "import uuid\n",
    "from typing import List\n",
    "from langchain.text_splitter import TextSplitter\n",
    "from langchain.docstore.document import Document\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "\n",
    "# -------------------------------\n",
    "# 1. Our own Custom TextSplitter \n",
    "# note: we are using Hierarchical Chunking\n",
    "# -------------------------------\n",
    "class LessonPlanTextSplitter(TextSplitter):\n",
    "    def split_text(self, text: str) -> List[Document]:\n",
    "        stop_marker = r\"Note: The following pages are intended for classroom use for students as a visual aid to learning\\.\"\n",
    "        split_block = re.split(stop_marker, text, maxsplit=1)\n",
    "        content = split_block[0].strip() if split_block else text.strip()\n",
    "\n",
    "        # Split into intro and remainder\n",
    "        parts = re.split(r\"Student/Teacher Actions:\\s*\", content, flags=re.IGNORECASE, maxsplit=1)\n",
    "        if len(parts) == 2:\n",
    "            intro, remainder = parts[0].strip(), parts[1].strip()\n",
    "        else:\n",
    "            intro, remainder = content, \"\"\n",
    "\n",
    "        docs = []\n",
    "        docs.append(Document(page_content=intro, metadata={\"section\": \"intro_context\"}))\n",
    "\n",
    "        combined_pattern = r\"(Assessment\\s*\\n|Extensions(?: and Connections)?\\s*\\n|Strategies for Differentiation\\s*\\n)\"\n",
    "        split_sec = re.split(combined_pattern, remainder)\n",
    "\n",
    "        # Instructional steps (first segment)\n",
    "        instr = split_sec[0].strip()\n",
    "        if instr:\n",
    "            docs.append(Document(page_content=instr, metadata={\"section\": \"instructional_steps\"}))\n",
    "\n",
    "        # Subsequent header/content pairs\n",
    "        for i in range(1, len(split_sec) - 1, 2):\n",
    "            header = split_sec[i].strip().lower()\n",
    "            content = split_sec[i + 1].strip()\n",
    "            if header.startswith(\"assessment\"):\n",
    "                docs.append(Document(page_content=content, metadata={\"section\": \"assessment\"}))\n",
    "            elif header.startswith(\"extensions\"):\n",
    "                docs.append(Document(page_content=content, metadata={\"section\": \"extensions\"}))\n",
    "            elif header.startswith(\"strategies for differentiation\"):\n",
    "                docs.append(Document(page_content=content, metadata={\"section\": \"differentiation\"}))\n",
    "\n",
    "        return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# 2. Chunk the Lesson Plans \n",
    "# note: here our main file is test_with_notes.txt where all our lesson plan is stored\n",
    "# -------------------------------\n",
    "with open(\"test_with_notes.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    full_text = f.read()\n",
    "\n",
    "# Split into individual lesson plan blocks\n",
    "lesson_plan_blocks = re.findall(\n",
    "    r\"--- Start of Lesson Plan(.*?)--- End of Lesson Plan\",\n",
    "    full_text,\n",
    "    re.DOTALL\n",
    ")\n",
    "\n",
    "splitter = LessonPlanTextSplitter()\n",
    "all_docs: List[Document] = []\n",
    "\n",
    "for lesson_index, block in enumerate(lesson_plan_blocks):\n",
    "    docs = splitter.split_text(block)\n",
    "    # Tag each chunk with its lesson index\n",
    "    for doc in docs:\n",
    "        doc.metadata[\"lesson_index\"] = lesson_index\n",
    "    all_docs.extend(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# 3. Extract Lesson-Level Metadata \n",
    "# note: here our metda data is split based on lesson index and then we have grade, subject, topic and lesson title\n",
    "# -------------------------------\n",
    "def extract_metadata_from_intro(intro: str):\n",
    "    grade = re.search(r\"Grade\\s*(\\d+)\", intro)\n",
    "    subject = re.search(r\"Strand:\\s*(.+)\", intro)\n",
    "    topic = re.search(r\"Topic:\\s*(.+)\", intro)\n",
    "    title = intro.split(\"\\n\")[0].strip()\n",
    "    return {\n",
    "        \"grade\": grade.group(1) if grade else None,\n",
    "        \"subject\": subject.group(1).strip() if subject else None,\n",
    "        \"topic\": topic.group(1).strip() if topic else None,\n",
    "        \"lesson_title\": title\n",
    "    }\n",
    "\n",
    "# Build map: lesson_index -> metadata\n",
    "lesson_metadata_map = {}\n",
    "for doc in all_docs:\n",
    "    if doc.metadata[\"section\"] == \"intro_context\":\n",
    "        lesson_metadata_map[doc.metadata[\"lesson_index\"]] = extract_metadata_from_intro(doc.page_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: chroma-migrate in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (0.0.7)\n",
      "Requirement already satisfied: clickhouse-connect==0.6.6 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from chroma-migrate) (0.6.6)\n",
      "Requirement already satisfied: duckdb==0.7.1 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from chroma-migrate) (0.7.1)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from chroma-migrate) (4.67.1)\n",
      "Requirement already satisfied: chromadb in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from chroma-migrate) (0.6.3)\n",
      "Requirement already satisfied: chroma-bullet in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from chroma-migrate) (2.2.0)\n",
      "Requirement already satisfied: more-itertools>=9.1.0 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from chroma-migrate) (10.6.0)\n",
      "Requirement already satisfied: certifi in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from clickhouse-connect==0.6.6->chroma-migrate) (2025.1.31)\n",
      "Requirement already satisfied: importlib-metadata in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from clickhouse-connect==0.6.6->chroma-migrate) (8.6.1)\n",
      "Requirement already satisfied: urllib3>=1.26 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from clickhouse-connect==0.6.6->chroma-migrate) (2.3.0)\n",
      "Requirement already satisfied: pytz in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from clickhouse-connect==0.6.6->chroma-migrate) (2025.1)\n",
      "Requirement already satisfied: zstandard in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from clickhouse-connect==0.6.6->chroma-migrate) (0.23.0)\n",
      "Requirement already satisfied: lz4 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from clickhouse-connect==0.6.6->chroma-migrate) (4.4.4)\n",
      "Requirement already satisfied: build>=1.0.3 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from chromadb->chroma-migrate) (1.2.2.post1)\n",
      "Requirement already satisfied: pydantic>=1.9 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from chromadb->chroma-migrate) (2.10.6)\n",
      "Requirement already satisfied: chroma-hnswlib==0.7.6 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from chromadb->chroma-migrate) (0.7.6)\n",
      "Requirement already satisfied: fastapi>=0.95.2 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from chromadb->chroma-migrate) (0.115.12)\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from uvicorn[standard]>=0.18.3->chromadb->chroma-migrate) (0.34.0)\n",
      "Requirement already satisfied: numpy>=1.22.5 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from chromadb->chroma-migrate) (2.0.2)\n",
      "Requirement already satisfied: posthog>=2.4.0 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from chromadb->chroma-migrate) (3.23.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from chromadb->chroma-migrate) (4.12.2)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from chromadb->chroma-migrate) (1.19.2)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from chromadb->chroma-migrate) (1.31.1)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from chromadb->chroma-migrate) (1.31.1)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from chromadb->chroma-migrate) (0.52b1)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from chromadb->chroma-migrate) (1.31.1)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from chromadb->chroma-migrate) (0.21.1)\n",
      "Requirement already satisfied: pypika>=0.48.9 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from chromadb->chroma-migrate) (0.48.9)\n",
      "Requirement already satisfied: overrides>=7.3.1 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from chromadb->chroma-migrate) (7.7.0)\n",
      "Requirement already satisfied: importlib-resources in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from chromadb->chroma-migrate) (6.5.2)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from chromadb->chroma-migrate) (1.71.0)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from chromadb->chroma-migrate) (4.3.0)\n",
      "Requirement already satisfied: typer>=0.9.0 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from chromadb->chroma-migrate) (0.15.2)\n",
      "Requirement already satisfied: kubernetes>=28.1.0 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from chromadb->chroma-migrate) (32.0.1)\n",
      "Requirement already satisfied: tenacity>=8.2.3 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from chromadb->chroma-migrate) (9.0.0)\n",
      "Requirement already satisfied: PyYAML>=6.0.0 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from chromadb->chroma-migrate) (6.0.2)\n",
      "Requirement already satisfied: mmh3>=4.0.1 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from chromadb->chroma-migrate) (5.1.0)\n",
      "Requirement already satisfied: orjson>=3.9.12 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from chromadb->chroma-migrate) (3.10.16)\n",
      "Requirement already satisfied: httpx>=0.27.0 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from chromadb->chroma-migrate) (0.28.1)\n",
      "Requirement already satisfied: rich>=10.11.0 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from chromadb->chroma-migrate) (13.9.4)\n",
      "Requirement already satisfied: packaging>=19.1 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from build>=1.0.3->chromadb->chroma-migrate) (24.2)\n",
      "Requirement already satisfied: pyproject_hooks in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from build>=1.0.3->chromadb->chroma-migrate) (1.2.0)\n",
      "Requirement already satisfied: tomli>=1.1.0 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from build>=1.0.3->chromadb->chroma-migrate) (2.2.1)\n",
      "Requirement already satisfied: starlette<0.47.0,>=0.40.0 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from fastapi>=0.95.2->chromadb->chroma-migrate) (0.46.1)\n",
      "Requirement already satisfied: anyio in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from httpx>=0.27.0->chromadb->chroma-migrate) (4.9.0)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from httpx>=0.27.0->chromadb->chroma-migrate) (1.0.7)\n",
      "Requirement already satisfied: idna in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from httpx>=0.27.0->chromadb->chroma-migrate) (3.10)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from httpcore==1.*->httpx>=0.27.0->chromadb->chroma-migrate) (0.14.0)\n",
      "Requirement already satisfied: zipp>=3.20 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from importlib-metadata->clickhouse-connect==0.6.6->chroma-migrate) (3.21.0)\n",
      "Requirement already satisfied: six>=1.9.0 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from kubernetes>=28.1.0->chromadb->chroma-migrate) (1.17.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from kubernetes>=28.1.0->chromadb->chroma-migrate) (2.9.0.post0)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from kubernetes>=28.1.0->chromadb->chroma-migrate) (2.38.0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from kubernetes>=28.1.0->chromadb->chroma-migrate) (1.8.0)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from kubernetes>=28.1.0->chromadb->chroma-migrate) (2.32.3)\n",
      "Requirement already satisfied: requests-oauthlib in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from kubernetes>=28.1.0->chromadb->chroma-migrate) (2.0.0)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from kubernetes>=28.1.0->chromadb->chroma-migrate) (3.2.2)\n",
      "Requirement already satisfied: durationpy>=0.7 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from kubernetes>=28.1.0->chromadb->chroma-migrate) (0.9)\n",
      "Requirement already satisfied: coloredlogs in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from onnxruntime>=1.14.1->chromadb->chroma-migrate) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from onnxruntime>=1.14.1->chromadb->chroma-migrate) (25.2.10)\n",
      "Requirement already satisfied: protobuf in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from onnxruntime>=1.14.1->chromadb->chroma-migrate) (5.29.4)\n",
      "Requirement already satisfied: sympy in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from onnxruntime>=1.14.1->chromadb->chroma-migrate) (1.13.1)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from opentelemetry-api>=1.2.0->chromadb->chroma-migrate) (1.2.18)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb->chroma-migrate) (1.69.2)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.31.1 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb->chroma-migrate) (1.31.1)\n",
      "Requirement already satisfied: opentelemetry-proto==1.31.1 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb->chroma-migrate) (1.31.1)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-asgi==0.52b1 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb->chroma-migrate) (0.52b1)\n",
      "Requirement already satisfied: opentelemetry-instrumentation==0.52b1 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb->chroma-migrate) (0.52b1)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.52b1 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb->chroma-migrate) (0.52b1)\n",
      "Requirement already satisfied: opentelemetry-util-http==0.52b1 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb->chroma-migrate) (0.52b1)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from opentelemetry-instrumentation==0.52b1->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb->chroma-migrate) (1.17.2)\n",
      "Requirement already satisfied: asgiref~=3.0 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from opentelemetry-instrumentation-asgi==0.52b1->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb->chroma-migrate) (3.8.1)\n",
      "Requirement already satisfied: monotonic>=1.5 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from posthog>=2.4.0->chromadb->chroma-migrate) (1.6)\n",
      "Requirement already satisfied: backoff>=1.10.0 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from posthog>=2.4.0->chromadb->chroma-migrate) (2.2.1)\n",
      "Requirement already satisfied: distro>=1.5.0 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from posthog>=2.4.0->chromadb->chroma-migrate) (1.9.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from pydantic>=1.9->chromadb->chroma-migrate) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from pydantic>=1.9->chromadb->chroma-migrate) (2.27.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from rich>=10.11.0->chromadb->chroma-migrate) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from rich>=10.11.0->chromadb->chroma-migrate) (2.19.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from tokenizers>=0.13.2->chromadb->chroma-migrate) (0.29.3)\n",
      "Requirement already satisfied: click>=8.0.0 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from typer>=0.9.0->chromadb->chroma-migrate) (8.1.8)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from typer>=0.9.0->chromadb->chroma-migrate) (1.5.4)\n",
      "Requirement already satisfied: httptools>=0.6.3 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from uvicorn[standard]>=0.18.3->chromadb->chroma-migrate) (0.6.4)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from uvicorn[standard]>=0.18.3->chromadb->chroma-migrate) (1.1.0)\n",
      "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from uvicorn[standard]>=0.18.3->chromadb->chroma-migrate) (0.21.0)\n",
      "Requirement already satisfied: watchfiles>=0.13 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from uvicorn[standard]>=0.18.3->chromadb->chroma-migrate) (1.0.4)\n",
      "Requirement already satisfied: websockets>=10.4 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from uvicorn[standard]>=0.18.3->chromadb->chroma-migrate) (15.0.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb->chroma-migrate) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb->chroma-migrate) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb->chroma-migrate) (4.9)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb->chroma-migrate) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb->chroma-migrate) (2025.3.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb->chroma-migrate) (0.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from requests->kubernetes>=28.1.0->chromadb->chroma-migrate) (3.4.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from anyio->httpx>=0.27.0->chromadb->chroma-migrate) (1.2.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from anyio->httpx>=0.27.0->chromadb->chroma-migrate) (1.3.1)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb->chroma-migrate) (10.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from sympy->onnxruntime>=1.14.1->chromadb->chroma-migrate) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb->chroma-migrate) (0.6.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install chroma-migrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding complete. All chunks stored with full metadata.\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from chromadb import PersistentClient\n",
    "\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "client = PersistentClient(path=\"./chroma_store\")\n",
    "collection = client.get_or_create_collection(\"lesson_plans\")\n",
    "\n",
    "# Fetch existing IDs\n",
    "existing_ids = set(collection.get()[\"ids\"])\n",
    "\n",
    "for doc in all_docs:\n",
    "    lm = lesson_metadata_map.get(doc.metadata[\"lesson_index\"], {})\n",
    "    doc.metadata.update(lm)\n",
    "    cleaned_metadata = {k: v for k, v in doc.metadata.items() if v is not None}\n",
    "\n",
    "    doc_id = f\"lesson{cleaned_metadata['lesson_index']}_{cleaned_metadata['section']}\"\n",
    "    if doc_id in existing_ids:\n",
    "        continue\n",
    "\n",
    "    embedding = model.encode(doc.page_content).tolist()\n",
    "    collection.add(\n",
    "        documents=[doc.page_content],\n",
    "        metadatas=[cleaned_metadata],\n",
    "        embeddings=[embedding],\n",
    "        ids=[doc_id]\n",
    "    )\n",
    "    existing_ids.add(doc_id)\n",
    "\n",
    "print(\"Embedding complete. All chunks stored with full metadata.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"ids\": [\n",
      "        \"lesson266_intro_context\",\n",
      "        \"lesson266_instructional_steps\",\n",
      "        \"lesson266_differentiation\",\n",
      "        \"lesson266_assessment\",\n",
      "        \"lesson266_extensions\"\n",
      "    ],\n",
      "    \"documents\": [\n",
      "        \"Grade 5-Computation and Estimation-5.5ab - Party Time: Computation and Estimation with Decimals.pdf ---\\nParty Time: Computation and Estimation\\nwith Decimals\\nStrand:\\nComputation and Estimation\\nTopic:\\nSolving practical problems involving decimal estimation and computation.\\nPrimary SOL:\\n5.5 The student will\\na) estimate and determine the product, and quotient of two numbers\\ninvolving decimals; and\\nb) create and solve single-step and multistep practical problems\\ninvolving addition, subtraction, and multiplication of decimals, and\\ncreate and solve single-step practical problems involving division of\\ndecimals.\\nRelated SOL:\\n5.4\\nMaterials\\n\\uf0b7 Bundle of Books advertisement (attached)\\n\\uf0b7 Party Time! activity sheet (attached)\\n\\uf0b7 Pencil and paper for each student\\n\\uf0b7 Calculator (optional)\\n\\uf0b7 Large paper and markers for each small group\\nVocabulary\\nbudget, difference, product, quotient, sum\",\n",
      "        \"What should students be doing? What should teachers be doing?\\n1. Bundles of Books\\na. Tell the students that they have $50 to spend on new books for the classroom.\\nDisplay the Bundle of Books advertisement. Have students read the flyer, and ask\\nthem to think about books they would like to purchase for the classroom.\\nb. Ask students, \\u201cHow would you estimate the cost of three dictionaries?\\u201d Allow\\nstudents to share their responses and strategies for estimating. Then have students\\nestimate what books on the list they might purchase without exceeding the $50\\nallotment. While students are estimating, circulate and ask students questions\\nabout their estimates: \\u201cHow did you estimate your book totals?\\u201d \\u201cWhat operations\\ndid you use?\\u201d Allow students to share their estimates with the class.\\u201d Ask, \\u201cWho\\nthinks their estimates are close to $50?\\u201d Determine who is closest to $50 without\\ngoing over.\\nc. Have each student compute the actual cost of the books they selected and the\\namount of money that would be left over. Then ask them to compare their\\nestimates to their actual book amounts. Allow students to share their results with\\nthe class.\\n2. Party Time\\na. Tell the students that they will be planning an end-of\\u2013the-year party for the class,\\nand they have $100 to spend on food, decorations, and games. Distribute the Party\\nTime! activity sheet, which lists the cost of each item. Working with partners, have\\nstudents list the items they would select for the party. Have them use estimation to\\nstop when they think they are close to the $100 total. Ask students to exchange\\npapers and figure the actual cost of the party and the amount of money left over.\\nDetermine who came closest to spending all the money without going over.\\nb. Have partners/small groups depict their party budgets on large butcher or chart\\npaper. Post the budgets around the classroom, and allow members of each group\\nto explain their budgets to the class and share how they came up with their\\nestimates.\\nc. Decimal Division: Present students with the following situation: \\u201cIt costs $2.25 for a\\npack of forks, and each pack has 10 forks. How would you figure out the cost of one\\nfork?\\u201d Allow partners to discuss and work out this problem, and then have\\nvolunteers share their solutions with the class. Have students then complete the\\nchart in part B of the Party Time! activity sheet, showing their computation on a\\nseparate sheet.\\n3. Bring closure to the activity by posing questions such as, \\u201cWhen you go shopping, what do\\nyou need to consider when estimating to find the total cost?\\u201d and \\u201cWhat are some reasons\\nthat an estimation might be less than an exact cost?\\u201d\",\n",
      "        \"\\uf0b7 Use grid paper to assist students in lining up vertical columns.\\n\\uf0b7 Use a calculator to help with computation.\\n\\uf0b7 Some students may need to have the items followed by the cost in a list rather than a\\nbrochure.\",\n",
      "        \"\\uf0b7 Questions\\no How does your estimate compare to your actual amount? Why do you think you are\\nover the exact cost? Under the exact cost?\\no When planning your party, which operations did you use? Why?\\no When did you need more than one operation?\\no How does estimating the cost help with the exact cost?\\n\\uf0b7 Journal/writing prompts\\no Describe the steps you took in planning your party, and identify the operations you\\nused.\\no What did you learn about budgeting as you planned your party? For example, was it\\nhard or easy to stay within your budget? What things did you have to change or go\\nwithout? Which items do you wish you could purchase and why?\\n\\uf0b7 Other Assessments\\no While students work on their party purchases, circulate around the room and\\nobserve: What estimation strategies are students using? Are students\\ndemonstrating an understanding of each of the operations? Are they understanding\\nthe problem and using reasonable operations to get to correct solutions?\\no Have students calculate how much it would cost for each party guest given a set\\nbudget amount. For example, if the given budget is $100, and 28 people were to\\nattend the party, about how much could be spent on each guest?\",\n",
      "        \"\\uf0b7 Suppose your budget was increased to $150. How would you adjust your spending for the\\nparty?\\n\\uf0b7 What other items would you need for your party that are not included on the Party Time!\\nlist? Search through catalogs and sale flyers for the prices of these items, and adjust your\\nbudgets to include them for your party.\\n\\uf0b7 Have students search the internet to select items to decorate a new bedroom. Purchases\\ncan include furniture, electronics, or other items the students would like in the room. Keep\\na record of the purchases on a chart. Calculate totals and subtract from a given budget.\\nHave students also determine how many hours of work it would take to pay for the items,\\ngiven different hourly wages.\"\n",
      "    ],\n",
      "    \"metadatas\": [\n",
      "        {\n",
      "            \"grade\": \"5\",\n",
      "            \"lesson_index\": 266,\n",
      "            \"lesson_title\": \"Grade 5-Computation and Estimation-5.5ab - Party Time: Computation and Estimation with Decimals.pdf ---\",\n",
      "            \"section\": \"intro_context\",\n",
      "            \"subject\": \"Computation and Estimation\",\n",
      "            \"topic\": \"Solving practical problems involving decimal estimation and computation.\"\n",
      "        },\n",
      "        {\n",
      "            \"grade\": \"5\",\n",
      "            \"lesson_index\": 266,\n",
      "            \"lesson_title\": \"Grade 5-Computation and Estimation-5.5ab - Party Time: Computation and Estimation with Decimals.pdf ---\",\n",
      "            \"section\": \"instructional_steps\",\n",
      "            \"subject\": \"Computation and Estimation\",\n",
      "            \"topic\": \"Solving practical problems involving decimal estimation and computation.\"\n",
      "        },\n",
      "        {\n",
      "            \"grade\": \"5\",\n",
      "            \"lesson_index\": 266,\n",
      "            \"lesson_title\": \"Grade 5-Computation and Estimation-5.5ab - Party Time: Computation and Estimation with Decimals.pdf ---\",\n",
      "            \"section\": \"differentiation\",\n",
      "            \"subject\": \"Computation and Estimation\",\n",
      "            \"topic\": \"Solving practical problems involving decimal estimation and computation.\"\n",
      "        },\n",
      "        {\n",
      "            \"grade\": \"5\",\n",
      "            \"lesson_index\": 266,\n",
      "            \"lesson_title\": \"Grade 5-Computation and Estimation-5.5ab - Party Time: Computation and Estimation with Decimals.pdf ---\",\n",
      "            \"section\": \"assessment\",\n",
      "            \"subject\": \"Computation and Estimation\",\n",
      "            \"topic\": \"Solving practical problems involving decimal estimation and computation.\"\n",
      "        },\n",
      "        {\n",
      "            \"grade\": \"5\",\n",
      "            \"lesson_index\": 266,\n",
      "            \"lesson_title\": \"Grade 5-Computation and Estimation-5.5ab - Party Time: Computation and Estimation with Decimals.pdf ---\",\n",
      "            \"section\": \"extensions\",\n",
      "            \"subject\": \"Computation and Estimation\",\n",
      "            \"topic\": \"Solving practical problems involving decimal estimation and computation.\"\n",
      "        }\n",
      "    ],\n",
      "    \"distances\": [\n",
      "        0.420373797416687,\n",
      "        1.0712968111038208,\n",
      "        1.116530418395996,\n",
      "        1.1645891666412354,\n",
      "        1.2971526384353638\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# STEP 5: Querying the Lesson Plan\n",
    "# note: trial sample query to check if its working\n",
    "# -------------------------------\n",
    "import json\n",
    "\n",
    "query_text = \"How can I solve practical problems involving decimal estimation and computation for grade 5?\"\n",
    "\n",
    "metadata_filter = {\n",
    "    \"$and\": [\n",
    "        {\"grade\": \"5\"},\n",
    "        {\"topic\": \"Solving practical problems involving decimal estimation and computation.\"}\n",
    "    ]\n",
    "}\n",
    "\n",
    "results = collection.query(\n",
    "    query_texts=[query_text],\n",
    "    n_results=5,\n",
    "    where=metadata_filter,\n",
    "    include=[\"documents\", \"metadatas\", \"distances\"]\n",
    ")\n",
    "\n",
    "flat = {\n",
    "    \"ids\": results[\"ids\"][0],\n",
    "    \"documents\": results[\"documents\"][0],\n",
    "    \"metadatas\": results[\"metadatas\"][0],\n",
    "    \"distances\": results[\"distances\"][0]\n",
    "}\n",
    "\n",
    "# Pretty-print for verification\n",
    "print(json.dumps(flat, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grade 5-Computation and Estimation-5.5ab - Party Time: Computation and Estimation with Decimals.pdf ---\n",
      "Party Time: Computation and Estimation\n",
      "with Decimals\n",
      "Strand:\n",
      "Computation and Estimation\n",
      "Topic:\n",
      "Solving practical problems involving decimal estimation and computation.\n",
      "Primary SOL:\n",
      "5.5 The student will\n",
      "a) estimate and determine the product, and quotient of two numbers\n",
      "involving decimals; and\n",
      "b) create and solve single-step and multistep practical problems\n",
      "involving addition, subtraction, and multiplication of decimals, and\n",
      "create and solve single-step practical problems involving division of\n",
      "decimals.\n",
      "Related SOL:\n",
      "5.4\n",
      "Materials\n",
      " Bundle of Books advertisement (attached)\n",
      " Party Time! activity sheet (attached)\n",
      " Pencil and paper for each student\n",
      " Calculator (optional)\n",
      " Large paper and markers for each small group\n",
      "Vocabulary\n",
      "budget, difference, product, quotient, sum\n",
      "\n",
      "What should students be doing? What should teachers be doing?\n",
      "1. Bundles of Books\n",
      "a. Tell the students that they have $50 to spend on new books for the classroom.\n",
      "Display the Bundle of Books advertisement. Have students read the flyer, and ask\n",
      "them to think about books they would like to purchase for the classroom.\n",
      "b. Ask students, “How would you estimate the cost of three dictionaries?” Allow\n",
      "students to share their responses and strategies for estimating. Then have students\n",
      "estimate what books on the list they might purchase without exceeding the $50\n",
      "allotment. While students are estimating, circulate and ask students questions\n",
      "about their estimates: “How did you estimate your book totals?” “What operations\n",
      "did you use?” Allow students to share their estimates with the class.” Ask, “Who\n",
      "thinks their estimates are close to $50?” Determine who is closest to $50 without\n",
      "going over.\n",
      "c. Have each student compute the actual cost of the books they selected and the\n",
      "amount of money that would be left over. Then ask them to compare their\n",
      "estimates to their actual book amounts. Allow students to share their results with\n",
      "the class.\n",
      "2. Party Time\n",
      "a. Tell the students that they will be planning an end-of–the-year party for the class,\n",
      "and they have $100 to spend on food, decorations, and games. Distribute the Party\n",
      "Time! activity sheet, which lists the cost of each item. Working with partners, have\n",
      "students list the items they would select for the party. Have them use estimation to\n",
      "stop when they think they are close to the $100 total. Ask students to exchange\n",
      "papers and figure the actual cost of the party and the amount of money left over.\n",
      "Determine who came closest to spending all the money without going over.\n",
      "b. Have partners/small groups depict their party budgets on large butcher or chart\n",
      "paper. Post the budgets around the classroom, and allow members of each group\n",
      "to explain their budgets to the class and share how they came up with their\n",
      "estimates.\n",
      "c. Decimal Division: Present students with the following situation: “It costs $2.25 for a\n",
      "pack of forks, and each pack has 10 forks. How would you figure out the cost of one\n",
      "fork?” Allow partners to discuss and work out this problem, and then have\n",
      "volunteers share their solutions with the class. Have students then complete the\n",
      "chart in part B of the Party Time! activity sheet, showing their computation on a\n",
      "separate sheet.\n",
      "3. Bring closure to the activity by posing questions such as, “When you go shopping, what do\n",
      "you need to consider when estimating to find the total cost?” and “What are some reasons\n",
      "that an estimation might be less than an exact cost?”\n",
      "\n",
      " Use grid paper to assist students in lining up vertical columns.\n",
      " Use a calculator to help with computation.\n",
      " Some students may need to have the items followed by the cost in a list rather than a\n",
      "brochure.\n",
      "\n",
      " Questions\n",
      "o How does your estimate compare to your actual amount? Why do you think you are\n",
      "over the exact cost? Under the exact cost?\n",
      "o When planning your party, which operations did you use? Why?\n",
      "o When did you need more than one operation?\n",
      "o How does estimating the cost help with the exact cost?\n",
      " Journal/writing prompts\n",
      "o Describe the steps you took in planning your party, and identify the operations you\n",
      "used.\n",
      "o What did you learn about budgeting as you planned your party? For example, was it\n",
      "hard or easy to stay within your budget? What things did you have to change or go\n",
      "without? Which items do you wish you could purchase and why?\n",
      " Other Assessments\n",
      "o While students work on their party purchases, circulate around the room and\n",
      "observe: What estimation strategies are students using? Are students\n",
      "demonstrating an understanding of each of the operations? Are they understanding\n",
      "the problem and using reasonable operations to get to correct solutions?\n",
      "o Have students calculate how much it would cost for each party guest given a set\n",
      "budget amount. For example, if the given budget is $100, and 28 people were to\n",
      "attend the party, about how much could be spent on each guest?\n",
      "\n",
      " Suppose your budget was increased to $150. How would you adjust your spending for the\n",
      "party?\n",
      " What other items would you need for your party that are not included on the Party Time!\n",
      "list? Search through catalogs and sale flyers for the prices of these items, and adjust your\n",
      "budgets to include them for your party.\n",
      " Have students search the internet to select items to decorate a new bedroom. Purchases\n",
      "can include furniture, electronics, or other items the students would like in the room. Keep\n",
      "a record of the purchases on a chart. Calculate totals and subtract from a given budget.\n",
      "Have students also determine how many hours of work it would take to pay for the items,\n",
      "given different hourly wages.\n"
     ]
    }
   ],
   "source": [
    "# note: Just for confirmation we are printing full lesson plan\n",
    "# Concatenate all 5 documents into a full lesson text if we want to use it as a single lesson\n",
    "full_lesson = \"\\n\\n\".join(flat[\"documents\"])\n",
    "\n",
    "print(full_lesson)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chunking textbook using Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n1. Processed using both recursive (and optionally semantic) chunking approaches.\\n\\n2. Chunked further into smaller text pieces with overlap to preserve context.\\n\\n3.Embedded and stored with metadata (e.g., executive_skill and section_title).\\n\\n4.Retrieval is performed using metadata filters such as \"executive_skill\": \"Working Memory\"\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # High Level explanation\n",
    "\"\"\"\n",
    "1. Processed using both recursive (and optionally semantic) chunking approaches.\n",
    "\n",
    "2. Chunked further into smaller text pieces with overlap to preserve context.\n",
    "\n",
    "3.Embedded and stored with metadata (e.g., executive_skill and section_title).\n",
    "\n",
    "4.Retrieval is performed using metadata filters such as \"executive_skill\": \"Working Memory\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (0.3.23)\n",
      "Requirement already satisfied: langchain-experimental in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (0.3.4)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.51 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from langchain) (0.3.51)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from langchain) (0.3.8)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from langchain) (0.3.19)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from langchain) (2.10.6)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from langchain) (2.0.39)\n",
      "Requirement already satisfied: requests<3,>=2 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from langchain) (4.0.3)\n",
      "Requirement already satisfied: langchain-community<0.4.0,>=0.3.0 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from langchain-experimental) (0.3.20)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental) (3.11.14)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental) (9.0.0)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental) (2.8.1)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental) (0.4.0)\n",
      "Requirement already satisfied: numpy<3,>=1.26.2 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental) (2.0.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from langchain-core<1.0.0,>=0.3.51->langchain) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from langchain-core<1.0.0,>=0.3.51->langchain) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from langchain-core<1.0.0,>=0.3.51->langchain) (4.12.2)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.16)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from requests<3,>=2->langchain) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from requests<3,>=2->langchain) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from requests<3,>=2->langchain) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from requests<3,>=2->langchain) (2025.1.31)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (6.2.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (1.18.3)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (0.9.0)\n",
      "Requirement already satisfied: anyio in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (4.9.0)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.51->langchain) (3.0.0)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (1.1.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (1.0.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.2.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.3.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U langchain langchain-experimental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sections: 24\n",
      "Section 1: introduction\n",
      "Section 2: end of introduction\n",
      "Section 3: Building Response Inhibition\n",
      "Section 4: end of Building Response Inhibition\n",
      "Section 5: Enhancing Working Memory\n",
      "Section 6: end of Enhancing Working Memory\n",
      "Section 7: Improving Emotional Control\n",
      "Section 8: end of Improving Emotional Control\n",
      "Section 9: Strengthening Sustained Attention\n",
      "Section 10: end of Strengthening Sustained Attention\n",
      "Section 11: Teaching Task Initiation\n",
      "Section 12: end of Teaching Task Initiation\n",
      "Section 13: Promoting, Planning, and Prioritizing\n",
      "Section 14: end of Promoting, Planning, and Prioritizing\n",
      "Section 15: Fostering Organization\n",
      "Section 16: end of Fostering Organization\n",
      "Section 17: Instilling Time Management\n",
      "Section 18: end of Instilling Time Management\n",
      "Section 19: Encouraging Flexibility\n",
      "Section 20: end of Encouraging Flexibility\n",
      "Section 21: Increasing Goal-Directed Persistence\n",
      "Section 22: end of Increasing Goal-Directed Persistence\n",
      "Section 23: Cultivating Metacognition\n",
      "Section 24: end of Cultivating Metacognition\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "# --- Step 1: Load and split the text file by markers ---\n",
    "def split_by_markers(text: str):\n",
    "    \"\"\"\n",
    "    Splits the text into sections based on markers of the form \"---- <section title> ----\"\n",
    "    Returns a list of tuples: (section_title, content)\n",
    "    \"\"\"\n",
    "    pattern = r\"----\\s*(.*?)\\s*----\\n(.*?)(?=----|$)\"\n",
    "    matches = re.findall(pattern, text, re.DOTALL)\n",
    "    sections = [(title.strip(), content.strip()) for title, content in matches]\n",
    "    return sections\n",
    "\n",
    "with open(\"Smartbutscattered.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    book_text = f.read()\n",
    "\n",
    "sections = split_by_markers(book_text)\n",
    "print(f\"Total sections: {len(sections)}\")\n",
    "for i, (title, _) in enumerate(sections):\n",
    "    print(f\"Section {i+1}: {title}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total semantic chunks created: 119\n"
     ]
    }
   ],
   "source": [
    "# --- Step 2: Semantic chunking using LangChain's SemanticChunker ---\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "chunker = SemanticChunker(embeddings=embedding_model)\n",
    "\n",
    "docs = []\n",
    "for section_title, content in sections:\n",
    "    metadata = {\"executive_skill\": section_title, \"section_title\": section_title}\n",
    "    chunks = chunker.split_text(content)\n",
    "    for chunk in chunks:\n",
    "        docs.append(Document(page_content=chunk, metadata=metadata))\n",
    "\n",
    "print(f\"Total semantic chunks created: {len(docs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 119 new chunks to the collection.\n"
     ]
    }
   ],
   "source": [
    "# --- Step 3: Store chunks into Chroma DB with duplicate checking ---\n",
    "import chromadb\n",
    "\n",
    "# Initialize the Chroma client and create/get the \"exec_skills\" collection.\n",
    "# client = chromadb.Client()\n",
    "collection = client.get_or_create_collection(\"exec_skills\")\n",
    "\n",
    "existing_result = collection.get() \n",
    "existing_ids = set(existing_result.get(\"ids\", []))\n",
    "\n",
    "new_ids = []\n",
    "new_docs = []\n",
    "new_metadatas = []\n",
    "new_embeddings = []\n",
    "\n",
    "for i, doc in enumerate(docs):\n",
    "    # Create a deterministic ID using a sequential number and the executive_skill (with spaces removed)\n",
    "    doc_id = f\"exec_{i}_{doc.metadata['executive_skill'].replace(' ', '_')}\"\n",
    "    if doc_id in existing_ids:\n",
    "        continue\n",
    "\n",
    "    embedding = embedding_model.embed_documents([doc.page_content])[0]\n",
    "    new_ids.append(doc_id)\n",
    "    new_docs.append(doc.page_content)\n",
    "    new_metadatas.append(doc.metadata)\n",
    "    new_embeddings.append(embedding)\n",
    "\n",
    "if new_ids:\n",
    "    collection.add(\n",
    "        ids=new_ids,\n",
    "        documents=new_docs,\n",
    "        metadatas=new_metadatas,\n",
    "        embeddings=new_embeddings\n",
    "    )\n",
    "    print(f\"Added {len(new_ids)} new chunks to the collection.\")\n",
    "else:\n",
    "    print(\"No new chunks to add.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query results:\n",
      "{\n",
      "    \"documents\": [\n",
      "        \"Enhancing Working Memory Working memory is the capacity to hold information in mind while performing complex tasks. We rely on working memory all the time. It\\u2019s the ability to run out to the store to buy a few things and remember what they are without having to write them down. When you remember to stop by the dry cleaner on your way home from work, you\\u2019re using working memory. When you look up a phone number in the phone book and remember it long enough to make the call, you\\u2019re using working memory. When your spouse asks you to do something and you say, \\u201cI\\u2019ll do it as soon as I finish loading the dishwasher,\\u201d and then you actually remember to do it, chances are your working memory is pretty good. Odds are it\\u2019s not so good, however, if you can\\u2019t remember anyone\\u2019s birthday, you tend to return home with only half your errands done unless you have a written agenda, and you\\u2019ll do anything to avoid having to introduce people at a cocktail party because you can\\u2019t remember anyone\\u2019s name. In that case, be sure to use the tips in Chapter 3 to help you enhance your child\\u2019s working memory when you have the same weakness.\",\n",
      "        \"How Working Memory Develops Working memory begins to develop fairly early in infancy. When you\\u2019re playing with a baby and you hide a favorite toy under a blanket, you know the baby is using working memory if he lifts the blanket to retrieve the toy. This is because the baby is able to hold an image of the toy in mind as well as the memory of what you did to hide it. Children develop nonverbal working memory before they develop verbal working memory because this skill begins to emerge before language does. When children develop language, however, their working memory skills expand, because now they can draw on visual imagery and language to retrieve information. As we told you in Chapter 1, when children and teenagers perform tasks that require executive skills such as working memory, they rely on the prefrontal cortex to do all the work rather than distributing the workload to other specialized regions of the brain, as adults are able to do. Thus, activating working memory takes more conscious effort with children and with teenagers than it does with adults, which may help explain why they are less inclined to use their working memory to complete daily routine tasks. We tend naturally to limit our expectations for working memory in very young children. Before the age of 3, we generally expect children to remember only things that are in close proximity\\u2014either in time or in space. If we want them to do something, we don\\u2019t say, \\u201cWould you mind putting your toys away after you finish watchingBarney ?\\u201d (unless we also expect to cue them once Barney is over). And while we might ask them to put all their blocks in the toy box while we\\u2019re standing in the playroom with them, we generally don\\u2019t instruct them to go to their bedroom and do a similar task all by themselves. Gradually, we\\u2019re able to stretch both time and distance in terms of what we expect our children to be able to remember. In the questionnaire below, you can evaluate where your child might fall on the developmental ladder, based on the kinds of tasks children are capable of carrying out independently at various childhood stages. Using this scale will give you a closer look than the scales in Chapter did at how well your child uses the skill of working memory. HOW GOOD ISYOUR CHILD\\u2019SWORKING MEMORY? Use the following scale to rate how well your child performs e ach of the tasks listed. At each level, children can be expected to perform all the tasks listed fairly well to very well. Scale \\u2014 Never or rarely \\u2014 Does but not well (about 25%of the time) \\u2014 Does fairly well (about 75%of the time) \\u2014 Does very well (always or almost always) Preschool/kindergarten Runs simple errands (e.g., gets shoes from bedroom when aske d) Remembers instructions that were just given Follows a routine with only one prompt per step (e.g., brushi ng teeth after breakfast)198 PUTTING IT ALL TOGETHER Lower elementary (grades1\\u20133) Able to run an errand with two to three steps Remembers instructions that were given a couple of minutes e arlier Follows two steps of a routine with one prompt Upper elementary (grades4\\u20135) Remembers to perform a routine chore after school without re minder Takes books, papers, assignments to and from school Keeps track of changing daily schedule (e.g., different act ivities after school) Middle school (grades6\\u20138) Able to keep track of assignments and classroom expectation s of multiple teachers Remembers events or responsibilities that deviate from the norm (e.g., permission slips for field trips, special instructions regard ing extracurricular activities, etc.) Remembers multistep directions, given sufficient time or p ractice From Smart but Scattered by Peg Dawson and Richard Guare. Copyright 2009 by The Guilfo rd Press.\",\n",
      "        \"Building Working Memory in Everyday Situations \\u2022Make eye contact with your child before telling him something you want him to remember. \\u2022Keep external distractions to a minimum if you want your child\\u2019s full attention (for example, turn off the television or turn down the volume). \\u2022Have the child repeat back to you what you just said so you know she has heard you. \\u2022Use written reminders \\u2014picture schedules, lists, and schedules, depending on the age of the child. Prompt the child at each step to \\u201ccheck your schedule\\u201d or \\u201clook at your list.\\u201d \\u2022Rehearse with the child what you expect him to remember just before the situation (for example, \\u201cWhat do you need to say to Aunt Mary after she gives you your birthday present?\\u201d). \\u2022Help the child think about ways to help her remember something important that she thinks will work for her. \\u2022With children in middle school, use cell phones, text messages, or instant messages (IMs) to remind them of important things they have to do.Enhancing Working Memory \\u2022Consider using a reward for remembering key information or imposing a penalty for forgetting. For example, a child might be allowed to rent a video game on the weekend if he goes a whole week without forgetting to bring home all his homework materials. Rewards and penalties are useful when your child\\u2019s working memory is only mildly underdeveloped. Ending the Waiting Game: Teaching Y our Child to Get Dressed without Dawdling Annie is a bright 8-year-old second grader who can be absent-minded at times but is one of the more advanced students in her class. She has a variety of interests and is a good friend to her peers.\",\n",
      "        \"They are satisfied that the cutout will work. Step 1: Establish Behavioral Goal Target executive skill(s): Working memory Specific behavioral objective: Jake will organize his sports equipment before each game and have the equipment he needs for each game with no more than one a dult prompt Step 2: Design Intervention What environmental supports will be provided to help reach the target goal? \\u2022 A cutout of Jake that will be labeled with equipment needed for pract ice and games. \\u2022 Reminder from his dad the night before a game to check and pack his equip - ment. What specific skill will be taught, who will teach the skill, and what procedure will be used to teach it? Skill: Working memory (remember all required sports equipment for practice and games)Enhancing Working Memory (cont.) Who will teach the skill? Father Procedure: \\u2022 Jake and Dad meet and agree on a plan for organizing the equipment.\",\n",
      "        \"\\u2022 Jake, with Dad\\u2019s help, makes a cutout. \\u2022 Jake makes labels and hooks for all equipment and puts them on the cutou t. \\u2022 He tries one practice run with his father watching. \\u2022 Dad agrees to cue him to get equipment ready the night before. \\u2022 For two weeks, Dad checks with him after he has given the cue to ensure that he has followed through. What incentives will be used to help motivate the child to use/practice the skill? \\u2022 Jake will be able to participate in sports without experiencing cons equences from coaches for not having equipment. Keysto Success \\u2022Don\\u2019t rely on your child\\u2019s statement that he or she has acted on your cue. In this example the cutout served as a reminder and an organizing too l for Jake. While this may be enough in most cases, children with working memor y weaknesses, when asked about or cued to remember something, will often in dicate that they have done what they need to do or will take care of it and then pr oceed to forget. Therefore you\\u2019ll need to follow up the cue with a check to see if your child has in fact acted on the cue. Acting at the time that the cue is g iven is key, which may involve your checking on a more frequent basis until the d esired behavior is established.204 PUTTING IT ALL TOGETHER\"\n",
      "    ],\n",
      "    \"metadatas\": [\n",
      "        {\n",
      "            \"executive_skill\": \"Enhancing Working Memory\",\n",
      "            \"section_title\": \"Enhancing Working Memory\"\n",
      "        },\n",
      "        {\n",
      "            \"executive_skill\": \"Enhancing Working Memory\",\n",
      "            \"section_title\": \"Enhancing Working Memory\"\n",
      "        },\n",
      "        {\n",
      "            \"executive_skill\": \"Enhancing Working Memory\",\n",
      "            \"section_title\": \"Enhancing Working Memory\"\n",
      "        },\n",
      "        {\n",
      "            \"executive_skill\": \"Enhancing Working Memory\",\n",
      "            \"section_title\": \"Enhancing Working Memory\"\n",
      "        },\n",
      "        {\n",
      "            \"executive_skill\": \"Enhancing Working Memory\",\n",
      "            \"section_title\": \"Enhancing Working Memory\"\n",
      "        }\n",
      "    ],\n",
      "    \"distances\": [\n",
      "        0.8731342554092407,\n",
      "        0.903792679309845,\n",
      "        0.9836469292640686,\n",
      "        1.2383180856704712,\n",
      "        1.352657675743103\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    " #--- Step 4: Query test using metadata filtering ---\n",
    "# For this example, we query for chunks where the executive_skill is \"Enhancing Working Memory\".\n",
    "query_text = \"What strategies can help improve working memory?\"\n",
    "metadata_filter = {\"executive_skill\": \"Enhancing Working Memory\"}\n",
    "\n",
    "results = collection.query(\n",
    "    query_texts=[query_text],\n",
    "    n_results=5,\n",
    "    where=metadata_filter,\n",
    "    include=[\"documents\", \"metadatas\", \"distances\"]\n",
    ")\n",
    "\n",
    "if results[\"documents\"]:\n",
    "    flat = {\n",
    "        \"documents\": results[\"documents\"][0],\n",
    "        \"metadatas\": results[\"metadatas\"][0],\n",
    "        \"distances\": results[\"distances\"][0]\n",
    "    }\n",
    "    print(\"Query results:\")\n",
    "    print(json.dumps(flat, indent=4))\n",
    "else:\n",
    "    print(\"No documents retrieved for the given query filter.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra things done - different methods used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## parabased chunking for tb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import uuid\n",
    "from typing import List, Tuple\n",
    "from langchain.docstore.document import Document\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "\n",
    "# -------------------------------\n",
    "# 1. Top-Level Splitting by Explicit Section Markers\n",
    "# -------------------------------\n",
    "def split_by_markers(text: str) -> List[Tuple[str, str]]:\n",
    "    \"\"\"\n",
    "    Splits text into sections based on markers that look like:\n",
    "      ---- section title ----\n",
    "    Ignores markers with \"end\" (case-insensitive).\n",
    "    \n",
    "    Returns a list of tuples: (section_title, section_content)\n",
    "    \"\"\"\n",
    "    marker_pattern = r\"(?m)^----\\s*(.*?)\\s*----\\s*$\"\n",
    "    markers = list(re.finditer(marker_pattern, text))\n",
    "    sections = []\n",
    "    for i, m in enumerate(markers):\n",
    "        title = m.group(1).strip()\n",
    "        if \"end\" in title.lower():\n",
    "            continue\n",
    "        start_index = m.end()\n",
    "        end_index = len(text)\n",
    "        # Look for the next marker that is not an \"end\" marker or take rest of text.\n",
    "        for j in range(i+1, len(markers)):\n",
    "            next_title = markers[j].group(1).strip()\n",
    "            if \"end\" not in next_title.lower():\n",
    "                end_index = markers[j].start()\n",
    "                break\n",
    "            else:\n",
    "                end_index = markers[j].start()\n",
    "                break\n",
    "        content = text[start_index:end_index].strip()\n",
    "        sections.append((title, content))\n",
    "    return sections\n",
    "\n",
    "# -------------------------------\n",
    "# 2. Second-Level Splitting by Paragraphs\n",
    "# -------------------------------\n",
    "def split_section_paragraphs(sections: List[Tuple[str, str]]) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Splits each section by paragraphs (using two or more newlines) and creates a Document for each chunk.\n",
    "    Applies a simple mapping rule for sections representing executive skills.\n",
    "    \"\"\"\n",
    "    docs = []\n",
    "    \n",
    "    def map_title(title: str) -> str:\n",
    "        lower = title.lower()\n",
    "        if lower.startswith(\"building response inhibition\"):\n",
    "            return \"Response Inhibition\"\n",
    "        elif lower.startswith(\"enhancing working memory\"):\n",
    "            return \"Working Memory\"\n",
    "        # Add additional mappings if needed.\n",
    "        return title  # Default\n",
    "    \n",
    "    for title, content in sections:\n",
    "        mapped_title = map_title(title)\n",
    "        # Split by paragraph (two or more newlines)\n",
    "        paragraphs = re.split(r\"\\n\\s*\\n\", content)\n",
    "        for para in paragraphs:\n",
    "            para = para.strip()\n",
    "            if para:\n",
    "                docs.append(Document(\n",
    "                    page_content=para,\n",
    "                    metadata={\"executive_skill\": mapped_title, \"section_title\": title}\n",
    "                ))\n",
    "    return docs\n",
    "\n",
    "# -------------------------------\n",
    "# 3. Load \"Smartbutscattered.txt\" and Process\n",
    "# -------------------------------\n",
    "with open(\"Smartbutscattered.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    book_text = f.read()\n",
    "\n",
    "sections = split_by_markers(book_text)\n",
    "exec_docs = split_section_paragraphs(sections)\n",
    "\n",
    "# Optional: Save structured chunks to a JSON file for review\n",
    "with open(\"exec_skill_chunks_parabased.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump([{\"executive_skill\": doc.metadata[\"executive_skill\"], \"section_title\": doc.metadata[\"section_title\"], \"content\": doc.page_content} for doc in exec_docs], f, indent=4)\n",
    "\n",
    "# -------------------------------\n",
    "# 4. Embed & Store in ChromaDB Collection \"exec_skills\"\n",
    "# -------------------------------\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "client = chromadb.Client()\n",
    "exec_collection = client.get_or_create_collection(\"exec_skills\")\n",
    "\n",
    "# Use a deterministic ID: based on executive_skill and the chunk index\n",
    "existing_exec_ids = set(exec_collection.get()[\"ids\"])\n",
    "\n",
    "for idx, doc in enumerate(exec_docs):\n",
    "    skill = doc.metadata[\"executive_skill\"].replace(\" \", \"_\")\n",
    "    doc_id = f\"exec_{skill}_{idx}\"\n",
    "    if doc_id in existing_exec_ids:\n",
    "        continue\n",
    "    embedding = model.encode(doc.page_content).tolist()\n",
    "    exec_collection.add(\n",
    "        documents=[doc.page_content],\n",
    "        metadatas=[doc.metadata],\n",
    "        embeddings=[embedding],\n",
    "        ids=[doc_id]\n",
    "    )\n",
    "    existing_exec_ids.add(doc_id)\n",
    "\n",
    "print(\"Executive skill chunks embedded and stored in the 'exec_skills' collection.\")\n",
    "\n",
    "# -------------------------------\n",
    "# 5. (Optional) Retrieval Example\n",
    "# -------------------------------\n",
    "query_text = \"What are some strategies to improve Working Memory?\"\n",
    "\n",
    "# Use a single filter expression directly (do not wrap in $and if there's only one condition)\n",
    "# metadata_filter = {\"executive_skill\": \"Working Memory\"}\n",
    "\n",
    "results = exec_collection.query(\n",
    "    query_texts=[query_text],\n",
    "    n_results=5,\n",
    "    # where=metadata_filter,\n",
    "    include=[\"documents\", \"metadatas\", \"distances\"]\n",
    ")\n",
    "\n",
    "# Unpack results from the first query batch.\n",
    "flat = {\n",
    "    \"ids\": results[\"ids\"][0],\n",
    "    \"documents\": results[\"documents\"][0],\n",
    "    \"metadatas\": results[\"metadatas\"][0],\n",
    "    \"distances\": results[\"distances\"][0]\n",
    "}\n",
    "\n",
    "import json\n",
    "print(json.dumps(flat, indent=4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking how are meta data is for tb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import chromadb\n",
    "\n",
    "# Initialize ChromaDB client and connect to your \"exec_skills\" collection.\n",
    "client = chromadb.Client()\n",
    "exec_collection = client.get_or_create_collection(\"exec_skills\")\n",
    "\n",
    "# Retrieve all records from the collection.\n",
    "records = exec_collection.get()\n",
    "\n",
    "# The \"metadatas\" field is expected to be a list of lists.\n",
    "# We want to flatten that to a list of dictionaries.\n",
    "flat_metadatas = []\n",
    "\n",
    "# Loop over each batch (each batch is expected to be a list)\n",
    "for batch in records.get(\"metadatas\", []):\n",
    "    if isinstance(batch, list):\n",
    "        # Check each element in the batch:\n",
    "        for meta in batch:\n",
    "            # If the element is a dictionary, append it directly.\n",
    "            if isinstance(meta, dict):\n",
    "                flat_metadatas.append(meta)\n",
    "            else:\n",
    "                # If it is not a dict, it might be a string (i.e. a key),\n",
    "                # so we skip or wrap it as needed.\n",
    "                flat_metadatas.append({\"value\": meta})\n",
    "    elif isinstance(batch, dict):\n",
    "        # In case the batch itself is a dictionary, add it.\n",
    "        flat_metadatas.append(batch)\n",
    "    else:\n",
    "        # Otherwise, add as a simple string wrapped in a dict.\n",
    "        flat_metadatas.append({\"value\": batch})\n",
    "\n",
    "# Print the metadata in a readable JSON format.\n",
    "print(\"All metadata from the 'exec_skills' collection:\")\n",
    "print(json.dumps(flat_metadatas, indent=4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recursive Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install nltk langchain chromadb sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "from typing import List, Tuple\n",
    "from langchain.docstore.document import Document\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# -------------------------------\n",
    "# Step 1: Top-Level Splitting by Explicit Markers\n",
    "# -------------------------------\n",
    "def split_by_markers(text: str) -> List[Tuple[str, str]]:\n",
    "    \"\"\"\n",
    "    Splits the text using markers of the form:\n",
    "       ---- Section Title ----\n",
    "    Ignores markers that have \"end\" (case-insensitive) in the title.\n",
    "    Returns a list of (section_title, section_content) tuples.\n",
    "    \"\"\"\n",
    "    marker_pattern = r\"(?m)^----\\s*(.*?)\\s*----\\s*$\"\n",
    "    markers = list(re.finditer(marker_pattern, text))\n",
    "    sections = []\n",
    "    for i, m in enumerate(markers):\n",
    "        title = m.group(1).strip()\n",
    "        if \"end\" in title.lower():\n",
    "            continue\n",
    "        start_index = m.end()\n",
    "        end_index = len(text)\n",
    "        for j in range(i+1, len(markers)):\n",
    "            next_title = markers[j].group(1).strip()\n",
    "            # If next marker is an \"end\" marker or a new section, stop here.\n",
    "            if \"end\" in next_title.lower() or next_title:\n",
    "                end_index = markers[j].start()\n",
    "                break\n",
    "        content = text[start_index:end_index].strip()\n",
    "        sections.append((title, content))\n",
    "    return sections\n",
    "\n",
    "# -------------------------------\n",
    "# Step 2: Sentence-Based Chunking with Overlap\n",
    "# -------------------------------\n",
    "def group_sentences(sentences: List[str], max_chunk_size: int = 1000, overlap_sent_count: int = 2) -> List[str]:\n",
    "    \"\"\"\n",
    "    Groups sentences into chunks that do not exceed max_chunk_size (by character count)\n",
    "    and adds overlap of the last few sentences to the next chunk.\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    current_sentences = []\n",
    "    current_length = 0\n",
    "\n",
    "    for sentence in sentences:\n",
    "        # +1 for a space between sentences.\n",
    "        if current_length + len(sentence) + (1 if current_sentences else 0) <= max_chunk_size:\n",
    "            current_sentences.append(sentence)\n",
    "            current_length = len(\" \".join(current_sentences))\n",
    "        else:\n",
    "            # Current chunk is complete; join it.\n",
    "            chunk = \" \".join(current_sentences)\n",
    "            chunks.append(chunk)\n",
    "            # Now, determine overlap: use the last overlap_sent_count sentences.\n",
    "            overlap_sentences = current_sentences[-overlap_sent_count:] if len(current_sentences) >= overlap_sent_count else current_sentences\n",
    "            # Start a new chunk with the overlap and the current sentence.\n",
    "            current_sentences = overlap_sentences + [sentence]\n",
    "            current_length = len(\" \".join(current_sentences))\n",
    "    if current_sentences:\n",
    "        chunks.append(\" \".join(current_sentences))\n",
    "    return chunks\n",
    "\n",
    "def split_section_paragraphs(sections: List[Tuple[str, str]], max_chunk_size: int = 1000, overlap_sent_count: int = 2) -> List[Document]:\n",
    "    \"\"\"\n",
    "    For each section (tuple of title and content), first split into paragraphs using blank lines,\n",
    "    then for each paragraph split into sentences (using nltk.sent_tokenize) and group them into chunks.\n",
    "    Each resulting chunk is saved as a Document with metadata.\n",
    "    \"\"\"\n",
    "    docs = []\n",
    "\n",
    "    def map_title(title: str) -> str:\n",
    "        lower = title.lower()\n",
    "        if lower.startswith(\"building response inhibition\"):\n",
    "            return \"Response Inhibition\"\n",
    "        elif lower.startswith(\"enhancing working memory\"):\n",
    "            return \"Working Memory\"\n",
    "        # Extend mappings for additional executive skill sections as needed.\n",
    "        return title  # fallback: use the raw title\n",
    "\n",
    "    for title, content in sections:\n",
    "        canonical_skill = map_title(title)\n",
    "        # First, split the section into paragraphs (using two or more newlines)\n",
    "        paragraphs = re.split(r\"\\n\\s*\\n\", content)\n",
    "        for para in paragraphs:\n",
    "            para = para.strip()\n",
    "            if not para:\n",
    "                continue\n",
    "            # Use nltk to split into sentences.\n",
    "            sentences = sent_tokenize(para)\n",
    "            # Group sentences into chunks with our function.\n",
    "            chunk_texts = group_sentences(sentences, max_chunk_size=max_chunk_size, overlap_sent_count=overlap_sent_count)\n",
    "            for chunk in chunk_texts:\n",
    "                docs.append(Document(\n",
    "                    page_content=chunk,\n",
    "                    metadata={\n",
    "                        \"executive_skill\": canonical_skill,\n",
    "                        \"section_title\": title\n",
    "                    }\n",
    "                ))\n",
    "    return docs\n",
    "\n",
    "# -------------------------------\n",
    "# Step 3: Load Text File and Process\n",
    "# -------------------------------\n",
    "with open(\"Smartbutscattered.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    book_text = f.read()\n",
    "\n",
    "sections = split_by_markers(book_text)\n",
    "exec_docs = split_section_paragraphs(sections, max_chunk_size=1000, overlap_sent_count=2)\n",
    "\n",
    "# (Optional) Write the resulting chunks (metadata and content) to a JSON file for inspection.\n",
    "with open(\"exec_skill_chunks.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(\n",
    "        [{\"executive_skill\": doc.metadata[\"executive_skill\"],\n",
    "          \"section_title\": doc.metadata[\"section_title\"],\n",
    "          \"content\": doc.page_content}\n",
    "         for doc in exec_docs],\n",
    "        f,\n",
    "        indent=4\n",
    "    )\n",
    "# -------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Langchain recursive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "from typing import List, Tuple\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "\n",
    "# -------------------------------\n",
    "# Step 1: Hierarchical Splitting by Explicit Markers\n",
    "# -------------------------------\n",
    "def split_by_markers_recursivelangchain(text: str) -> List[Tuple[str, str]]:\n",
    "    \"\"\"\n",
    "    Splits the text using markers of the form:\n",
    "        ---- Section Title ----\n",
    "    Skips markers that have \"end\" in the title (case-insensitive). Returns a list of tuples:\n",
    "    (section_title, section_content)\n",
    "    \"\"\"\n",
    "    marker_pattern = r\"(?m)^----\\s*(.*?)\\s*----\\s*$\"\n",
    "    markers = list(re.finditer(marker_pattern, text))\n",
    "    sections_recursivelangchain = []\n",
    "    for i, m in enumerate(markers):\n",
    "        title = m.group(1).strip()\n",
    "        if \"end\" in title.lower():\n",
    "            continue  # Skip markers that indicate an end section\n",
    "        start_index = m.end()\n",
    "        end_index = len(text)\n",
    "        # Look for the next marker (regardless of its title) to define the end of this section.\n",
    "        for j in range(i + 1, len(markers)):\n",
    "            next_title = markers[j].group(1).strip()\n",
    "            # We stop at the very next marker (even if it is an 'end' marker)\n",
    "            end_index = markers[j].start()\n",
    "            break\n",
    "        content = text[start_index:end_index].strip()\n",
    "        sections_recursivelangchain.append((title, content))\n",
    "    return sections_recursivelangchain\n",
    "\n",
    "# -------------------------------\n",
    "# Step 2: Mapping Section Titles to Canonical Executive Skills\n",
    "# -------------------------------\n",
    "def map_title_to_skill_recursivelangchain(title: str) -> str:\n",
    "    lower = title.lower()\n",
    "    if lower.startswith(\"building response inhibition\"):\n",
    "        return \"Response Inhibition\"\n",
    "    elif lower.startswith(\"enhancing working memory\"):\n",
    "        return \"Working Memory\"\n",
    "    elif lower.startswith(\"improving emotional control\"):\n",
    "        return \"Improving Emotional Control\"\n",
    "    elif lower.startswith(\"strengthening sustained attention\"):\n",
    "        return \"Strengthening Sustained Attention\"\n",
    "    elif lower.startswith(\"teaching task initiation\"):\n",
    "        return \"Teaching Task Initiation\"\n",
    "    elif lower.startswith(\"promoting, planning, and prioritizing\"):\n",
    "        return \"Promoting, Planning, and Prioritizing\"\n",
    "    elif lower.startswith(\"fostering organization\"):\n",
    "        return \"Fostering Organization\"\n",
    "    elif lower.startswith(\"instilling time management\"):\n",
    "        return \"Instilling Time Management\"\n",
    "    elif lower.startswith(\"increasing goal-directed persistence\"):\n",
    "        return \"Increasing Goal-Directed Persistence\"\n",
    "    elif lower.startswith(\"cultivating metacognition\"):\n",
    "        return \"Cultivating Metacognition\"\n",
    "    # Fallback: return the raw title\n",
    "    return title\n",
    "\n",
    "# -------------------------------\n",
    "# Step 3: Recursive Chunking Using RecursiveCharacterTextSplitter\n",
    "# -------------------------------\n",
    "def recursive_chunk_sections_recursivelangchain(\n",
    "    sections: List[Tuple[str, str]], \n",
    "    chunk_size: int = 1000, \n",
    "    chunk_overlap: int = 200\n",
    ") -> List[Document]:\n",
    "    docs_recursivelangchain = []\n",
    "    recursive_splitter_recursivelangchain = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "    )\n",
    "    for title, content in sections:\n",
    "        canonical_skill = map_title_to_skill_recursivelangchain(title)\n",
    "        chunks = recursive_splitter_recursivelangchain.split_text(content)\n",
    "        for chunk in chunks:\n",
    "            docs_recursivelangchain.append(Document(\n",
    "                page_content=chunk,\n",
    "                metadata={\n",
    "                    \"executive_skill\": canonical_skill,\n",
    "                    \"section_title\": title\n",
    "                }\n",
    "            ))\n",
    "    return docs_recursivelangchain\n",
    "\n",
    "# -------------------------------\n",
    "# Step 4: Load the Text File and Process into Documents\n",
    "# -------------------------------\n",
    "with open(\"Smartbutscattered.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    book_text_recursivelangchain = f.read()\n",
    "\n",
    "sections_recursivelangchain = split_by_markers_recursivelangchain(book_text_recursivelangchain)\n",
    "print(f\"Total sections found: {len(sections_recursivelangchain)}\")\n",
    "\n",
    "exec_docs_recursivelangchain = recursive_chunk_sections_recursivelangchain(\n",
    "    sections_recursivelangchain,\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200\n",
    ")\n",
    "print(f\"Total recursive chunks created: {len(exec_docs_recursivelangchain)}\")\n",
    "\n",
    "# (Optional) Write the resulting recursive chunks to a JSON file for inspection.\n",
    "with open(\"exec_skill_chunks_recursive_recursivelangchain.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(\n",
    "        [{\n",
    "            \"executive_skill\": doc.metadata[\"executive_skill\"],\n",
    "            \"section_title\": doc.metadata[\"section_title\"],\n",
    "            \"content\": doc.page_content\n",
    "        } for doc in exec_docs_recursivelangchain],\n",
    "        f,\n",
    "        indent=4\n",
    "    )\n",
    "\n",
    "# -------------------------------\n",
    "# Step 5: Embed & Store Documents in ChromaDB (with duplicate check)\n",
    "# -------------------------------\n",
    "# Initialize the embedding model.\n",
    "embedding_model_recursivelangchain = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Set the persistent directory.\n",
    "persist_dir_recursivelangchain = \"./chroma_exec_skills_db_recursivelangchain\"\n",
    "\n",
    "# Attempt to initialize a new Chroma client with our settings.\n",
    "try:\n",
    "    client_recursivelangchain = chromadb.Client(Settings(\n",
    "        chroma_db_impl=\"duckdb+parquet\",\n",
    "        persist_directory=persist_dir_recursivelangchain\n",
    "    ))\n",
    "except ValueError as e:\n",
    "    # If an instance already exists with different settings, reuse the existing client.\n",
    "    print(\"Chroma client already exists with different settings. Reusing the existing client.\")\n",
    "    client_recursivelangchain = chromadb.Client()  # reusing default ephemeral client\n",
    "\n",
    "# Create (or get) the collection.\n",
    "collection_recursivelangchain = client_recursivelangchain.get_or_create_collection(\n",
    "    name=\"exec_skills_recursivelangchain\"\n",
    ")\n",
    "\n",
    "# Fetch existing IDs from the collection (to avoid duplicates).\n",
    "existing_ids_recursivelangchain = set()\n",
    "try:\n",
    "    for sublist in collection_recursivelangchain.get()[\"ids\"]:\n",
    "        existing_ids_recursivelangchain.update(sublist)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# Loop over each document chunk and add it if not already stored.\n",
    "for idx, doc in enumerate(exec_docs_recursivelangchain):\n",
    "    doc_id_recursivelangchain = f\"skill_{doc.metadata['executive_skill']}_{idx}_recursivelangchain\"\n",
    "    if doc_id_recursivelangchain in existing_ids_recursivelangchain:\n",
    "        continue  # Skip duplicate chunks\n",
    "    embedding_recursivelangchain = embedding_model_recursivelangchain.encode(doc.page_content).tolist()\n",
    "    collection_recursivelangchain.add(\n",
    "        documents=[doc.page_content],\n",
    "        metadatas=[doc.metadata],\n",
    "        embeddings=[embedding_recursivelangchain],\n",
    "        ids=[doc_id_recursivelangchain]\n",
    "    )\n",
    "    existing_ids_recursivelangchain.add(doc_id_recursivelangchain)\n",
    "\n",
    "print(\"Embedding complete. All recursive chunks stored in the 'exec_skills_recursivelangchain' collection.\")\n",
    "\n",
    "# -------------------------------\n",
    "# Step 6: Query Example\n",
    "# -------------------------------\n",
    "query_text_recursivelangchain = \"What are some strategies to improve working memory?\"\n",
    "# Use a metadata filter that selects only the chunks with executive_skill = \"Working Memory\"\n",
    "metadata_filter_recursivelangchain = {\"executive_skill\": \"Working Memory\"}\n",
    "\n",
    "results_recursivelangchain = collection_recursivelangchain.query(\n",
    "    query_texts=[query_text_recursivelangchain],\n",
    "    n_results=5,\n",
    "    where=metadata_filter_recursivelangchain,\n",
    "    include=[\"documents\", \"metadatas\", \"distances\"]\n",
    ")\n",
    "\n",
    "flat_results_recursivelangchain = {\n",
    "    \"documents\": results_recursivelangchain[\"documents\"][0],\n",
    "    \"metadatas\": results_recursivelangchain[\"metadatas\"][0],\n",
    "    \"distances\": results_recursivelangchain[\"distances\"][0]\n",
    "}\n",
    "\n",
    "import pprint\n",
    "print(\"Query Results (for executive_skill 'Working Memory'):\")\n",
    "pprint.pprint(flat_results_recursivelangchain)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual Semantic Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk langchain chromadb sentence_transformers scikit-learn numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "from typing import List, Tuple\n",
    "from langchain.docstore.document import Document\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# -------------------------------\n",
    "# Step 1: Top-Level Splitting by Markers\n",
    "# -------------------------------\n",
    "def split_by_markers(text: str) -> List[Tuple[str, str]]:\n",
    "    \"\"\"\n",
    "    Splits the text using markers (lines starting with \"----\" and ending with \"----\").\n",
    "    Ignores markers that contain \"end\" (case-insensitive).\n",
    "    Returns a list of tuples: (section_title, section_content)\n",
    "    \"\"\"\n",
    "    marker_pattern = r\"(?m)^----\\s*(.*?)\\s*----\\s*$\"\n",
    "    markers = list(re.finditer(marker_pattern, text))\n",
    "    sections = []\n",
    "    for i, m in enumerate(markers):\n",
    "        title = m.group(1).strip()\n",
    "        if \"end\" in title.lower():\n",
    "            continue\n",
    "        start_index = m.end()\n",
    "        end_index = len(text)\n",
    "        for j in range(i+1, len(markers)):\n",
    "            next_title = markers[j].group(1).strip()\n",
    "            # If the next marker is an \"end\" marker or any marker then stop.\n",
    "            if \"end\" in next_title.lower() or next_title:\n",
    "                end_index = markers[j].start()\n",
    "                break\n",
    "        content = text[start_index:end_index].strip()\n",
    "        sections.append((title, content))\n",
    "    return sections\n",
    "\n",
    "# -------------------------------\n",
    "# Step 2: Sentence-Based Chunking with Overlap Using Semantic Grouping\n",
    "# -------------------------------\n",
    "def recursive_split_by_length(text: str, max_chunk_size: int) -> List[str]:\n",
    "    \"\"\"\n",
    "    Fallback: Split a given text (which should already be a coherent chunk)\n",
    "    into smaller pieces (using sentence boundaries) if it exceeds max_chunk_size.\n",
    "    \"\"\"\n",
    "    sentences = sent_tokenize(text)\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_length = 0\n",
    "    for sentence in sentences:\n",
    "        if current_length + len(sentence) + (1 if current_chunk else 0) <= max_chunk_size:\n",
    "            current_chunk.append(sentence)\n",
    "            current_length = len(\" \".join(current_chunk))\n",
    "        else:\n",
    "            if current_chunk:\n",
    "                chunks.append(\" \".join(current_chunk))\n",
    "            current_chunk = [sentence]\n",
    "            current_length = len(sentence) + 1\n",
    "    if current_chunk:\n",
    "        chunks.append(\" \".join(current_chunk))\n",
    "    return chunks\n",
    "\n",
    "def semantic_chunking(text: str, model: SentenceTransformer, similarity_threshold: float = 0.7, \n",
    "                      max_chunk_size: int = 1000) -> List[str]:\n",
    "    \"\"\"\n",
    "    Splits the text into sentences, embeds them, and clusters them using Agglomerative\n",
    "    Clustering based on cosine similarity. Groups adjacent sentences that are semantically similar.\n",
    "    If a group produces a chunk longer than max_chunk_size, further split it.\n",
    "    Returns a list of text chunks.\n",
    "    \"\"\"\n",
    "    sentences = sent_tokenize(text)\n",
    "    if not sentences:\n",
    "        return []\n",
    "    \n",
    "    # Compute embeddings for each sentence\n",
    "    embeddings = model.encode(sentences)\n",
    "    \n",
    "    # Perform clustering using AgglomerativeClustering.\n",
    "    # Replace the \"affinity\" parameter with \"metric\" to suit newer scikit-learn versions.\n",
    "    clustering = AgglomerativeClustering(n_clusters=None, metric='cosine', linkage='average',\n",
    "                                           distance_threshold=1 - similarity_threshold)\n",
    "    clustering.fit(embeddings)\n",
    "    cluster_labels = clustering.labels_\n",
    "    \n",
    "    chunks = []\n",
    "    current_cluster = cluster_labels[0]\n",
    "    current_sentences = []\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        if cluster_labels[i] == current_cluster:\n",
    "            current_sentences.append(sentence)\n",
    "        else:\n",
    "            chunk = \" \".join(current_sentences)\n",
    "            if len(chunk) > max_chunk_size:\n",
    "                sub_chunks = recursive_split_by_length(chunk, max_chunk_size)\n",
    "                chunks.extend(sub_chunks)\n",
    "            else:\n",
    "                chunks.append(chunk)\n",
    "            # Start the new chunk with overlap: include the last sentence of the previous group\n",
    "            overlap = [current_sentences[-1]] if current_sentences else []\n",
    "            current_sentences = overlap + [sentence]\n",
    "            current_cluster = cluster_labels[i]\n",
    "    if current_sentences:\n",
    "        chunk = \" \".join(current_sentences)\n",
    "        if len(chunk) > max_chunk_size:\n",
    "            sub_chunks = recursive_split_by_length(chunk, max_chunk_size)\n",
    "            chunks.extend(sub_chunks)\n",
    "        else:\n",
    "            chunks.append(chunk)\n",
    "    return chunks\n",
    "\n",
    "def split_section_semantically(sections: List[Tuple[str, str]], model: SentenceTransformer,\n",
    "                               max_chunk_size: int = 1000, similarity_threshold: float = 0.7) -> List[Document]:\n",
    "    \"\"\"\n",
    "    For each section, split the content into paragraphs (using blank lines), then apply semantic\n",
    "    chunking to each paragraph. Each resulting chunk becomes a Document with metadata.\n",
    "    \"\"\"\n",
    "    docs = []\n",
    "    \n",
    "    def map_title(title: str) -> str:\n",
    "        lower = title.lower()\n",
    "        if lower.startswith(\"building response inhibition\"):\n",
    "            return \"Response Inhibition\"\n",
    "        elif lower.startswith(\"enhancing working memory\"):\n",
    "            return \"Working Memory\"\n",
    "        # Extend mappings for additional sections as needed.\n",
    "        return title  # fallback using raw title\n",
    "    \n",
    "    for title, content in sections:\n",
    "        canonical_skill = map_title(title)\n",
    "        paragraphs = re.split(r\"\\n\\s*\\n\", content)\n",
    "        for para in paragraphs:\n",
    "            para = para.strip()\n",
    "            if not para:\n",
    "                continue\n",
    "            chunks = semantic_chunking(para, model, similarity_threshold=similarity_threshold,\n",
    "                                       max_chunk_size=max_chunk_size)\n",
    "            for chunk in chunks:\n",
    "                docs.append(Document(\n",
    "                    page_content=chunk,\n",
    "                    metadata={\"executive_skill\": canonical_skill, \"section_title\": title}\n",
    "                ))\n",
    "    return docs\n",
    "\n",
    "# -------------------------------\n",
    "# Step 3: Load the Text File and Process\n",
    "# -------------------------------\n",
    "with open(\"Smartbutscattered.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    book_text = f.read()\n",
    "\n",
    "sections = split_by_markers(book_text)\n",
    "\n",
    "# Initialize SentenceTransformer model once for chunking\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "exec_docs = split_section_semantically(sections, model=model, max_chunk_size=1000, similarity_threshold=0.7)\n",
    "\n",
    "# (Optional) Write the resulting chunks to a JSON file for inspection.\n",
    "with open(\"exec_skill_chunks.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(\n",
    "        [{\"executive_skill\": doc.metadata[\"executive_skill\"],\n",
    "          \"section_title\": doc.metadata[\"section_title\"],\n",
    "          \"content\": doc.page_content}\n",
    "         for doc in exec_docs],\n",
    "        f,\n",
    "        indent=4\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra for lesson plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import uuid\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "\n",
    "# -------------------------------\n",
    "# STEP 1: Read & Chunk the Lesson Plans\n",
    "# -------------------------------\n",
    "\n",
    "# Read the file that contains all lesson plans.\n",
    "with open(\"lessonplans.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    full_text = f.read()\n",
    "\n",
    "lesson_plan_blocks = re.findall(\n",
    "    r\"--- Start of Lesson Plan(.*?)--- End of Lesson Plan\",\n",
    "    full_text,\n",
    "    re.DOTALL\n",
    ")\n",
    "\n",
    "# Define the stop marker (we stop processing at this note)\n",
    "stop_marker = r\"Note: The following pages are intended for classroom use for students as a visual aid to learning\\.\"\n",
    "\n",
    "section_headers = {\n",
    "    \"assessment\": r\"Assessment\\s*\\n\",\n",
    "    \"extensions\": r\"Extensions(?: and Connections)?\\s*\\n\",\n",
    "    \"differentiation\": r\"Strategies for Differentiation\\s*\\n\"\n",
    "}\n",
    "\n",
    "chunked_lesson_plans = []\n",
    "\n",
    "for block in lesson_plan_blocks:\n",
    "    # 1. Limit processing to the content before the stop marker.\n",
    "    split_block = re.split(stop_marker, block, maxsplit=1)\n",
    "    content_to_process = split_block[0].strip() if split_block else block.strip()\n",
    "    \n",
    "    # 2. Split into intro_context and remainder using \"Student/Teacher Actions:\" (case-insensitive)\n",
    "    parts = re.split(r\"Student/Teacher Actions:\\s*\", content_to_process, flags=re.IGNORECASE, maxsplit=1)\n",
    "    if len(parts) == 2:\n",
    "        intro_context = parts[0].strip()\n",
    "        remainder = parts[1].strip()\n",
    "    else:\n",
    "        # If no \"Student/Teacher Actions:\" found, treat entire content as intro_context.\n",
    "        intro_context = content_to_process\n",
    "        remainder = \"\"\n",
    "\n",
    "    combined_pattern = r\"(Assessment\\s*\\n|Extensions(?: and Connections)?\\s*\\n|Strategies for Differentiation\\s*\\n)\"\n",
    "    split_sections = re.split(combined_pattern, remainder)\n",
    "    \n",
    "    # The first part (before any extra header) is the instructional_steps.\n",
    "    instructional_steps = split_sections[0].strip() if split_sections else \"\"\n",
    "    \n",
    "    # Initialize placeholders for extra sections.\n",
    "    assessment_text = \"\"\n",
    "    extensions_text = \"\"\n",
    "    differentiation_text = \"\"\n",
    "    \n",
    "    # Process remaining parts in pairs: header then content.\n",
    "    for i in range(1, len(split_sections) - 1, 2):\n",
    "        header = split_sections[i].strip().lower()\n",
    "        content = split_sections[i+1].strip()\n",
    "        if header.startswith(\"assessment\"):\n",
    "            assessment_text = content\n",
    "        elif header.startswith(\"extensions\"):\n",
    "            extensions_text = content\n",
    "        elif header.startswith(\"strategies for differentiation\"):\n",
    "            differentiation_text = content\n",
    "\n",
    "    # Build the JSON structure for this lesson plan.\n",
    "    lesson_plan_json = {\n",
    "        \"intro_context\": intro_context,\n",
    "        \"instructional_steps\": instructional_steps,\n",
    "        \"assessment\": assessment_text,\n",
    "        \"extensions\": extensions_text,\n",
    "        \"differentiation\": differentiation_text\n",
    "    }\n",
    "    \n",
    "    chunked_lesson_plans.append(lesson_plan_json)\n",
    "\n",
    "# Optionally, save the chunked lesson plans for later reference.\n",
    "with open(\"chunked_lesson_plans.json\", \"w\", encoding=\"utf-8\") as outfile:\n",
    "    json.dump(chunked_lesson_plans, outfile, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# STEP 2: Extract Metadata & Embed Chunks into ChromaDB\n",
    "# -------------------------------\n",
    "\n",
    "def extract_metadata_from_intro(intro_context):\n",
    "    \"\"\"\n",
    "    Extract additional metadata from the intro_context.\n",
    "    Attempts to extract:\n",
    "      - grade (e.g., \"Grade 5\")\n",
    "      - subject (from the \"Strand:\" line)\n",
    "      - topic (from the \"Topic:\" line)\n",
    "      - lesson_title (from the first line)\n",
    "    \"\"\"\n",
    "    grade_match = re.search(r\"Grade\\s*(\\d+)\", intro_context)\n",
    "    grade = grade_match.group(1) if grade_match else None\n",
    "    \n",
    "    subject_match = re.search(r\"Strand:\\s*(.+)\", intro_context)\n",
    "    subject = subject_match.group(1).strip() if subject_match else None\n",
    "    \n",
    "    topic_match = re.search(r\"Topic:\\s*(.+)\", intro_context)\n",
    "    topic = topic_match.group(1).strip() if topic_match else None\n",
    "    \n",
    "    title_line = intro_context.split(\"\\n\")[0].strip() if intro_context else \"\"\n",
    "    lesson_title = title_line if title_line else None\n",
    "    \n",
    "    return {\n",
    "        \"grade\": grade,\n",
    "        \"subject\": subject,\n",
    "        \"topic\": topic,\n",
    "        \"lesson_title\": lesson_title\n",
    "    }\n",
    "\n",
    "# Define a custom embedding function class that meets the new interface.\n",
    "class SentenceTransformerEmbedding:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "    def __call__(self, input):\n",
    "        # 'input' should be a list of strings.\n",
    "        return self.model.encode(input).tolist()\n",
    "\n",
    "# Initialize the embedding model.\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Initialize a ChromaDB client using the new client constructor.\n",
    "client = chromadb.Client()\n",
    "\n",
    "embedding_fn = SentenceTransformerEmbedding(model)\n",
    "collection = client.get_or_create_collection(\"lesson_plans\", embedding_function=embedding_fn)\n",
    "\n",
    "# Iterate over each lesson plan and each section.\n",
    "for lesson_index, lesson in enumerate(chunked_lesson_plans):\n",
    "\n",
    "    additional_metadata = extract_metadata_from_intro(lesson.get(\"intro_context\", \"\"))\n",
    "    \n",
    "\n",
    "    for section in [\"intro_context\", \"instructional_steps\", \"assessment\", \"extensions\", \"differentiation\"]:\n",
    "        text = lesson.get(section, \"\").strip()\n",
    "        if text:\n",
    "            # Create a unique ID for this chunk.\n",
    "            doc_id = f\"lesson{lesson_index}_{section}_{str(uuid.uuid4())[:8]}\"\n",
    "            # Compute the embedding.\n",
    "            embedding = model.encode(text).tolist()  # Convert numpy array to list.\n",
    "        \n",
    "            metadata = {\n",
    "                \"lesson_index\": lesson_index,\n",
    "                \"section\": section,\n",
    "                **additional_metadata  # Merge in grade, subject, topic, lesson_title.\n",
    "            }\n",
    "            collection.add(\n",
    "                documents=[text],\n",
    "                metadatas=[metadata],\n",
    "                embeddings=[embedding],\n",
    "                ids=[doc_id]\n",
    "            )\n",
    "\n",
    "print(\"All lesson plan chunks have been embedded and stored in the vector database.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# -------------------------------\n",
    "# Verification Example 1: Metadata Filter\n",
    "# -------------------------------\n",
    "\n",
    "results_metadata = collection.query(\n",
    "    query_texts=\"\", \n",
    "    n_results=100, \n",
    "    where={\"lesson_index\": 0}\n",
    ")\n",
    "\n",
    "print(\"Results filtered by metadata (lesson_index == 0):\")\n",
    "print(json.dumps(results_metadata, indent=4))\n",
    "\n",
    "query_texts = \"Computation and Estimation with Decimals\"\n",
    "results_similarity = collection.query(\n",
    "    query_texts=query_texts,\n",
    "    n_results=5,\n",
    "    where={\"lesson_index\": 0}\n",
    ")\n",
    "\n",
    "print(\"\\nResults from similarity search with query '{}':\".format(query_texts))\n",
    "print(json.dumps(results_similarity, indent=4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
